+ BASE_DIR=/data/yinxiaoln
++ date +%Y-%m-%d_%H:%M:%S
+ SAVE_PATH=2023-11-18_06:56:37
+ main
+ train
+ work_dir=/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37
+ CUDA_VISIBLE_DEVICES=1
+ xtuner train baichuan_xtuner_config.py --work-dir /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37
[2023-11-18 06:56:43,124] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 06:56:47,153] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/18 06:56:48 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
    CUDA available: True
    numpy_random_seed: 127766124
    GPU 0,1: NVIDIA A40
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.3, V12.3.52
    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0+cu121
    OpenCV: 4.8.1
    MMEngine: 0.9.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 127766124
    deterministic: False
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

11/18 06:56:48 - mmengine - INFO - Config:
BASE_PATH = '/data/yinxiaoln/'
accumulative_counts = 16
alpaca_en = dict(
    dataset=dict(
        data_files=dict(
            train=
            '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
        ),
        path='json',
        type='datasets.load_dataset'),
    dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
    max_length=2048,
    pack_to_max_length=True,
    remove_unused_columns=True,
    shuffle_before_pack=True,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.process_hf_dataset')
alpaca_en_path = '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
batch_size = 64
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.DatasetInfoHook'),
    dict(
        evaluation_inputs=[
            '请给我介绍五个上海的景点',
            'Please tell me five scenic spots in Shanghai',
        ],
        every_n_iters=500,
        instruction=
        'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat.INSTRUCTION_START',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.EvaluateChatHook'),
]
dataloader_num_workers = 0
default_hooks = dict(
    checkpoint=dict(interval=1, type='mmengine.hooks.CheckpointHook'),
    logger=dict(interval=10, type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_inputs = [
    '请给我介绍五个上海的景点',
    'Please tell me five scenic spots in Shanghai',
]
launcher = 'none'
load_from = None
log_level = 'INFO'
lr = 0.0002
max_epochs = 10
max_length = 2048
max_norm = 1
model = dict(
    llm=dict(
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        quantization_config=dict(
            bnb_4bit_compute_dtype='torch.float16',
            bnb_4bit_quant_type='nf4',
            bnb_4bit_use_double_quant=True,
            llm_int8_has_fp16_weight=False,
            llm_int8_threshold=6.0,
            load_in_4bit=True,
            load_in_8bit=False,
            type='transformers.BitsAndBytesConfig'),
        torch_dtype='torch.float16',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    lora=dict(
        bias='none',
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        task_type='CAUSAL_LM',
        type='peft.LoraConfig'),
    type='xtuner.model.SupervisedFinetune')
optim_type = 'bitsandbytes.optim.PagedAdamW32bit'
optim_wrapper = dict(
    accumulative_counts=16,
    clip_grad=dict(error_if_nonfinite=False, max_norm=1),
    dtype='float16',
    loss_scale='dynamic',
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=0.0002,
        type='bitsandbytes.optim.PagedAdamW32bit',
        weight_decay=0),
    type='mmengine.optim.AmpOptimWrapper')
pack_to_max_length = True
param_scheduler = dict(
    T_max=10,
    by_epoch=True,
    convert_to_iter_based=True,
    eta_min=2e-05,
    type='mmengine.optim.CosineAnnealingLR')
pretrained_model_name_or_path = '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat'
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(by_epoch=True, max_epochs=10, val_interval=1)
train_dataloader = dict(
    batch_size=64,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        dataset=dict(
            data_files=dict(
                train=
                '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
            ),
            path='json',
            type='datasets.load_dataset'),
        dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
        max_length=2048,
        pack_to_max_length=True,
        remove_unused_columns=True,
        shuffle_before_pack=True,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.process_hf_dataset'),
    num_workers=0,
    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))
visualizer = None
weight_decay = 0
work_dir = '/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37'

quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
11/18 06:56:49 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 14.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:30<00:00, 15.08s/it]
11/18 06:57:20 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 06:57:20 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 06:57:20 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 06:57:20 - mmengine - INFO - dispatch baichuan2-13B attn forward
11/18 06:58:20 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
11/18 06:58:20 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
Found cached dataset json (/home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 658.34it/s]
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-817222a3ff087594.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-efe2e5113f9035af.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7868ac168b99dcee.arrow
Flattening the indices:   0%|          | 0/5256 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 5256/5256 [00:00<00:00, 50977.48 examples/s]                                                                                     Map:   0%|          | 0/5256 [00:00<?, ? examples/s]Map:  76%|███████▌  | 4000/5256 [00:00<00:00, 28522.54 examples/s]                                                                  11/18 06:58:21 - mmengine - WARNING - Dataset Dataset has no metainfo. ``dataset_meta`` in visualizer will be None.
11/18 06:58:21 - mmengine - INFO - Num train samples 80
11/18 06:58:21 - mmengine - INFO - train example:
11/18 06:58:21 - mmengine - INFO - <s> <reserved_106>判断以下陈述是否正确。
游标卡尺可用来测量长度、厚度、外径、内径、孔深等。<reserved_107>正确</s><s> <reserved_106>回答下列问题
刃口不锋利会造成（ ）<reserved_107>制件毛刺大</s><s> <reserved_106>判断以下陈述是否正确。
辅助支承用于提高工件装夹刚度和稳定性，并不起定位作用。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
可靠的防松装置有附加摩擦力防松装置和机械方法防松装置两大类<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
刀具材料的硬度越高，强度和韧性越低。<reserved_107>正确</s><s> <reserved_106>回答下列问题
加工孔距精度要求高的钻模板，常采用（ ）。<reserved_107>量套找正加工法</s><s> <reserved_106>回答下列问题
压注的作用是将塑料从（ ）中压入型腔，压柱是施加压力的主要零件。<reserved_107>加料室</s><s> <reserved_106>回答下列问题
试模过程中，注射时间太长，可能发生的缺陷有（ ）。<reserved_107>凹痕;熔接痕;裂纹</s><s> <reserved_106>回答下列问题
激光加工的应用（ ）。<reserved_107>切割;打孔;焊接</s><s> <reserved_106>判断以下陈述是否正确。
模具装配工艺规程包括：模具零件和组件的装配顺序，装配基准的确定，装配工艺方法和技术要求，装配工序划分以及关键工序的详细说明，必备的二级工具和设备，检验方法和验收条件等。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
塑件上设加强筋可增加塑件的强度和刚度。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
用平面铣刀铣削平面时，若平面铣刀直径小于工件宽度，为提高效率，建议每次铣削的最大宽度等于刀具直径。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
机械固定压入法常用于凹模与固定板的连接。<reserved_107>错误。</s><s> <reserved_106>判断以下陈述是否正确。
收缩性：指塑件从温度较高的模具中取出冷却到室温后，其尺寸或体积发生收缩的现像。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
模具粗加工铣削刀具一般使用圆刀片铣刀。<reserved_107>正确。</s><s> <reserved_106>判断以下陈述是否正确。
模具的闭合高度H应大于等于注射成型机模座最小开距；小于于等于注射成型机模座最大开距。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
出件装置主要有刚性推件装置与弹性出件装置两种<reserved_107>正确</s><s> <reserved_106>回答下列问题
推杆推出机构装配后，推杆工作端面应高于型面（ ）。<reserved_107>0.05~0.1mm</s><s> <reserved_106>判断以下陈述是否正确。
职业道德具有自愿性的特点。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
对于重要的主轴毛坯，一般采用（ ）。<reserved_107>锻造</s><s> <reserved_106>回答下列问题
塑件轴类尺寸的标注一般是（ ）。<reserved_107>双向偏差</s><s> <reserved_106>回答下列问题
以下不是孔特征的几种类型的是（ ）：<reserved_107>沉头孔</s><s> <reserved_106>判断以下陈述是否正确。
聚乙烯与聚苯乙烯塑料的分子结构相似，可以互相更换。<reserved_107>错误。</s><s> <reserved_106>判断以下陈述是否正确。
砂轮是由磨料用结合剂粘结在一起培烧而成的紧致多孔体<reserved_107>错误</s><s> <reserved_106>回答下列问题
将注射模具分为单分型面注射模，双分型面注射模等是按（ ）分类的。<reserved_107>按注射模的总体结构特征</s><s> <reserved_106>回答下列问题
热继电器主要用于电动机的（ ）。<reserved_107>过载保护</s><s> <reserved_106>回答下列问题
钢在淬火时的硬化能力取决于马氏体中的（ ）<reserved_107>含碳量</s><s> <reserved_106>判断以下陈述是否正确。
塑料的加工温度需达到玻璃态温度以上时，才能进行注塑成型。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
以下哪些是创建【直纹】（Rule）特征时的对齐方式：<reserved_107>等参数;等（圆）弧长;根据点;脊线</s><s> <reserved_106>判断以下陈述是否正确。
铰孔是对淬硬的孔进行精加工的一种方法。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
孔的形状精度主要有圆度和圆柱度。<reserved_107>正确</s><s> <reserved_106>回答下列问题
对产品尺寸精度要求比较高的非平衡布局的注射模具制造时，流道可选择（ ）尺寸，通过试模逐步修改完备。<reserved_107>偏小</s><s> <reserved_106>回答下列问题
专用量规是一种没有刻度的检验工具，用于检验工件的（ ）。<reserved_107>尺寸与形位误差</s><s> <reserved_106>回答下列问题
模具交付使用前，通常要根据（ ）等进行验收。<reserved_107>试模样品检验报告;试模状态记录;国家验收技术条件;模具的性能检验</s><s> <reserved_106>判断以下陈述是否正确。
注塑机运转时应注意料筒温度分布规律是，由加料斗向喷嘴方向逐渐升高。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
按照完成零件制造过程中采用的不同工艺方法，（ ）可以分为铸造、锻造、冲压、焊接、热处理、机械加工、表面处理和装配等工艺过程。<reserved_107>工艺过程</s><s> <reserved_106>判断以下陈述是否正确。
当没有保险丝时，可以普通铜线代替保险丝。<reserved_107>错误</s><s> <reserved_106>回答下列问题
关于量块的使用方法，下列说法中错误的是（ ）。<reserved_107>使用后为了保护测量面不碰伤，应使量块结合在一起存放</s><s> <reserved_106>回答下列问题
模具闭合高度是（ ）<reserved_107>模具在最低工作位置时，模具的凸模.凹模之间的距离</s><s> <reserved_106>回答下列问题
划阿基米德螺旋线常采用的方法有（ ）。<reserved_107>逐点划线法;圆弧划线法;分段作圆弧法</s><s> <reserved_106>判断以下陈述是否正确。
塑料制品在充模流动方向上和垂直流动方向上的收缩率是一样的。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
化学稳定性很好，俗称不腐胶是（ ）。<reserved_107>聚四氟乙烯</s><s> <reserved_106>判断以下陈述是否正确。
数控机床按控制方式不同可分为：开环控制系统、闭环控制系统、半闭环控制系统。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
推杆推出机构的特点有推出效果好、制造简单、更换方便、（ ）。<reserved_107>易实现自动化</s><s> <reserved_106>回答下列问题
用电设备的安秒特性表明，当负载电流超过额定值，用电器将（ ）。<reserved_107>延迟损坏</s><s> <reserved_106>回答下列问题
压注模分流道截面形状宜取相同截面积时周边最长的形状（如梯形），不利于（ ）。<reserved_107>节约原料</s><s> <reserved_106>回答下列问题
控制送料步距，称为（ ）<reserved_107>挡料</s><s> <reserved_106>填入正确答案
当机件上的倾斜部分具有明显的回转轴线时，可采用（ ）。<reserved_107>旋转视图</s><s> <reserved_106>回答下列问题
切削运动可分为主运动与进给运动。关于进给运动，（ ）的描述是不正确的。<reserved_107>进给运动只有且必须有一个</s><s> <reserved_106>回答下列问题
压边圈不起压边作用会使（ ）<reserved_107>制品边缘折皱</s><s> <reserved_106>回答下列问题
固定基准平面是相对于（ ）建立的：<reserved_107>模型空间</s><s> <reserved_106>判断以下陈述是否正确。
零件的清理是清除零件上残存的铁锈，特别是要仔细清除小孔、沟槽等易存杂物的角落。<reserved_107>错误</s><s> <reserved_106>回答下列问题
下列选项中，关于型芯结构形式说法正确的是（ ）。<reserved_107>型芯一般是成型制品中较大的主要内型的成型零件;成型杆一般指成型制品上较小孔的成型零件;固定部分为圆形而成型部分为非圆形的型芯须止转;复杂形状的型芯应采用拼块组合以简化加工工艺;组合式型芯的拼接必须牢靠严密</s><s> <reserved_106>判断以下陈述是否正确。
模具工作部分尺寸及公差的计算方法与加工方法无关<reserved_107>错误</s><s> <reserved_106>回答下列问题
电火花加工放电所在的液体介质必须具有一定的（ ）。<reserved_107>绝缘性能</s><s> <reserved_106>判断以下陈述是否正确。
塑件的侧面带有侧凸或者侧凹的结构才要用侧向抽芯机构。<reserved_107>正确</s><s> <reserved_106>回答下列问题
测量样板或对模具轮廓检验一般采用（ ）、极坐标测量法或采用光学接触法测量。<reserved_107>直角坐标测量法</s><s> <reserved_106>回答下列问题
在正弦分中夹具上无法用平面磨床完成不同轴圆弧面的磨削，其原因是（ ）。<reserved_107>没有安装正弦圆柱的空间</s><s> <reserved_106>回答下列问题
机械零件的真实大小是以图样上的（ ）为依据。<reserved_107>尺寸数值</s><s> <reserved_106>回答下列问题
减小压边力是为了解决（ ）<reserved_107>材料承受的径向拉应力</s><s> <reserved_106>回答下列问题
工艺基准分为（ ）、测量和装配基准。<reserved_107>定位</s><s> <reserved_106>回答下列问题
以下不是表面粗糙度检测方法的是（ ）。<reserved_107>目测法</s><s> <reserved_106>判断以下陈述是否正确。
凸模固定板材料一般采用T8A。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
分型面形状常用的为（ ）。<reserved_107>阶梯形;U形</s><s> <reserved_106>回答下列问题
表示板料弯曲变形程度大小的参数是（ ）。<reserved_107>r/t</s><s> <reserved_106>回答下列问题
（ ）一般不作为导柱、导套的材料。<reserved_107>3Cr13</s><s><reserved_106>回答下列问题
双分型面注射模一个分型面取出塑件，另一个分型面取出（
11/18 06:58:21 - mmengine - INFO - before_train in EvaluateChatHook .
11/18 06:58:32 - mmengine - INFO - Sample output:
<reserved_106>请给我介绍五个上海的景点<reserved_107>当然可以！以下是五个上海著名的旅游景点：

1. 外滩：外滩是上海最著名的景点之一，位于黄浦江畔。这里有许多历史悠久的建筑，如和平饭店、上海海关大楼等。在外滩，您可以欣赏到

11/18 06:58:37 - mmengine - INFO - Sample output:
 <reserved_106>Please tell me five scenic spots in Shanghai<reserved_107>1. The Bund (Shanghai)
2. The Oriental Pearl Tower (Shanghai)
3. The People's Square (Shanghai)
4. The Shanghai Zoo (Shanghai)
5. The Shanghai Ocean Park (Shanghai)</s>

11/18 06:58:37 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
11/18 06:58:37 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
11/18 06:58:37 - mmengine - INFO - Checkpoints will be saved to /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37.
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Traceback (most recent call last):
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/tools/train.py", line 246, in <module>
    main()
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/tools/train.py", line 242, in main
    runner.train()
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/runner/runner.py", line 1777, in train
    model = self.train_loop.run()  # type: ignore
            ^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/runner/loops.py", line 96, in run
    self.run_epoch()
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/runner/loops.py", line 112, in run_epoch
    self.run_iter(idx, data_batch)
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/runner/loops.py", line 128, in run_iter
    outputs = self.runner.model.train_step(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/model/base_model/base_model.py", line 114, in train_step
    losses = self._run_forward(data, mode='loss')  # type: ignore
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/model/base_model/base_model.py", line 346, in _run_forward
    results = self(**data, mode=mode)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/model/sft.py", line 82, in forward
    return self.compute_loss(data, data_samples)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/model/sft.py", line 102, in compute_loss
    outputs = self.llm(**data)
              ^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/peft/peft_model.py", line 918, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 94, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yinxiaoln/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py", line 684, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yinxiaoln/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py", line 453, in forward
    layer_outputs = torch.utils.checkpoint.checkpoint(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/home/yinxiaoln/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py", line 449, in custom_forward
    return module(*inputs, output_attentions, None)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yinxiaoln/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat/modeling_baichuan.py", line 273, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/model/modules/baichuan.py", line 43, in baichuan_7b_attn_forward
    proj = self.W_pack(hidden_states)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/peft/tuners/lora.py", line 1208, in forward
    result = super().forward(x)
             ^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/bitsandbytes/nn/modules.py", line 248, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py", line 579, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, state).to(A.dtype).t(), bias)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.00 GiB. GPU 0 has a total capacty of 44.35 GiB of which 2.63 GiB is free. Process 37074 has 26.16 GiB memory in use. Including non-PyTorch memory, this process has 15.53 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
+ test
+ xtuner_config=/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py
+ pth_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.pth
+ hf_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.hf
+ xtuner convert pth_to_hf /data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.pth /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.hf
[2023-11-18 06:58:45,364] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 06:58:50,067] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.12s/it]
11/18 06:59:12 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 06:59:12 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 06:59:12 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 06:59:12 - mmengine - INFO - dispatch baichuan2-13B attn forward
Traceback (most recent call last):
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/tools/model_converters/pth_to_hf.py", line 108, in <module>
    main()
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/tools/model_converters/pth_to_hf.py", line 87, in main
    state_dict = guess_load_checkpoint(args.pth_model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/xtuner/tools/model_converters/pth_to_hf.py", line 66, in guess_load_checkpoint
    raise FileNotFoundError(f'Cannot find {pth_model}')
FileNotFoundError: Cannot find /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.pth
+ python merge.py /data/yinxiaoln/pre_models/Baichuan2-7B-Chat /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.hf /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/merged.model
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.14s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.37s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.08s/it]
Traceback (most recent call last):
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/peft/config.py", line 180, in _get_peft_type
    config_file = hf_hub_download(
                  ^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 110, in _inner_fn
    validate_repo_id(arg_value)
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 158, in validate_repo_id
    raise HFValidationError(
huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.hf'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/merge.py", line 58, in <module>
    main()
  File "/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/merge.py", line 42, in main
    model_unmerged = PeftModel.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/peft/peft_model.py", line 251, in from_pretrained
    PeftConfig._get_peft_type(
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/peft/config.py", line 186, in _get_peft_type
    raise ValueError(f"Can't find '{CONFIG_NAME}' at '{model_id}'")
ValueError: Can't find 'adapter_config.json' at '/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/epoch_10.hf'
+ python xtuner_test.py /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/output.json /data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/merged.model
2023-11-18 07:00:36,411 - modelscope - INFO - PyTorch version 2.1.0 Found.
2023-11-18 07:00:36,411 - modelscope - INFO - Loading ast index from /home/yinxiaoln/.cache/modelscope/ast_indexer
2023-11-18 07:00:36,440 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 3cdb0bb552f9ef7c5b6c350d835395e7 and a total number of 941 components indexed
Traceback (most recent call last):
  File "/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/xtuner_test.py", line 23, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_path, device_map="cuda:0",
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 686, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 519, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/transformers/utils/hub.py", line 429, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 110, in _inner_fn
    validate_repo_id(arg_value)
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 158, in validate_repo_id
    raise HFValidationError(
huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/yinxiaoln/save/Baichuan2/2023-11-18_06:56:37/merged.model'. Use `repo_type` argument if needed.
+ BASE_DIR=/data/yinxiaoln
++ date +%Y-%m-%d_%H:%M:%S
+ SAVE_PATH=2023-11-18_07:32:32
+ main
+ train
+ work_dir=/data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32
+ CUDA_VISIBLE_DEVICES=1
+ xtuner train baichuan_xtuner_config.py --work-dir /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32
[2023-11-18 07:32:37,063] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 07:32:41,319] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/18 07:32:42 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
    CUDA available: True
    numpy_random_seed: 514433139
    GPU 0: NVIDIA A40
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.3, V12.3.52
    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0+cu121
    OpenCV: 4.8.1
    MMEngine: 0.9.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 514433139
    deterministic: False
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

11/18 07:32:43 - mmengine - INFO - Config:
BASE_PATH = '/data/yinxiaoln/'
accumulative_counts = 16
alpaca_en = dict(
    dataset=dict(
        data_files=dict(
            train=
            '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
        ),
        path='json',
        type='datasets.load_dataset'),
    dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
    max_length=2048,
    pack_to_max_length=True,
    remove_unused_columns=True,
    shuffle_before_pack=True,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.process_hf_dataset')
alpaca_en_path = '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
batch_size = 4
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.DatasetInfoHook'),
    dict(
        evaluation_inputs=[
            '请给我介绍五个上海的景点',
            'Please tell me five scenic spots in Shanghai',
        ],
        every_n_iters=500,
        instruction=
        'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat.INSTRUCTION_START',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.EvaluateChatHook'),
]
dataloader_num_workers = 0
default_hooks = dict(
    checkpoint=dict(interval=1, type='mmengine.hooks.CheckpointHook'),
    logger=dict(interval=10, type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_inputs = [
    '请给我介绍五个上海的景点',
    'Please tell me five scenic spots in Shanghai',
]
launcher = 'none'
load_from = None
log_level = 'INFO'
lr = 0.0002
max_epochs = 10
max_length = 2048
max_norm = 1
model = dict(
    llm=dict(
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        quantization_config=dict(
            bnb_4bit_compute_dtype='torch.float16',
            bnb_4bit_quant_type='nf4',
            bnb_4bit_use_double_quant=True,
            llm_int8_has_fp16_weight=False,
            llm_int8_threshold=6.0,
            load_in_4bit=True,
            load_in_8bit=False,
            type='transformers.BitsAndBytesConfig'),
        torch_dtype='torch.float16',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    lora=dict(
        bias='none',
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        task_type='CAUSAL_LM',
        type='peft.LoraConfig'),
    type='xtuner.model.SupervisedFinetune')
optim_type = 'bitsandbytes.optim.PagedAdamW32bit'
optim_wrapper = dict(
    accumulative_counts=16,
    clip_grad=dict(error_if_nonfinite=False, max_norm=1),
    dtype='float16',
    loss_scale='dynamic',
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=0.0002,
        type='bitsandbytes.optim.PagedAdamW32bit',
        weight_decay=0),
    type='mmengine.optim.AmpOptimWrapper')
pack_to_max_length = True
param_scheduler = dict(
    T_max=10,
    by_epoch=True,
    convert_to_iter_based=True,
    eta_min=2e-05,
    type='mmengine.optim.CosineAnnealingLR')
pretrained_model_name_or_path = '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat'
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(by_epoch=True, max_epochs=10, val_interval=1)
train_dataloader = dict(
    batch_size=4,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        dataset=dict(
            data_files=dict(
                train=
                '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
            ),
            path='json',
            type='datasets.load_dataset'),
        dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
        max_length=2048,
        pack_to_max_length=True,
        remove_unused_columns=True,
        shuffle_before_pack=True,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.process_hf_dataset'),
    num_workers=0,
    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))
visualizer = None
weight_decay = 0
work_dir = '/data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32'

quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
11/18 07:32:43 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.66s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.35s/it]
11/18 07:33:04 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 07:33:04 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 07:33:04 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 07:33:04 - mmengine - INFO - dispatch baichuan2-13B attn forward
11/18 07:34:01 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
11/18 07:34:02 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
Found cached dataset json (/home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 663.13it/s]
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-817222a3ff087594.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-efe2e5113f9035af.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7868ac168b99dcee.arrow
Flattening the indices:   0%|          | 0/5256 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 5256/5256 [00:00<00:00, 50498.48 examples/s]                                                                                     Map:   0%|          | 0/5256 [00:00<?, ? examples/s]Map:  76%|███████▌  | 4000/5256 [00:00<00:00, 27873.99 examples/s]                                                                  11/18 07:34:03 - mmengine - WARNING - Dataset Dataset has no metainfo. ``dataset_meta`` in visualizer will be None.
11/18 07:34:03 - mmengine - INFO - Num train samples 80
11/18 07:34:03 - mmengine - INFO - train example:
11/18 07:34:03 - mmengine - INFO - <s> <reserved_106>回答下列问题
影响热塑性塑料收缩性的因素很多，下面哪个结论错误（ ）。<reserved_107>在同一塑件的不同部位收缩率是一样的</s><s> <reserved_106>判断以下陈述是否正确。
V形块斜槽的对称轴心可用标准心轴模拟。<reserved_107>正确</s><s> <reserved_106>回答下列问题
在单件和小批生产中，零件在洗涤槽内用棉纱或泡沫塑料擦洗或进行（ ）。<reserved_107>冲洗</s><s> <reserved_106>判断以下陈述是否正确。
划线时用来确定工件各部分尺寸、几何形状及相对位置的依据称为划线基准。<reserved_107>正确</s><s> <reserved_106>回答下列问题
当（ ）时，模板相关孔系的加工采用配加工法。<reserved_107>一块模板为非淬火件，另一块模板为淬火件</s><s> <reserved_106>填入正确答案
在过载时可产生打滑，应采用（ ）。<reserved_107>带传动</s><s> <reserved_106>回答下列问题
适用于处理各种高碳钢工具.模具.滚动轴承以及渗碳和表面淬火的零件回火方法是（ ）<reserved_107>低温回火</s><s> <reserved_106>回答下列问题
NX是UGS公司推出的（ ）一体化软件。<reserved_107>CAD/CAE/CAM</s><s> <reserved_106>回答下列问题
属于形位公差的检测是（ ）。<reserved_107>垂直度</s><s> <reserved_106>回答下列问题
直接浇口应用于（ ）。<reserved_107>单型腔模具</s><s> <reserved_106>回答下列问题
在拟定机械加工工艺过程、安排工艺顺序时，首要考虑的问题是（ ）。<reserved_107>精度要求高的工件表面加工问题</s><s> <reserved_106>判断以下陈述是否正确。
夹具设计在保证夹具结构具有良好的工艺性的同时，也要便于排屑，便于工件装夹。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
一般情况下，从拉深变形的特点考虑，拉深模的凹模的圆角表面粗糙度应比凸模的圆角表面粗糙度小些。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
在形位公差符号中，符号“◎”表示（ ）。<reserved_107>同轴度</s><s> <reserved_106>回答下列问题
丝锥的校准部分磨损时，应修磨丝锥的（ ）。<reserved_107>前面</s><s> <reserved_106>回答下列问题
利用（ ）从工件上切去多余的金属，以获得符合要求的零件，这种加工方法称为金属切削加工。<reserved_107>刀具</s><s> <reserved_106>判断以下陈述是否正确。
沿引导线扫略时，引导线串必须为相切连续的曲线。<reserved_107>错误</s><s> <reserved_106>回答下列问题
一个（ ）可同时被工程图、装配、加工、机构分析和有限元分析等应用模块引用。<reserved_107>主模型</s><s> <reserved_106>回答下列问题
材料报表又称BOM表，用于详细地列出（ ）。<reserved_107>各零件的状态、参数</s><s> <reserved_106>判断以下陈述是否正确。
螺纹连接是一种可拆的固定连接，它具有结构简单.连接可靠.装拆方便等优点在机械中应用广泛<reserved_107>正确</s><s> <reserved_106>回答下列问题
为了保证制件和废料能顺利的卸下和顶出，冲裁模的卸料装置和顶料装置的装配应（ ）<reserved_107>正确而灵活</s><s> <reserved_106>回答下列问题
在注射模中，水嘴一般根据（ ）选用。<reserved_107>冷却水道孔径大小</s><s> <reserved_106>回答下列问题
铅垂线的三面投影中，能真实反映线段实长的投影为（ ）。<reserved_107>正面投影;侧面投影</s><s> <reserved_106>回答下列问题
在电火花加工中，若采用“钢打钢”直接配合法，则该机床应具有（ ）。<reserved_107>高低压复合脉冲电源</s><s> <reserved_106>判断以下陈述是否正确。
零件只要能够加工出来，并能够满足零件的使用要求，就说明零件的结构工艺性良好。<reserved_107>错误</s><s> <reserved_106>回答下列问题
深孔加工有以下特点（ ）。<reserved_107>刀具刀杆细长，刚性差</s><s> <reserved_106>判断以下陈述是否正确。
计算机辅助编程生成的刀具轨迹就是数控加工程序。<reserved_107>错误</s><s> <reserved_106>回答下列问题
当两轴相距较远且要求传动准确，应采用（ ）。<reserved_107>轮系传动</s><s> <reserved_106>回答下列问题
铰孔的切削速度比钻孔切削速度（ ）。<reserved_107>小</s><s> <reserved_106>回答下列问题
气辅模具调试时，应注意的要点，下列哪一项是错误的（ ）。<reserved_107>气辅模具都应该设置溢料井</s><s> <reserved_106>判断以下陈述是否正确。
同一公称直径的粗牙、细牙螺纹，由于细牙螺纹的螺距小，因此强度高，自锁性能好，应用更广。<reserved_107>错误</s><s> <reserved_106>回答下列问题
属于压力机类型分类的是（ ）<reserved_107>双动压力机用拉深模</s><s> <reserved_106>回答下列问题
F11125型是铣床上常用的一种（ ）。<reserved_107>万能分度头</s><s> <reserved_106>回答下列问题
工业固体废物化学处理方法包括（ ）。<reserved_107>氧化;还原;中和;化学沉淀</s><s> <reserved_106>回答下列问题
指出下面哪个不是进行注射模型腔数目计算的常用方法（ ）。<reserved_107>根据模具的制造复杂程度</s><s> <reserved_106>回答下列问题
适当提高钢中的（ ）有利于改善钢的切削性能。<reserved_107>硫;磷</s><s> <reserved_106>判断以下陈述是否正确。
职业纪律包括劳动纪律、保密纪律、财经纪律、组织纪律等。<reserved_107>正确</s><s> <reserved_106>回答下列问题
.对于电火花加工，我国通常把工件接脉冲电源的正极（ ）加工。<reserved_107>正极性</s><s> <reserved_106>回答下列问题
低压力损耗的角度出发，宜选用（ ）分流道。<reserved_107>圆形</s><s> <reserved_106>回答下列问题
任何一个未被约束的物体，在空间具有进行（ ）种运动的可能性。<reserved_107>六</s><s> <reserved_106>回答下列问题
拉深间隙太小的调整方法是（ ）<reserved_107>加大拉深间隙</s><s> <reserved_106>回答下列问题
以下属于投影法分类的是（ ）<reserved_107>中心投影法</s><s> <reserved_106>判断以下陈述是否正确。
金属切削加工时，提高背吃刀量可以有效降低切削温度。<reserved_107>错误</s><s> <reserved_106>回答下列问题
表面质量对零件使用性能的影响表现在对零件的（ ）影响等方面。<reserved_107>耐磨性;疲劳强度;耐腐蚀性;配合性质</s><s> <reserved_106>回答下列问题
国标中规定的几种图纸幅面中，幅面最大的是（ ）。<reserved_107>A0</s><s> <reserved_106>判断以下陈述是否正确。
居民家庭用的保险丝可用铜丝代替<reserved_107>错误</s><s> <reserved_106>回答下列问题
关于轴测图，下列说法正确的有（ ）。<reserved_107>轴测图即是人们常说的立体图;轴测图主要用于工程上的辅助图样;用平行投影法将物体投射到单一投影面上所得的具有立体感的图形，称为轴测投影图， 简称轴测图</s><s> <reserved_106>回答下列问题
适用于形状复杂或精度要求较高的冲压件，但刃口修磨量大，总寿命低的是（ ）<reserved_107>直壁式孔口</s><s> <reserved_106>回答下列问题
液压马达是液压系统的（ ）元件<reserved_107>执行</s><s> <reserved_106>回答下列问题
以下不属于简易冲模常见应用领域的是（ ）。<reserved_107>汽车覆盖件</s><s> <reserved_106>填入正确答案
有间隙的冲孔模，孔的尺寸应等于（ ）模尺寸。<reserved_107>凸</s><s> <reserved_106>回答下列问题
推杆与其配合孔的配合常采用（ ）配合。<reserved_107>间隙</s><s> <reserved_106>回答下列问题
冲裁模刃口相碰的原因（ ）<reserved_107>凸模.导柱等零件安装不垂直</s><s> <reserved_106>回答下列问题
特征操作是对已存的实体或特征进行操作以满足设计要求，相当于（ ）过程。<reserved_107>精加工</s><s> <reserved_106>回答下列问题
（ ）模具装配工艺方法不包括以下哪种？<reserved_107>随意装配法</s><s> <reserved_106>判断以下陈述是否正确。
砂轮是由磨料用结合剂粘结在一起培烧而成的紧致多孔体<reserved_107>错误</s><s> <reserved_106>回答下列问题
化学表面热处理的主要特征是通过（ ）使某些元素渗入工件表面<reserved_107>加热</s><s> <reserved_106>回答下列问题
常用定位方法有（ ）种<reserved_107>三</s><s> <reserved_106>回答下列问题
针阀式流道漏料的原因以下正确的是（ ）。<reserved_107>热嘴上驱动缸的密封件损坏</s><s> <reserved_106>判断以下陈述是否正确。
浇口的位置如果设计在产品壁厚较厚的部位，有可能造成产品出现填充不足。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
某一表面在一道工序中所切除的金属层深度称为（ ）。<reserved_107>工序余量</s><s> <reserved_106>回答下列问题
以下不属于国家规定的配合类型的是（ ）<reserved_107>精确配合</s><s> <reserved_106>回答下列问题
钻孔加工的表面粗糙度值是（ ）。<reserved_107>Ra12.5～3.2μm</s><s> <reserved_106>回答下列问题
用百分表检测法检测两孔的同轴度，其检测精度取决于（ ）。<reserved_107>百分表的精度</s><s> <reserved_106>回答下列问题
安全文化的核心是树立（ ）)的价值观念，真正做到“安全第一，预防为主”。<reserved_107>以人为本</s><s> <reserved_106>回答下列问题
螺纹标记M20×1.5-5g6g表示（ ）。<reserved_107>普通细牙螺纹;螺距1.5mm;右旋</s><s> <reserved_106>判断以下陈述是否正确。
表达式既可以用于控制模型内部的尺寸及尺寸与尺寸之间的关系，也可以控制装配件中零件之间的尺寸关系。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
V带传动常用于机械传动的高速级。<reserved_107>正确</s><s><reserved_106>回答下列问题
锯割线应和钳口边缘平行，并夹在台虎钳的（ ）
11/18 07:34:03 - mmengine - INFO - before_train in EvaluateChatHook .
11/18 07:34:11 - mmengine - INFO - Sample output:
<reserved_106>请给我介绍五个上海的景点<reserved_107>当然可以！以下是五个上海著名的旅游景点：

1. 外滩：外滩是上海最著名的景点之一，位于黄浦江畔。这里有许多历史悠久的建筑，如和平饭店、上海海关大楼等。在外滩，您可以欣赏到

11/18 07:34:15 - mmengine - INFO - Sample output:
 <reserved_106>Please tell me five scenic spots in Shanghai<reserved_107>1. The Bund (Shanghai)
2. The Oriental Pearl Tower (Shanghai)
3. The People's Square (Shanghai)
4. The Shanghai Aquarium (Shanghai)
5. The Shanghai Science and Technology Museum (Shanghai)</s>

11/18 07:34:15 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
11/18 07:34:15 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
11/18 07:34:15 - mmengine - INFO - Checkpoints will be saved to /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32.
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/optim/scheduler/param_scheduler.py:198: UserWarning: Detected call of `scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `scheduler.step()`. Failure to do this will result in PyTorch skipping the first value of the parameter value schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
11/18 07:36:08 - mmengine - INFO - Epoch(train)  [1][10/20]  lr: 1.9910e-04  eta: 0:35:48  time: 11.3086  data_time: 0.0109  memory: 24015  loss: 3.5014
11/18 07:38:01 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 07:38:01 - mmengine - INFO - Epoch(train)  [1][20/20]  lr: 1.9602e-04  eta: 0:33:55  time: 11.3102  data_time: 0.0106  memory: 24016  loss: 3.3550  grad_norm: 1.4311
11/18 07:38:01 - mmengine - INFO - Saving checkpoint at 1 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 07:39:57 - mmengine - INFO - Epoch(train)  [2][10/20]  lr: 1.9082e-04  eta: 0:31:58  time: 11.2397  data_time: 0.0106  memory: 24016  loss: 3.0540  grad_norm: 1.4311
11/18 07:41:50 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 07:41:50 - mmengine - INFO - Epoch(train)  [2][20/20]  lr: 1.8363e-04  eta: 0:30:04  time: 11.2488  data_time: 0.0100  memory: 24016  loss: 2.8234  grad_norm: 1.0717
11/18 07:41:50 - mmengine - INFO - Saving checkpoint at 2 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 07:43:45 - mmengine - INFO - Epoch(train)  [3][10/20]  lr: 1.7463e-04  eta: 0:28:10  time: 11.2489  data_time: 0.0102  memory: 24016  loss: 2.7777  grad_norm: 0.8632
11/18 07:45:38 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 07:45:38 - mmengine - INFO - Epoch(train)  [3][20/20]  lr: 1.6404e-04  eta: 0:26:17  time: 11.2386  data_time: 0.0098  memory: 24016  loss: 2.6607  grad_norm: 0.8632
11/18 07:45:38 - mmengine - INFO - Saving checkpoint at 3 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 07:47:33 - mmengine - INFO - Epoch(train)  [4][10/20]  lr: 1.5211e-04  eta: 0:24:24  time: 11.2482  data_time: 0.0102  memory: 24016  loss: 2.5436  grad_norm: 0.7448
11/18 07:49:25 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 07:49:25 - mmengine - INFO - Epoch(train)  [4][20/20]  lr: 1.3915e-04  eta: 0:22:31  time: 11.2455  data_time: 0.0098  memory: 24016  loss: 2.5759  grad_norm: 0.6543
11/18 07:49:25 - mmengine - INFO - Saving checkpoint at 4 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 07:51:21 - mmengine - INFO - Epoch(train)  [5][10/20]  lr: 1.2547e-04  eta: 0:20:38  time: 11.2404  data_time: 0.0105  memory: 24016  loss: 2.4999  grad_norm: 0.6543
11/18 07:53:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 07:53:14 - mmengine - INFO - Epoch(train)  [5][20/20]  lr: 1.1141e-04  eta: 0:18:45  time: 11.2504  data_time: 0.0097  memory: 24016  loss: 2.4196  grad_norm: 0.5806
11/18 07:53:14 - mmengine - INFO - Saving checkpoint at 5 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 07:55:09 - mmengine - INFO - Epoch(train)  [6][10/20]  lr: 9.7319e-05  eta: 0:16:53  time: 11.2386  data_time: 0.0101  memory: 24016  loss: 2.4250  grad_norm: 0.5806
11/18 07:57:02 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 07:57:02 - mmengine - INFO - Epoch(train)  [6][20/20]  lr: 8.3536e-05  eta: 0:15:00  time: 11.2497  data_time: 0.0097  memory: 24016  loss: 2.4092  grad_norm: 0.5297
11/18 07:57:02 - mmengine - INFO - Saving checkpoint at 6 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 07:58:57 - mmengine - INFO - Epoch(train)  [7][10/20]  lr: 7.0405e-05  eta: 0:13:07  time: 11.2512  data_time: 0.0100  memory: 24016  loss: 2.4342  grad_norm: 0.4898
11/18 08:00:50 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 08:00:50 - mmengine - INFO - Epoch(train)  [7][20/20]  lr: 5.8250e-05  eta: 0:11:15  time: 11.2425  data_time: 0.0096  memory: 24016  loss: 2.3479  grad_norm: 0.4898
11/18 08:00:50 - mmengine - INFO - Saving checkpoint at 7 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 08:02:45 - mmengine - INFO - Epoch(train)  [8][10/20]  lr: 4.7368e-05  eta: 0:09:22  time: 11.2517  data_time: 0.0101  memory: 24016  loss: 2.4213  grad_norm: 0.4561
11/18 08:04:38 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 08:04:38 - mmengine - INFO - Epoch(train)  [8][20/20]  lr: 3.8028e-05  eta: 0:07:30  time: 11.2451  data_time: 0.0096  memory: 24016  loss: 2.3169  grad_norm: 0.4284
11/18 08:04:38 - mmengine - INFO - Saving checkpoint at 8 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 08:06:33 - mmengine - INFO - Epoch(train)  [9][10/20]  lr: 3.0461e-05  eta: 0:05:37  time: 11.2437  data_time: 0.0101  memory: 24016  loss: 2.3126  grad_norm: 0.4284
11/18 08:08:26 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 08:08:26 - mmengine - INFO - Epoch(train)  [9][20/20]  lr: 2.4852e-05  eta: 0:03:45  time: 11.2545  data_time: 0.0098  memory: 24016  loss: 2.3960  grad_norm: 0.3031
11/18 08:08:26 - mmengine - INFO - Saving checkpoint at 9 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 08:10:21 - mmengine - INFO - Epoch(train) [10][10/20]  lr: 2.1340e-05  eta: 0:01:52  time: 11.2466  data_time: 0.0124  memory: 24016  loss: 2.3451  grad_norm: 0.3031
11/18 08:12:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_073242
11/18 08:12:14 - mmengine - INFO - Epoch(train) [10][20/20]  lr: 2.0011e-05  eta: 0:00:00  time: 11.2605  data_time: 0.0100  memory: 24016  loss: 2.3446  grad_norm: 0.2206
11/18 08:12:14 - mmengine - INFO - Saving checkpoint at 10 epochs
+ test
+ xtuner_config=/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py
+ pth_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.pth
+ hf_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.hf
+ xtuner convert pth_to_hf /data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.pth /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.hf
[2023-11-18 08:12:23,772] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 08:12:28,420] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.61s/it]
11/18 08:12:48 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 08:12:48 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 08:12:48 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 08:12:48 - mmengine - INFO - dispatch baichuan2-13B attn forward
Load PTH model from /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.pth
Convert weights to float16
Saving HuggingFace model to /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.hf
All done!
+ python merge.py /data/yinxiaoln/pre_models/Baichuan2-7B-Chat /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/epoch_10.hf /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/merged.model
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.04s/it]
Saving to /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/merged.model...
All done!
+ python xtuner_test.py /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/output.json /data/yinxiaoln/save/Baichuan2/2023-11-18_07:32:32/merged.model
2023-11-18 08:16:57,583 - modelscope - INFO - PyTorch version 2.1.0 Found.
2023-11-18 08:16:57,583 - modelscope - INFO - Loading ast index from /home/yinxiaoln/.cache/modelscope/ast_indexer
2023-11-18 08:16:57,694 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 3cdb0bb552f9ef7c5b6c350d835395e7 and a total number of 941 components indexed
Traceback (most recent call last):
  File "/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/xtuner_test.py", line 23, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_path, device_map="cuda:0",
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 719, in from_pretrained
    tokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 497, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module.replace(".py", ""))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 199, in get_class_in_module
    module = importlib.import_module(module_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1126, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1126, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'transformers_modules.merged'
+ BASE_DIR=/data/yinxiaoln
++ date +%Y-%m-%d_%H-%M-%S
+ SAVE_PATH=2023-11-18_08-52-18
+ main
+ train
+ work_dir=/data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18
+ CUDA_VISIBLE_DEVICES=0
+ xtuner train baichuan_xtuner_config.py --work-dir /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18
[2023-11-18 08:52:23,215] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 08:52:27,296] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/18 08:52:28 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
    CUDA available: True
    numpy_random_seed: 1194916004
    GPU 0: NVIDIA A40
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.3, V12.3.52
    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0+cu121
    OpenCV: 4.8.1
    MMEngine: 0.9.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1194916004
    deterministic: False
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

11/18 08:52:28 - mmengine - INFO - Config:
BASE_PATH = '/data/yinxiaoln/'
accumulative_counts = 16
alpaca_en = dict(
    dataset=dict(
        data_files=dict(
            train=
            '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
        ),
        path='json',
        type='datasets.load_dataset'),
    dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
    max_length=2048,
    pack_to_max_length=True,
    remove_unused_columns=True,
    shuffle_before_pack=True,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.process_hf_dataset')
alpaca_en_path = '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
batch_size = 1
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.DatasetInfoHook'),
    dict(
        evaluation_inputs=[
            '请给我介绍五个上海的景点',
            'Please tell me five scenic spots in Shanghai',
        ],
        every_n_iters=500,
        instruction=
        'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat.INSTRUCTION_START',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.EvaluateChatHook'),
]
dataloader_num_workers = 0
default_hooks = dict(
    checkpoint=dict(interval=1, type='mmengine.hooks.CheckpointHook'),
    logger=dict(interval=10, type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_inputs = [
    '请给我介绍五个上海的景点',
    'Please tell me five scenic spots in Shanghai',
]
launcher = 'none'
load_from = None
log_level = 'INFO'
lr = 0.0002
max_epochs = 10
max_length = 2048
max_norm = 1
model = dict(
    llm=dict(
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        quantization_config=dict(
            bnb_4bit_compute_dtype='torch.float16',
            bnb_4bit_quant_type='nf4',
            bnb_4bit_use_double_quant=True,
            llm_int8_has_fp16_weight=False,
            llm_int8_threshold=6.0,
            load_in_4bit=True,
            load_in_8bit=False,
            type='transformers.BitsAndBytesConfig'),
        torch_dtype='torch.float16',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    lora=dict(
        bias='none',
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        task_type='CAUSAL_LM',
        type='peft.LoraConfig'),
    type='xtuner.model.SupervisedFinetune')
optim_type = 'bitsandbytes.optim.PagedAdamW32bit'
optim_wrapper = dict(
    accumulative_counts=16,
    clip_grad=dict(error_if_nonfinite=False, max_norm=1),
    dtype='float16',
    loss_scale='dynamic',
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=0.0002,
        type='bitsandbytes.optim.PagedAdamW32bit',
        weight_decay=0),
    type='mmengine.optim.AmpOptimWrapper')
pack_to_max_length = True
param_scheduler = dict(
    T_max=10,
    by_epoch=True,
    convert_to_iter_based=True,
    eta_min=2e-05,
    type='mmengine.optim.CosineAnnealingLR')
pretrained_model_name_or_path = '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat'
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(by_epoch=True, max_epochs=10, val_interval=1)
train_dataloader = dict(
    batch_size=1,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        dataset=dict(
            data_files=dict(
                train=
                '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
            ),
            path='json',
            type='datasets.load_dataset'),
        dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
        max_length=2048,
        pack_to_max_length=True,
        remove_unused_columns=True,
        shuffle_before_pack=True,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.process_hf_dataset'),
    num_workers=0,
    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))
visualizer = None
weight_decay = 0
work_dir = '/data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18'

quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
11/18 08:52:29 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:17<00:17, 17.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 12.55s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.34s/it]
11/18 08:52:56 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 08:52:56 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 08:52:56 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 08:52:56 - mmengine - INFO - dispatch baichuan2-13B attn forward
11/18 08:53:48 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
11/18 08:53:48 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
Found cached dataset json (/home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 710.78it/s]
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-817222a3ff087594.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-efe2e5113f9035af.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7868ac168b99dcee.arrow
Flattening the indices:   0%|          | 0/5256 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 5256/5256 [00:00<00:00, 49825.54 examples/s]                                                                                     Map:   0%|          | 0/5256 [00:00<?, ? examples/s]Map:  76%|███████▌  | 4000/5256 [00:00<00:00, 28956.09 examples/s]                                                                  11/18 08:53:50 - mmengine - WARNING - Dataset Dataset has no metainfo. ``dataset_meta`` in visualizer will be None.
11/18 08:53:50 - mmengine - INFO - Num train samples 80
11/18 08:53:50 - mmengine - INFO - train example:
11/18 08:53:50 - mmengine - INFO - <s> <reserved_106>回答下列问题
划线不准确或锉削时检查测量有误差，锉削量太大会造成（ ）。<reserved_107>尺寸不准确</s><s> <reserved_106>回答下列问题
在单件和小批生产中，零件在洗涤槽内用棉纱或泡沫塑料擦洗或进行（ ）。<reserved_107>冲洗</s><s> <reserved_106>判断以下陈述是否正确。
压力机应根据冲压工序的性质，生产批量的大小。模具的外形尺寸以及现有设备等情况进行选择<reserved_107>正确</s><s> <reserved_106>回答下列问题
不属于溢式压缩模的特点是（ ）？<reserved_107>设计成本高</s><s> <reserved_106>判断以下陈述是否正确。
加工是一种电化学加工方法。<reserved_107>正确。</s><s> <reserved_106>判断以下陈述是否正确。
工件必须留有足够的装夹余量，比较大的工件还得有两个支承面，不能悬臂<reserved_107>正确</s><s> <reserved_106>回答下列问题
在滚动轴承中，以下各零件，需要采用含铬的合金钢为材料的是（ ）。<reserved_107>内圈;外圈;滚动体</s><s> <reserved_106>判断以下陈述是否正确。
为了顺利使制品脱离模具，可以使用较多的含有硅油的脱模剂。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
当需表达机件内部部分结构，需选择（ ）来表达。<reserved_107>局部剖视</s><s> <reserved_106>回答下列问题
注射机喷嘴球面半径r通常比模具主流道衬套球面半径（ ）。<reserved_107>小1 ～2mm</s><s> <reserved_106>判断以下陈述是否正确。
螺旋槽铰刀有右旋和左旋之分。其中左旋槽铰刀适用于铰削通孔。<reserved_107>正确</s><s> <reserved_106>回答下列问题
低速切削刀具（如拉刀、板牙和丝锥等）的主要磨损形式为（ ）。<reserved_107>硬质点磨损</s><s> <reserved_106>判断以下陈述是否正确。
塑件的侧面带有侧凸或者侧凹的结构才要用侧向抽芯机构。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
刀具磨损主要原因有：硬质点磨损、粘结磨损、扩散磨损、化学磨损。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
尺寸链封闭环的基本尺寸，是其他组成环基本尺寸的代数差。<reserved_107>错误</s><s> <reserved_106>回答下列问题
用三坐标测量仪可以测量（ ）等参数。<reserved_107>圆度、位置度、直线度</s><s> <reserved_106>回答下列问题
关于表面淬火，正确的描述是（ ）。<reserved_107>提高零件表面硬度和耐磨性;保证零件心部原有的韧性和塑性;特别适用于中碳钢</s><s> <reserved_106>判断以下陈述是否正确。
螺旋槽铰刀有右旋和左旋之分。其中左旋槽铰刀适用于铰削盲孔。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
将淬火后的钢质件进行高温回火，这种双热处理的操作为正火。<reserved_107>错误</s><s> <reserved_106>回答下列问题
修配装配法的特点是（ ）。<reserved_107>能获得很高的装配精度，而零件的制造公差却可以放宽</s><s> <reserved_106>回答下列问题
增加校正工序或改进弯曲模成型结构是解决（ ）<reserved_107>材料回弹</s><s> <reserved_106>回答下列问题
六个基本视图中最常用的是（ ）。<reserved_107>主、俯、左</s><s> <reserved_106>判断以下陈述是否正确。
同一塑件的壁厚应尽量一致。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
高速主轴一般做成模拟主轴的结构形式。<reserved_107>错误。</s><s> <reserved_106>判断以下陈述是否正确。
用钢直尺和90°角尺配合不能在工件上划出平行线。<reserved_107>错误</s><s> <reserved_106>回答下列问题
惰轮的作用有（ ）。<reserved_107>改变从动件的转动方向;增加主从动件的中心距</s><s> <reserved_106>回答下列问题
正火的目的是（ ）。<reserved_107>改善低碳钢和低碳合金钢的切削加工性能</s><s> <reserved_106>回答下列问题
不属于不溢式压缩模的特点是（ ）？<reserved_107>密实性最差</s><s> <reserved_106>回答下列问题
影响热塑性塑料收缩性的因素很多，下面哪个结论错误（ ）。<reserved_107>在同一塑件的不同部位收缩率是一样的</s><s> <reserved_106>判断以下陈述是否正确。
机构就是具有相对运动的零件的组合。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
当压机的顶杆与压缩模的推出机构采用直接连接时，模具中不需要再设复位机构。<reserved_107>正确</s><s> <reserved_106>回答下列问题
精加工时应首先考虑（ ）。<reserved_107>零件的加工精度和表面质量</s><s> <reserved_106>回答下列问题
职业病有（ ）级防护。<reserved_107>三</s><s> <reserved_106>回答下列问题
工业固体废物化学处理方法包括（ ）。<reserved_107>氧化;还原;中和;化学沉淀</s><s> <reserved_106>回答下列问题
（ ）是装配的零部件在装配运动过程中，其运动包络体是否存在零部件之间的运动干涉。<reserved_107>动态干涉检查</s><s> <reserved_106>回答下列问题
镶套修复法加工精度要求高，（ ）了零件的强度。<reserved_107>降低</s><s> <reserved_106>判断以下陈述是否正确。
当热继电器与其它电器装在一起时，应装在电器下方且远离其它电器50mm以上，以免受其它电器发热影响。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
对于线切割加工，下列说法正确的是（ ）。<reserved_107>线切割加工圆弧时，其运动轨迹是折线;加工圆弧时，取圆心为切割坐标系的原点</s><s> <reserved_106>判断以下陈述是否正确。
导柱热处理后一定要研磨两端中心孔才能进行磨削。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
光整加工的主要任务是提高被加工表面的尺寸精度和降低表面粗糙度，一般不能纠正几何形状和相互位置误差。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
水平仪不但能检验平面的位置是否成水平，而且能测出工件上两平面的平行度。<reserved_107>正确</s><s> <reserved_106>回答下列问题
对塑料模浇口套的装配，下列说法正确的有（ ）。<reserved_107>浇口套的压入端不允许有导入斜度;常将浇口套的压入端加工成小圆角</s><s> <reserved_106>判断以下陈述是否正确。
螺纹连接分普通螺纹连接和精密螺纹连接两大类<reserved_107>错误</s><s> <reserved_106>回答下列问题
代号TQW表示（ ）。<reserved_107>倾斜型微调镗刀</s><s> <reserved_106>回答下列问题
模具零件最常见的毛坯形式是（ ）。<reserved_107>锻件;型材;铸件</s><s> <reserved_106>回答下列问题
关于注塑模具对锁紧及紧固零件的装配技术要求，下列说法中错误的是（ ）。<reserved_107>圆柱销与模具零件要精确定位</s><s> <reserved_106>回答下列问题
某制品一型腔尺寸为50±0.20mm，收缩率为1%，模具设计时，考虑拔模斜度和模具磨损，该成型尺寸设计较好的是（ ）。<reserved_107>大端50.5mm、小端50.4mm</s><s> <reserved_106>判断以下陈述是否正确。
工件夹紧后，工件的六个自由度都被限制了。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
电火花线切割采用移动的电极丝进行加工，可用于通孔的型孔或直壁凸模加工；。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
通常夹具的制造误差应是工件在该工序中允许误差的（ ）。<reserved_107>1/3~1/5</s><s> <reserved_106>判断以下陈述是否正确。
机床夹具在机械加工过程中的主要作用是易于保证工件的加工精度，改变和扩大原机床的功能，缩短辅助时间，提高劳动生产率。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
在三相异步电动机接触器互锁正反转控制电路中，正转接触器KM1和反转接触器KM2之间互锁的连接方法是（ ）。<reserved_107>KM1的线圈与KM2的动断（常闭）辅助触点串联，KM2的线圈与KM1的动断（常闭）辅助触点串联</s><s> <reserved_106>回答下列问题
精密注射模具冷却水入口与出口的温差应控制在（ ）左右。<reserved_107>2°C</s><s> <reserved_106>回答下列问题
錾削一般钢材和中等硬度材料时，錾子的楔角应选取（ ）。<reserved_107>50°～60°</s><s> <reserved_106>回答下列问题
汽车覆盖件拉深模的凸模、凹模、压边圈所采用的材料一般是（ ）。<reserved_107>铸铁</s><s> <reserved_106>判断以下陈述是否正确。
斜导柱角度与锁紧块角度不同是为了在开模时使楔紧面的分开慢于斜导柱驱动滑块分开。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
采用压入式模座装配时，其配合应为（ ）方适合。<reserved_107>导柱、导套与模座的配合均采用H7/r6，而导柱与导套之间采用H7/h6</s><s> <reserved_106>判断以下陈述是否正确。
注射装置的作用主要是模具的启闭，在注射时保证成型模具可靠地合紧，以及脱出制品。<reserved_107>错误</s><s> <reserved_106>判断以下陈述是否正确。
基本偏差一定的轴的公差带，与不同的基本偏差的孔的公差带形成各种配合的一种制度。<reserved_107>正确</s><s> <reserved_106>回答下列问题
下列关于弯曲变形特点叙述正确的是（ ）。<reserved_107>内层材料受压缩短，外层材料受拉伸长</s><s> <reserved_106>回答下列问题
冲压常用材料是（ ）<reserved_107>金属材料</s><s><reserved_106>判断以下陈述是否正确。
模具的浇口应开设在塑件界面的最厚处
11/18 08:53:50 - mmengine - INFO - before_train in EvaluateChatHook .
11/18 08:54:02 - mmengine - INFO - Sample output:
<reserved_106>请给我介绍五个上海的景点<reserved_107>当然可以！以下是五个上海著名的旅游景点：

1. 外滩：外滩是上海最著名的景点之一，位于黄浦江畔。这里有许多历史悠久的建筑，如和平饭店、上海海关大楼等。在外滩，你可以欣赏到

11/18 08:54:06 - mmengine - INFO - Sample output:
 <reserved_106>Please tell me five scenic spots in Shanghai<reserved_107>1. The Bund (Shanghai)
2. The Oriental Pearl Tower (Shanghai)
3. The Shanghai World Expo Park
4. The Shanghai Zoo
5. The Shanghai Ocean Park</s>

11/18 08:54:06 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
11/18 08:54:06 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
11/18 08:54:06 - mmengine - INFO - Checkpoints will be saved to /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18.
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/optim/scheduler/param_scheduler.py:198: UserWarning: Detected call of `scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `scheduler.step()`. Failure to do this will result in PyTorch skipping the first value of the parameter value schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
11/18 08:55:19 - mmengine - INFO - Epoch(train)  [1][10/80]  lr: 1.9994e-04  eta: 1:35:14  time: 7.2335  data_time: 0.0040  memory: 13284  loss: 3.5778
11/18 08:56:31 - mmengine - INFO - Epoch(train)  [1][20/80]  lr: 1.9975e-04  eta: 1:33:57  time: 7.2209  data_time: 0.0034  memory: 13286  loss: 3.1376  grad_norm: 1.4609
11/18 08:57:41 - mmengine - INFO - Epoch(train)  [1][30/80]  lr: 1.9942e-04  eta: 1:31:52  time: 7.0214  data_time: 0.0033  memory: 13286  loss: 2.9360  grad_norm: 1.4609
11/18 08:58:52 - mmengine - INFO - Epoch(train)  [1][40/80]  lr: 1.9895e-04  eta: 1:30:24  time: 7.0727  data_time: 0.0033  memory: 13286  loss: 2.9210  grad_norm: 1.0974
11/18 09:00:03 - mmengine - INFO - Epoch(train)  [1][50/80]  lr: 1.9834e-04  eta: 1:29:03  time: 7.0743  data_time: 0.0033  memory: 13286  loss: 2.8929  grad_norm: 0.8895
11/18 09:01:13 - mmengine - INFO - Epoch(train)  [1][60/80]  lr: 1.9760e-04  eta: 1:27:38  time: 7.0140  data_time: 0.0033  memory: 13286  loss: 2.7804  grad_norm: 0.8895
11/18 09:02:24 - mmengine - INFO - Epoch(train)  [1][70/80]  lr: 1.9672e-04  eta: 1:26:23  time: 7.0724  data_time: 0.0032  memory: 13286  loss: 2.5926  grad_norm: 0.7714
11/18 09:03:34 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 09:03:34 - mmengine - INFO - Epoch(train)  [1][80/80]  lr: 1.9570e-04  eta: 1:25:10  time: 7.0720  data_time: 0.0033  memory: 13286  loss: 2.5158  grad_norm: 0.6816
11/18 09:03:34 - mmengine - INFO - Saving checkpoint at 1 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 09:04:49 - mmengine - INFO - Epoch(train)  [2][10/80]  lr: 1.9456e-04  eta: 1:23:53  time: 7.0190  data_time: 0.0036  memory: 13286  loss: 2.4762  grad_norm: 0.6816
11/18 09:06:01 - mmengine - INFO - Epoch(train)  [2][20/80]  lr: 1.9328e-04  eta: 1:22:46  time: 7.1443  data_time: 0.0034  memory: 13286  loss: 2.5515  grad_norm: 0.6143
11/18 09:07:11 - mmengine - INFO - Epoch(train)  [2][30/80]  lr: 1.9188e-04  eta: 1:21:30  time: 7.0152  data_time: 0.0033  memory: 13286  loss: 2.3753  grad_norm: 0.6143
11/18 09:08:21 - mmengine - INFO - Epoch(train)  [2][40/80]  lr: 1.9035e-04  eta: 1:20:18  time: 7.0745  data_time: 0.0033  memory: 13286  loss: 2.3898  grad_norm: 0.5696
11/18 09:09:32 - mmengine - INFO - Epoch(train)  [2][50/80]  lr: 1.8870e-04  eta: 1:19:07  time: 7.0731  data_time: 0.0033  memory: 13286  loss: 2.3399  grad_norm: 0.5349
11/18 09:10:42 - mmengine - INFO - Epoch(train)  [2][60/80]  lr: 1.8692e-04  eta: 1:17:52  time: 7.0141  data_time: 0.0031  memory: 13286  loss: 2.3685  grad_norm: 0.5349
11/18 09:11:53 - mmengine - INFO - Epoch(train)  [2][70/80]  lr: 1.8503e-04  eta: 1:16:41  time: 7.0695  data_time: 0.0031  memory: 13286  loss: 2.2696  grad_norm: 0.5039
11/18 09:13:04 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 09:13:04 - mmengine - INFO - Epoch(train)  [2][80/80]  lr: 1.8302e-04  eta: 1:15:30  time: 7.0726  data_time: 0.0031  memory: 13286  loss: 2.2765  grad_norm: 0.4754
11/18 09:13:04 - mmengine - INFO - Saving checkpoint at 2 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 09:14:17 - mmengine - INFO - Epoch(train)  [3][10/80]  lr: 1.8090e-04  eta: 1:14:17  time: 7.0197  data_time: 0.0035  memory: 13286  loss: 2.2413  grad_norm: 0.4754
11/18 09:15:28 - mmengine - INFO - Epoch(train)  [3][20/80]  lr: 1.7867e-04  eta: 1:13:09  time: 7.1459  data_time: 0.0032  memory: 13286  loss: 2.2811  grad_norm: 0.3471
11/18 09:16:39 - mmengine - INFO - Epoch(train)  [3][30/80]  lr: 1.7633e-04  eta: 1:11:56  time: 7.0183  data_time: 0.0032  memory: 13286  loss: 2.3059  grad_norm: 0.3471
11/18 09:17:49 - mmengine - INFO - Epoch(train)  [3][40/80]  lr: 1.7389e-04  eta: 1:10:45  time: 7.0744  data_time: 0.0031  memory: 13286  loss: 2.2202  grad_norm: 0.2877
11/18 09:19:00 - mmengine - INFO - Epoch(train)  [3][50/80]  lr: 1.7135e-04  eta: 1:09:34  time: 7.0797  data_time: 0.0031  memory: 13286  loss: 2.2344  grad_norm: 0.2555
11/18 09:20:10 - mmengine - INFO - Epoch(train)  [3][60/80]  lr: 1.6872e-04  eta: 1:08:22  time: 7.0227  data_time: 0.0031  memory: 13286  loss: 2.1694  grad_norm: 0.2555
11/18 09:21:21 - mmengine - INFO - Epoch(train)  [3][70/80]  lr: 1.6600e-04  eta: 1:07:12  time: 7.0768  data_time: 0.0031  memory: 13286  loss: 2.1340  grad_norm: 0.2298
11/18 09:22:32 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 09:22:32 - mmengine - INFO - Epoch(train)  [3][80/80]  lr: 1.6319e-04  eta: 1:06:01  time: 7.0752  data_time: 0.0031  memory: 13286  loss: 2.1779  grad_norm: 0.2145
11/18 09:22:32 - mmengine - INFO - Saving checkpoint at 3 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 09:23:45 - mmengine - INFO - Epoch(train)  [4][10/80]  lr: 1.6029e-04  eta: 1:04:49  time: 7.0123  data_time: 0.0035  memory: 13286  loss: 2.0869  grad_norm: 0.2145
11/18 09:24:57 - mmengine - INFO - Epoch(train)  [4][20/80]  lr: 1.5733e-04  eta: 1:03:40  time: 7.1458  data_time: 0.0032  memory: 13286  loss: 2.1569  grad_norm: 0.2017
11/18 09:26:07 - mmengine - INFO - Epoch(train)  [4][30/80]  lr: 1.5428e-04  eta: 1:02:28  time: 7.0168  data_time: 0.0031  memory: 13286  loss: 2.0567  grad_norm: 0.2017
11/18 09:27:17 - mmengine - INFO - Epoch(train)  [4][40/80]  lr: 1.5117e-04  eta: 1:01:17  time: 7.0727  data_time: 0.0032  memory: 13286  loss: 2.1910  grad_norm: 0.1883
11/18 09:28:28 - mmengine - INFO - Epoch(train)  [4][50/80]  lr: 1.4800e-04  eta: 1:00:06  time: 7.0704  data_time: 0.0032  memory: 13286  loss: 2.0793  grad_norm: 0.1751
11/18 09:29:38 - mmengine - INFO - Epoch(train)  [4][60/80]  lr: 1.4477e-04  eta: 0:58:55  time: 7.0157  data_time: 0.0031  memory: 13286  loss: 2.1130  grad_norm: 0.1751
11/18 09:30:49 - mmengine - INFO - Epoch(train)  [4][70/80]  lr: 1.4148e-04  eta: 0:57:44  time: 7.0723  data_time: 0.0031  memory: 13286  loss: 2.1504  grad_norm: 0.1646
11/18 09:32:00 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 09:32:00 - mmengine - INFO - Epoch(train)  [4][80/80]  lr: 1.3815e-04  eta: 0:56:33  time: 7.0749  data_time: 0.0031  memory: 13286  loss: 2.0642  grad_norm: 0.1587
11/18 09:32:00 - mmengine - INFO - Saving checkpoint at 4 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 09:33:13 - mmengine - INFO - Epoch(train)  [5][10/80]  lr: 1.3477e-04  eta: 0:55:22  time: 7.0186  data_time: 0.0057  memory: 13286  loss: 1.9686  grad_norm: 0.1587
11/18 09:34:24 - mmengine - INFO - Epoch(train)  [5][20/80]  lr: 1.3135e-04  eta: 0:54:12  time: 7.1454  data_time: 0.0031  memory: 13286  loss: 1.9573  grad_norm: 0.1546
11/18 09:35:35 - mmengine - INFO - Epoch(train)  [5][30/80]  lr: 1.2790e-04  eta: 0:53:01  time: 7.0177  data_time: 0.0031  memory: 13286  loss: 2.0449  grad_norm: 0.1546
11/18 09:36:45 - mmengine - INFO - Epoch(train)  [5][40/80]  lr: 1.2443e-04  eta: 0:51:50  time: 7.0714  data_time: 0.0032  memory: 13286  loss: 2.0325  grad_norm: 0.1554
11/18 09:37:56 - mmengine - INFO - Epoch(train)  [5][50/80]  lr: 1.2093e-04  eta: 0:50:39  time: 7.0705  data_time: 0.0031  memory: 13286  loss: 2.1124  grad_norm: 0.1556
11/18 09:39:06 - mmengine - INFO - Epoch(train)  [5][60/80]  lr: 1.1741e-04  eta: 0:49:28  time: 7.0162  data_time: 0.0031  memory: 13286  loss: 1.9580  grad_norm: 0.1556
11/18 09:40:17 - mmengine - INFO - Epoch(train)  [5][70/80]  lr: 1.1389e-04  eta: 0:48:18  time: 7.0669  data_time: 0.0030  memory: 13286  loss: 1.9432  grad_norm: 0.1568
11/18 09:41:28 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 09:41:28 - mmengine - INFO - Epoch(train)  [5][80/80]  lr: 1.1035e-04  eta: 0:47:07  time: 7.0802  data_time: 0.0031  memory: 13286  loss: 2.0411  grad_norm: 0.1559
11/18 09:41:28 - mmengine - INFO - Saving checkpoint at 5 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 09:42:41 - mmengine - INFO - Epoch(train)  [6][10/80]  lr: 1.0682e-04  eta: 0:45:56  time: 7.0207  data_time: 0.0036  memory: 13286  loss: 1.8710  grad_norm: 0.1559
11/18 09:43:53 - mmengine - INFO - Epoch(train)  [6][20/80]  lr: 1.0329e-04  eta: 0:44:46  time: 7.1463  data_time: 0.0033  memory: 13286  loss: 2.0041  grad_norm: 0.1579
11/18 09:45:03 - mmengine - INFO - Epoch(train)  [6][30/80]  lr: 9.9773e-05  eta: 0:43:35  time: 7.0202  data_time: 0.0031  memory: 13286  loss: 1.8391  grad_norm: 0.1579
11/18 09:46:14 - mmengine - INFO - Epoch(train)  [6][40/80]  lr: 9.6270e-05  eta: 0:42:24  time: 7.0809  data_time: 0.0031  memory: 13286  loss: 1.9691  grad_norm: 0.1598
11/18 09:47:25 - mmengine - INFO - Epoch(train)  [6][50/80]  lr: 9.2789e-05  eta: 0:41:14  time: 7.0772  data_time: 0.0031  memory: 13286  loss: 1.8863  grad_norm: 0.1601
11/18 09:48:35 - mmengine - INFO - Epoch(train)  [6][60/80]  lr: 8.9334e-05  eta: 0:40:02  time: 7.0202  data_time: 0.0031  memory: 13286  loss: 1.9483  grad_norm: 0.1601
11/18 09:49:45 - mmengine - INFO - Epoch(train)  [6][70/80]  lr: 8.5911e-05  eta: 0:38:52  time: 7.0750  data_time: 0.0031  memory: 13286  loss: 1.8182  grad_norm: 0.1613
11/18 09:50:56 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 09:50:56 - mmengine - INFO - Epoch(train)  [6][80/80]  lr: 8.2525e-05  eta: 0:37:41  time: 7.0757  data_time: 0.0030  memory: 13286  loss: 1.9619  grad_norm: 0.1619
11/18 09:50:56 - mmengine - INFO - Saving checkpoint at 6 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 09:52:10 - mmengine - INFO - Epoch(train)  [7][10/80]  lr: 7.9181e-05  eta: 0:36:30  time: 7.0175  data_time: 0.0035  memory: 13286  loss: 1.8836  grad_norm: 0.1619
11/18 09:53:21 - mmengine - INFO - Epoch(train)  [7][20/80]  lr: 7.5885e-05  eta: 0:35:20  time: 7.1466  data_time: 0.0031  memory: 13286  loss: 1.7313  grad_norm: 0.1650
11/18 09:54:31 - mmengine - INFO - Epoch(train)  [7][30/80]  lr: 7.2642e-05  eta: 0:34:09  time: 7.0148  data_time: 0.0032  memory: 13286  loss: 1.8889  grad_norm: 0.1650
11/18 09:55:42 - mmengine - INFO - Epoch(train)  [7][40/80]  lr: 6.9456e-05  eta: 0:32:58  time: 7.0725  data_time: 0.0035  memory: 13286  loss: 1.8978  grad_norm: 0.1667
11/18 09:56:53 - mmengine - INFO - Epoch(train)  [7][50/80]  lr: 6.6333e-05  eta: 0:31:48  time: 7.0749  data_time: 0.0031  memory: 13286  loss: 1.8034  grad_norm: 0.1683
11/18 09:58:03 - mmengine - INFO - Epoch(train)  [7][60/80]  lr: 6.3277e-05  eta: 0:30:37  time: 7.0191  data_time: 0.0032  memory: 13286  loss: 1.7584  grad_norm: 0.1683
11/18 09:59:14 - mmengine - INFO - Epoch(train)  [7][70/80]  lr: 6.0293e-05  eta: 0:29:26  time: 7.0726  data_time: 0.0032  memory: 13286  loss: 1.8857  grad_norm: 0.1679
11/18 10:00:24 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 10:00:24 - mmengine - INFO - Epoch(train)  [7][80/80]  lr: 5.7386e-05  eta: 0:28:16  time: 7.0756  data_time: 0.0032  memory: 13286  loss: 1.7616  grad_norm: 0.1693
11/18 10:00:24 - mmengine - INFO - Saving checkpoint at 7 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 10:01:38 - mmengine - INFO - Epoch(train)  [8][10/80]  lr: 5.4560e-05  eta: 0:27:05  time: 7.0171  data_time: 0.0036  memory: 13286  loss: 1.7146  grad_norm: 0.1693
11/18 10:02:49 - mmengine - INFO - Epoch(train)  [8][20/80]  lr: 5.1819e-05  eta: 0:25:54  time: 7.1471  data_time: 0.0032  memory: 13286  loss: 1.6047  grad_norm: 0.1689
11/18 10:03:59 - mmengine - INFO - Epoch(train)  [8][30/80]  lr: 4.9168e-05  eta: 0:24:44  time: 7.0221  data_time: 0.0031  memory: 13286  loss: 1.8604  grad_norm: 0.1689
11/18 10:05:10 - mmengine - INFO - Epoch(train)  [8][40/80]  lr: 4.6611e-05  eta: 0:23:33  time: 7.0763  data_time: 0.0031  memory: 13286  loss: 1.8675  grad_norm: 0.1674
11/18 10:06:21 - mmengine - INFO - Epoch(train)  [8][50/80]  lr: 4.4151e-05  eta: 0:22:22  time: 7.0825  data_time: 0.0031  memory: 13286  loss: 1.8643  grad_norm: 0.1681
11/18 10:07:31 - mmengine - INFO - Epoch(train)  [8][60/80]  lr: 4.1794e-05  eta: 0:21:11  time: 7.0169  data_time: 0.0030  memory: 13286  loss: 1.7274  grad_norm: 0.1681
11/18 10:08:42 - mmengine - INFO - Epoch(train)  [8][70/80]  lr: 3.9541e-05  eta: 0:20:01  time: 7.0787  data_time: 0.0033  memory: 13286  loss: 1.6724  grad_norm: 0.1699
11/18 10:09:53 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 10:09:53 - mmengine - INFO - Epoch(train)  [8][80/80]  lr: 3.7397e-05  eta: 0:18:50  time: 7.0767  data_time: 0.0031  memory: 13286  loss: 1.7326  grad_norm: 0.1715
11/18 10:09:53 - mmengine - INFO - Saving checkpoint at 8 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 10:11:06 - mmengine - INFO - Epoch(train)  [9][10/80]  lr: 3.5365e-05  eta: 0:17:39  time: 7.0181  data_time: 0.0055  memory: 13286  loss: 1.6900  grad_norm: 0.1715
11/18 10:12:17 - mmengine - INFO - Epoch(train)  [9][20/80]  lr: 3.3448e-05  eta: 0:16:29  time: 7.1481  data_time: 0.0031  memory: 13286  loss: 1.6200  grad_norm: 0.1724
11/18 10:13:28 - mmengine - INFO - Epoch(train)  [9][30/80]  lr: 3.1649e-05  eta: 0:15:18  time: 7.0220  data_time: 0.0031  memory: 13286  loss: 1.8372  grad_norm: 0.1724
11/18 10:14:38 - mmengine - INFO - Epoch(train)  [9][40/80]  lr: 2.9970e-05  eta: 0:14:07  time: 7.0722  data_time: 0.0031  memory: 13286  loss: 1.7619  grad_norm: 0.1736
11/18 10:15:49 - mmengine - INFO - Epoch(train)  [9][50/80]  lr: 2.8416e-05  eta: 0:12:57  time: 7.0749  data_time: 0.0032  memory: 13286  loss: 1.7861  grad_norm: 0.1748
11/18 10:16:59 - mmengine - INFO - Epoch(train)  [9][60/80]  lr: 2.6987e-05  eta: 0:11:46  time: 7.0186  data_time: 0.0031  memory: 13286  loss: 1.7077  grad_norm: 0.1748
11/18 10:18:10 - mmengine - INFO - Epoch(train)  [9][70/80]  lr: 2.5686e-05  eta: 0:10:35  time: 7.0786  data_time: 0.0030  memory: 13286  loss: 1.7059  grad_norm: 0.1770
11/18 10:19:21 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 10:19:21 - mmengine - INFO - Epoch(train)  [9][80/80]  lr: 2.4515e-05  eta: 0:09:25  time: 7.0694  data_time: 0.0031  memory: 13286  loss: 1.5257  grad_norm: 0.1785
11/18 10:19:21 - mmengine - INFO - Saving checkpoint at 9 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 10:20:34 - mmengine - INFO - Epoch(train) [10][10/80]  lr: 2.3476e-05  eta: 0:08:14  time: 7.0189  data_time: 0.0065  memory: 13286  loss: 1.7069  grad_norm: 0.1785
11/18 10:21:45 - mmengine - INFO - Epoch(train) [10][20/80]  lr: 2.2570e-05  eta: 0:07:04  time: 7.1480  data_time: 0.0032  memory: 13286  loss: 1.6586  grad_norm: 0.1808
11/18 10:22:55 - mmengine - INFO - Epoch(train) [10][30/80]  lr: 2.1799e-05  eta: 0:05:53  time: 7.0157  data_time: 0.0031  memory: 13286  loss: 1.6569  grad_norm: 0.1808
11/18 10:24:06 - mmengine - INFO - Epoch(train) [10][40/80]  lr: 2.1164e-05  eta: 0:04:42  time: 7.0756  data_time: 0.0031  memory: 13286  loss: 1.5829  grad_norm: 0.1820
11/18 10:25:17 - mmengine - INFO - Epoch(train) [10][50/80]  lr: 2.0666e-05  eta: 0:03:31  time: 7.0809  data_time: 0.0030  memory: 13286  loss: 1.7539  grad_norm: 0.1837
11/18 10:26:27 - mmengine - INFO - Epoch(train) [10][60/80]  lr: 2.0306e-05  eta: 0:02:21  time: 7.0170  data_time: 0.0030  memory: 13286  loss: 1.6429  grad_norm: 0.1837
11/18 10:27:38 - mmengine - INFO - Epoch(train) [10][70/80]  lr: 2.0084e-05  eta: 0:01:10  time: 7.0694  data_time: 0.0031  memory: 13286  loss: 1.7016  grad_norm: 0.1843
11/18 10:28:49 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_085228
11/18 10:28:49 - mmengine - INFO - Epoch(train) [10][80/80]  lr: 2.0001e-05  eta: 0:00:00  time: 7.0743  data_time: 0.0031  memory: 13286  loss: 1.6308  grad_norm: 0.1865
11/18 10:28:49 - mmengine - INFO - Saving checkpoint at 10 epochs
+ test
+ xtuner_config=/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py
+ pth_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.pth
+ hf_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.hf
+ xtuner convert pth_to_hf /data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.pth /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.hf
[2023-11-18 10:29:00,806] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 10:29:06,071] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 12.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.42s/it]
11/18 10:29:33 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 10:29:33 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 10:29:33 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 10:29:33 - mmengine - INFO - dispatch baichuan2-13B attn forward
Load PTH model from /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.pth
Convert weights to float16
Saving HuggingFace model to /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.hf
All done!
+ python merge.py /data/yinxiaoln/pre_models/Baichuan2-7B-Chat /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/epoch_10.hf /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/merged
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.14s/it]
Saving to /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/merged...
All done!
+ python xtuner_test.py /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/output.json /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/merged
2023-11-18 10:33:58,112 - modelscope - INFO - PyTorch version 2.1.0 Found.
2023-11-18 10:33:58,113 - modelscope - INFO - Loading ast index from /home/yinxiaoln/.cache/modelscope/ast_indexer
2023-11-18 10:33:58,168 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 3cdb0bb552f9ef7c5b6c350d835395e7 and a total number of 941 components indexed
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
/data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/merged
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:22,  3.19s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:05<00:16,  2.75s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:08<00:13,  2.64s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:10<00:10,  2.56s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:13<00:07,  2.52s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:15<00:05,  2.50s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:17<00:02,  2.48s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.36s/it]
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Dumping model to file cache /tmp/jieba.cache
Dump cache file failed.
Traceback (most recent call last):
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/jieba/__init__.py", line 154, in initialize
    _replace_file(fpath, cache_file)
PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpk3bm12w6' -> '/tmp/jieba.cache'
Loading model cost 0.573 seconds.
Prefix dict has been built successfully.
tot_cnt 585
0 / 585 Sat Nov 18 10:34:22 2023
错误 正确。
tensor([[5437]]) tensor([[4283,   66]])
100 / 585 Sat Nov 18 10:35:00 2023
被测要素 尺寸线的延长线;中心要素
tensor([[92593, 92869, 13813]]) tensor([[12014, 11684,  9535, 92597, 92399,  1925, 13813]])
200 / 585 Sat Nov 18 10:35:36 2023
钻削 镗孔
tensor([[94353, 95044]]) tensor([[98717, 93956]])
300 / 585 Sat Nov 18 10:36:17 2023
错误 错误
tensor([[5437]]) tensor([[5437]])
400 / 585 Sat Nov 18 10:36:57 2023
碳素工具钢;合金工具钢;高速钢 高速钢;高强度合金工具钢;硬质合金
tensor([[94215, 92959,  5027, 93655, 92399, 24902,  5027, 93655, 92399,  3622,
         93655]]) tensor([[ 3622, 93655, 92399, 50034, 24902,  5027, 93655, 92399, 93564, 92691,
         24902]])
500 / 585 Sat Nov 18 10:37:32 2023
错误 错误
tensor([[5437]]) tensor([[5437]])
{'rouge-1': 29.548147179487195, 'rouge-2': 5.145120341880341, 'rouge-l': 28.707772478632467, 'bleu-4': 18.541608717948716}
saved to  /data/yinxiaoln/save/Baichuan2/2023-11-18_08-52-18/output.json
+ BASE_DIR=/data/yinxiaoln
++ date +%Y-%m-%d_%H-%M-%S
+ SAVE_PATH=2023-11-18_11-37-32
+ main
+ train
+ work_dir=/data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32
+ CUDA_VISIBLE_DEVICES=0
+ xtuner train baichuan_xtuner_config.py --work-dir /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32
[2023-11-18 11:37:37,069] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-18 11:37:40,958] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/18 11:37:42 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
    CUDA available: True
    numpy_random_seed: 1577804761
    GPU 0: NVIDIA A40
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.3, V12.3.52
    GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0+cu121
    OpenCV: 4.8.1
    MMEngine: 0.9.0

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1577804761
    deterministic: False
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

11/18 11:37:42 - mmengine - INFO - Config:
BASE_PATH = '/data/yinxiaoln/'
accumulative_counts = 16
alpaca_en = dict(
    dataset=dict(
        data_files=dict(
            train=
            '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
        ),
        path='json',
        type='datasets.load_dataset'),
    dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
    max_length=2048,
    pack_to_max_length=True,
    remove_unused_columns=True,
    shuffle_before_pack=True,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.process_hf_dataset')
alpaca_en_path = '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
batch_size = 1
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.DatasetInfoHook'),
    dict(
        evaluation_inputs=[
            '请给我介绍五个上海的景点',
            'Please tell me five scenic spots in Shanghai',
        ],
        every_n_iters=500,
        instruction=
        'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat.INSTRUCTION_START',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.EvaluateChatHook'),
]
dataloader_num_workers = 0
default_hooks = dict(
    checkpoint=dict(interval=1, type='mmengine.hooks.CheckpointHook'),
    logger=dict(interval=10, type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_inputs = [
    '请给我介绍五个上海的景点',
    'Please tell me five scenic spots in Shanghai',
]
launcher = 'none'
load_from = None
log_level = 'INFO'
lr = 0.0002
max_epochs = 100
max_length = 2048
max_norm = 1
model = dict(
    llm=dict(
        pretrained_model_name_or_path=
        '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
        quantization_config=dict(
            bnb_4bit_compute_dtype='torch.float16',
            bnb_4bit_quant_type='nf4',
            bnb_4bit_use_double_quant=True,
            llm_int8_has_fp16_weight=False,
            llm_int8_threshold=6.0,
            load_in_4bit=True,
            load_in_8bit=False,
            type='transformers.BitsAndBytesConfig'),
        torch_dtype='torch.float16',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    lora=dict(
        bias='none',
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        task_type='CAUSAL_LM',
        type='peft.LoraConfig'),
    type='xtuner.model.SupervisedFinetune')
optim_type = 'bitsandbytes.optim.PagedAdamW32bit'
optim_wrapper = dict(
    accumulative_counts=16,
    clip_grad=dict(error_if_nonfinite=False, max_norm=1),
    dtype='float16',
    loss_scale='dynamic',
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=0.0002,
        type='bitsandbytes.optim.PagedAdamW32bit',
        weight_decay=0),
    type='mmengine.optim.AmpOptimWrapper')
pack_to_max_length = True
param_scheduler = dict(
    T_max=100,
    by_epoch=True,
    convert_to_iter_based=True,
    eta_min=2e-05,
    type='mmengine.optim.CosineAnnealingLR')
pretrained_model_name_or_path = '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat'
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)
train_dataloader = dict(
    batch_size=1,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        dataset=dict(
            data_files=dict(
                train=
                '/data/yinxiaoln/datasets/muju/question-ans-v2/train/question-ans-train-v2.json'
            ),
            path='json',
            type='datasets.load_dataset'),
        dataset_map_fn='xtuner.dataset.map_fns.alpaca_map_fn',
        max_length=2048,
        pack_to_max_length=True,
        remove_unused_columns=True,
        shuffle_before_pack=True,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.baichuan2_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/data/yinxiaoln/pre_models/Baichuan2-7B-Chat',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.process_hf_dataset'),
    num_workers=0,
    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))
visualizer = None
weight_decay = 0
work_dir = '/data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32'

quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
11/18 11:37:42 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.72s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 12.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.73s/it]
11/18 11:38:10 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/18 11:38:10 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/18 11:38:10 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/18 11:38:10 - mmengine - INFO - dispatch baichuan2-13B attn forward
11/18 11:39:03 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
11/18 11:39:03 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) EvaluateChatHook                   
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
Found cached dataset json (/home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 705.76it/s]
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-817222a3ff087594.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-efe2e5113f9035af.arrow
Loading cached processed dataset at /home/yinxiaoln/.cache/huggingface/datasets/json/default-2a2076e5e2533446/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7868ac168b99dcee.arrow
Flattening the indices:   0%|          | 0/5256 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 5256/5256 [00:00<00:00, 50290.20 examples/s]                                                                                     Map:   0%|          | 0/5256 [00:00<?, ? examples/s]Map:  76%|███████▌  | 4000/5256 [00:00<00:00, 26796.68 examples/s]                                                                  11/18 11:39:04 - mmengine - WARNING - Dataset Dataset has no metainfo. ``dataset_meta`` in visualizer will be None.
11/18 11:39:05 - mmengine - INFO - Num train samples 80
11/18 11:39:05 - mmengine - INFO - train example:
11/18 11:39:05 - mmengine - INFO - <s> <reserved_106>判断以下陈述是否正确。
在注射同一种塑料时，螺杆式料筒温度可比柱塞式料筒温度低。<reserved_107>正确</s><s> <reserved_106>回答下列问题
YG8硬质合金，其中数字8表示（ ）含量的百分数。<reserved_107>钴</s><s> <reserved_106>回答下列问题
国家标准规定优先选用基孔制配合的原因是（ ）。<reserved_107>为了减少定尺寸孔用刀、量具的规格和数量</s><s> <reserved_106>判断以下陈述是否正确。
工件固定，钻头安装在钻床主轴上左旋转运动（称为进给运动）<reserved_107>错误</s><s> <reserved_106>回答下列问题
注射成型工艺适用于（ ）。<reserved_107>主要成型热塑性塑料，某些热固性塑料也可用注射方法成型</s><s> <reserved_106>回答下列问题
同一表面上有不同的表面粗糙度要求时，须用（ ）画出其分界线<reserved_107>细实线</s><s> <reserved_106>回答下列问题
推杆与其配合孔的配合常采用（ ）配合。<reserved_107>间隙</s><s> <reserved_106>回答下列问题
有的制品要求不同颜色或透明度，在成型前应先在原料中加入所需的（ ）。<reserved_107>着色剂</s><s> <reserved_106>回答下列问题
以下说法正确的有（ ）。<reserved_107>使用匹配或对齐约束时，可对偏距输入负值;可以在装配环境下创建零部件</s><s> <reserved_106>回答下列问题
注射机的锁模力必须（ ）型腔内熔体压力与塑料制品及浇注系统在分型面上的投影面积之和的乘积。<reserved_107>大于</s><s> <reserved_106>回答下列问题
选择精基准的原则中讲到的“基准统一”是指（ ）。<reserved_107>在多数工序中采用同一组精基准定位</s><s> <reserved_106>判断以下陈述是否正确。
在其他条件相同的情况下，金属材料中，硬材料的冲裁合理间隙大于软材料合理间隙。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
（ ）是靠刀具和工件之间作相对运动来完成的。<reserved_107>金属切削加工</s><s> <reserved_106>判断以下陈述是否正确。
采用点浇口的特点就是可以获得外观清晰、表面光泽的塑件。<reserved_107>正确</s><s> <reserved_106>回答下列问题
为了使零件轴向定位可靠，轴肩或轴环过渡圆角的半径应比毂孔的圆角半径（ ）。<reserved_107>大</s><s> <reserved_106>回答下列问题
分割体操作将目标体作分割处理，操作结果会导致模型（ ）。<reserved_107>非参数化</s><s> <reserved_106>回答下列问题
组合体相邻表面可能形成（ ）关系。<reserved_107>相交;共面;相切</s><s> <reserved_106>判断以下陈述是否正确。
多数塑料的弹性模量和强度较低，受力时容易变形和破坏。<reserved_107>正确</s><s> <reserved_106>回答下列问题
下列不属于合金元素在钢中的作用是？（ ）。<reserved_107>强化铁素体，使钢的塑性、韧性下降</s><s> <reserved_106>回答下列问题
压缩模塑的制品变色产生的主要原因有（ ）。<reserved_107>模温太高</s><s> <reserved_106>回答下列问题
在拆画零件图时，下列说法错误的是（ ）。<reserved_107>画装配图时被简化的零件上的某些结构，如倒角，在零件图中无须表示出来</s><s> <reserved_106>回答下列问题
选用曲柄压力机时，对于大型复杂拉深件和成形件，主要选用（ ）。<reserved_107>双动或三动拉深压力机</s><s> <reserved_106>判断以下陈述是否正确。
塑料模试模时，塑件溢料和飞边的原因有可能是注塑压力太高。<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
铰链四杆机构中，若最短杆为机架，则该机构一定是双曲柄机构。<reserved_107>错误</s><s> <reserved_106>回答下列问题
团队与群体的区别有（ ）。<reserved_107>目标不同;技能结构不同;责任担当不同;领导方式不同</s><s> <reserved_106>回答下列问题
哪种模具定位件的形状应与前到工序冲压件形状相吻合（ ）<reserved_107>冲孔模</s><s> <reserved_106>判断以下陈述是否正确。
材料回弹的解决办法是准确计算毛坯尺寸<reserved_107>错误</s><s> <reserved_106>回答下列问题
提高电火花加工的生产效率应采取的措施有（ ）。<reserved_107>提高脉冲频率;增大单个脉冲能量</s><s> <reserved_106>回答下列问题
以下不适合线切割加工的对象是（ ）。<reserved_107>阶梯孔、阶梯轴;型腔;非导电材料</s><s> <reserved_106>判断以下陈述是否正确。
对放嵌件的注塑制品，由于塑料的收缩率远比金属的小，所以容易导致扭曲变形。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
增大刀具的前角，切屑（ ）。<reserved_107>变形小</s><s> <reserved_106>判断以下陈述是否正确。
对于热敏性塑料和流动性差的塑料的注射模塑一般选用柱塞式注射机成型。<reserved_107>错误</s><s> <reserved_106>回答下列问题
下列属于特征操作的特征有（ ）。<reserved_107>Sew;Instance</s><s> <reserved_106>回答下列问题
拉深模试冲时，由于拉深间隙太小，而产生拉深件，拉深太大，应（ ）。<reserved_107>整修凸模加大间隙</s><s> <reserved_106>回答下列问题
安排加工顺序时,正确的原则是（ ）。<reserved_107>先粗后精</s><s> <reserved_106>判断以下陈述是否正确。
冲模装配常用的工艺方法仍然是互换法，修配法和调整法<reserved_107>正确</s><s> <reserved_106>判断以下陈述是否正确。
使用【抽取曲线】（Extract Curve）命令得到的曲线作为一个特征存在于部件导航器。<reserved_107>正确</s><s> <reserved_106>回答下列问题
工业“三废”指的是（ ）。<reserved_107>废气;废水;废渣</s><s> <reserved_106>回答下列问题
产生加工误差的因素有（ ）。<reserved_107>工艺系统的几何误差;工艺系统的受力变形所引起的误差;工件内应力所引起的误差;工艺系统的受热变形所引起的误差</s><s> <reserved_106>判断以下陈述是否正确。
以职谋私型职业道德境界是高尚的共产主义道德品质在职业道德中的现实体现，是社会主义职业道德的最高境界。<reserved_107>错误。</s><s> <reserved_106>回答下列问题
钻孔歪斜的预防措施是（ ）。<reserved_107>修磨横刃使其定心准确</s><s> <reserved_106>回答下列问题
采用热流道浇注系统注射模生产塑件可节省（ ）。<reserved_107>原料</s><s> <reserved_106>回答下列问题
在精密夹具装配的调整过程中，应选择（ ）的元件为补偿件。<reserved_107>最后装配</s><s> <reserved_106>铆接固定？
测量样板或对模具轮廓检验一般采用、极坐标测量法或采用光（ ）学接触法测量。<reserved_107>直角坐标测量法</s><s> <reserved_106>回答下列问题
精度高、形状复杂的冲件一般采用（ ）凹模形式。<reserved_107>直筒式刃口</s><s> <reserved_106>回答下列问题
金属切削过程实质上是一种（ ）的过程<reserved_107>挤压</s><s> <reserved_106>回答下列问题
下列胀形时工件不是主要受力的是（ ）。<reserved_107>径向拉应力;径向压应力;切向压应力</s><s> <reserved_106>回答下列问题
合理利用覆盖效应，有利（ ）电极损耗。<reserved_107>降低</s><s> <reserved_106>回答下列问题
用同一平面上的三个支承点对工件的平面进行定位，能限制其（ ）自由度。<reserved_107>一个移动两个转动</s><s> <reserved_106>判断以下陈述是否正确。
工艺尺寸链中封闭环的确定是随着零件加工方案的变化而改变的。<reserved_107>正确</s><s> <reserved_106>回答下列问题
在工程图中标注几何公差时，不需要定义基准参照的几何公差是（ ）。<reserved_107>圆柱度</s><s> <reserved_106>回答下列问题
拉深成形所用的冲模叫（ ）<reserved_107>拉深模</s><s> <reserved_106>回答下列问题
工件渗碳后的热处理工艺通常为（ ）及低温回火<reserved_107>淬火</s><s> <reserved_106>回答下列问题
下列加工方法哪一个不属于孔的精密加工。（ ）。<reserved_107>精车</s><s> <reserved_106>回答下列问题
以下哪些是创建【直纹】（Rule）特征时的对齐方式：<reserved_107>等参数;等（圆）弧长;根据点;脊线</s><s> <reserved_106>判断以下陈述是否正确。
机械制图中标注绘图比例为2：1，表示所绘制图形是放大的图形，其绘制的尺寸是零件实物尺寸的2倍。<reserved_107>正确</s><s> <reserved_106>回答下列问题
下列不属于提高加工精度措施的是（ ）。<reserved_107>修改图纸</s><s> <reserved_106>判断以下陈述是否正确。
钻小孔时需选择高转速钻床。<reserved_107>正确</s><s> <reserved_106>回答下列问题
Ra在代号中仅用数值表示，单位为（ ）。<reserved_107>μm</s><s> <reserved_106>回答下列问题
对工件抛光的作用是（ ）。<reserved_107>提高尺寸精度;改善表面粗糙度;提高耐腐蚀性能;提高耐磨性能;提高工件的疲劳强度</s><s> <reserved_106>回答下列问题
当前最广泛采用的生活垃圾分选方法是（ ）。<reserved_107>人工手选</s><s> <reserved_106>回答下列问题
从概念上表示钢在淬火后获得马氏体组织的能力，冷却后获得全部马氏体组织的这部分截面厚度称为（ ）层。<reserved_107>淬透</s><s> <reserved_106>回答下列问题
当一对标准的圆锥齿轮传动时，必须使两齿轮的（ ）相切。<reserved_107>分度圆</s><s> <reserved_106>回答下列问题
铣床主要加工什么类零件？（ ）。<reserved_107>非回转体</s><s> <reserved_106>回答下列问题
下列属于管材挤出成型主要工艺参数的是（ ）。<reserved_107>温度;压力;挤出速度</s><s> <reserved_106>判断以下陈述是否正确。
在研磨中，应根据不同的精度要求，采用不同的研具和方法。<reserved_107>正确。</s><s> <reserved_106>回答下列问题
消除淬火应力属于（ ）<reserved_107>回火</s><s> <reserved_106>判断以下陈述是否正确。
裂纹的主要产生原因是由于制品所受应力太大或者应力集中所致。<reserved_107>正确</s><s><reserved_106>
11/18 11:39:05 - mmengine - INFO - before_train in EvaluateChatHook .
11/18 11:39:17 - mmengine - INFO - Sample output:
<reserved_106>请给我介绍五个上海的景点<reserved_107>当然可以！以下是五个上海著名的旅游景点：

1. 外滩：外滩是上海最著名的景点之一，位于黄浦江畔。这里有许多历史悠久的建筑，如和平饭店、上海海关大楼等。在外滩，你可以欣赏到

11/18 11:39:21 - mmengine - INFO - Sample output:
 <reserved_106>Please tell me five scenic spots in Shanghai<reserved_107>1. The Bund (Shanghai)
2. The Oriental Pearl Tower (Shanghai)
3. The Shanghai World Expo Park
4. The Shanghai Zoo
5. The Shanghai Ocean Park</s>

11/18 11:39:21 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
11/18 11:39:21 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
11/18 11:39:21 - mmengine - INFO - Checkpoints will be saved to /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32.
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/mmengine/optim/scheduler/param_scheduler.py:198: UserWarning: Detected call of `scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `scheduler.step()`. Failure to do this will result in PyTorch skipping the first value of the parameter value schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
11/18 11:40:33 - mmengine - INFO - Epoch(train)   [1][10/80]  lr: 2.0000e-04  eta: 15:58:39  time: 7.1989  data_time: 0.0040  memory: 13284  loss: 3.6363
11/18 11:41:45 - mmengine - INFO - Epoch(train)   [1][20/80]  lr: 2.0000e-04  eta: 15:59:04  time: 7.2233  data_time: 0.0037  memory: 13286  loss: 3.3531  grad_norm: 1.3546
11/18 11:42:55 - mmengine - INFO - Epoch(train)   [1][30/80]  lr: 1.9999e-04  eta: 15:49:10  time: 7.0148  data_time: 0.0035  memory: 13286  loss: 3.1210  grad_norm: 1.3546
11/18 11:44:06 - mmengine - INFO - Epoch(train)   [1][40/80]  lr: 1.9999e-04  eta: 15:45:35  time: 7.0735  data_time: 0.0035  memory: 13286  loss: 2.7549  grad_norm: 1.0029
11/18 11:45:17 - mmengine - INFO - Epoch(train)   [1][50/80]  lr: 1.9998e-04  eta: 15:42:59  time: 7.0739  data_time: 0.0033  memory: 13286  loss: 2.7942  grad_norm: 0.8304
11/18 11:46:27 - mmengine - INFO - Epoch(train)   [1][60/80]  lr: 1.9998e-04  eta: 15:39:31  time: 7.0138  data_time: 0.0034  memory: 13286  loss: 2.6618  grad_norm: 0.8304
11/18 11:47:38 - mmengine - INFO - Epoch(train)   [1][70/80]  lr: 1.9997e-04  eta: 15:37:49  time: 7.0719  data_time: 0.0033  memory: 13286  loss: 2.5862  grad_norm: 0.7361
11/18 11:48:48 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 11:48:48 - mmengine - INFO - Epoch(train)   [1][80/80]  lr: 1.9996e-04  eta: 15:36:14  time: 7.0720  data_time: 0.0033  memory: 13286  loss: 2.4187  grad_norm: 0.6443
11/18 11:48:48 - mmengine - INFO - Saving checkpoint at 1 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 11:50:04 - mmengine - INFO - Epoch(train)   [2][10/80]  lr: 1.9995e-04  eta: 15:34:01  time: 7.0215  data_time: 0.0060  memory: 13286  loss: 2.3634  grad_norm: 0.6443
11/18 11:51:15 - mmengine - INFO - Epoch(train)   [2][20/80]  lr: 1.9993e-04  eta: 15:33:40  time: 7.1488  data_time: 0.0034  memory: 13286  loss: 2.4434  grad_norm: 0.5887
11/18 11:52:25 - mmengine - INFO - Epoch(train)   [2][30/80]  lr: 1.9992e-04  eta: 15:31:36  time: 7.0165  data_time: 0.0034  memory: 13286  loss: 2.4501  grad_norm: 0.5887
11/18 11:53:36 - mmengine - INFO - Epoch(train)   [2][40/80]  lr: 1.9990e-04  eta: 15:30:19  time: 7.0758  data_time: 0.0034  memory: 13286  loss: 2.5282  grad_norm: 0.5457
11/18 11:54:47 - mmengine - INFO - Epoch(train)   [2][50/80]  lr: 1.9988e-04  eta: 15:29:03  time: 7.0746  data_time: 0.0033  memory: 13286  loss: 2.4157  grad_norm: 0.5130
11/18 11:55:57 - mmengine - INFO - Epoch(train)   [2][60/80]  lr: 1.9987e-04  eta: 15:27:16  time: 7.0192  data_time: 0.0032  memory: 13286  loss: 2.3007  grad_norm: 0.5130
11/18 11:57:08 - mmengine - INFO - Epoch(train)   [2][70/80]  lr: 1.9985e-04  eta: 15:26:03  time: 7.0739  data_time: 0.0032  memory: 13286  loss: 2.2617  grad_norm: 0.4866
11/18 11:58:19 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 11:58:19 - mmengine - INFO - Epoch(train)   [2][80/80]  lr: 1.9982e-04  eta: 15:24:53  time: 7.0805  data_time: 0.0032  memory: 13286  loss: 2.2614  grad_norm: 0.4602
11/18 11:58:19 - mmengine - INFO - Saving checkpoint at 2 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 11:59:32 - mmengine - INFO - Epoch(train)   [3][10/80]  lr: 1.9980e-04  eta: 15:23:17  time: 7.0221  data_time: 0.0035  memory: 13286  loss: 2.2402  grad_norm: 0.4602
11/18 12:00:43 - mmengine - INFO - Epoch(train)   [3][20/80]  lr: 1.9978e-04  eta: 15:22:39  time: 7.1518  data_time: 0.0033  memory: 13286  loss: 2.1830  grad_norm: 0.3405
11/18 12:01:53 - mmengine - INFO - Epoch(train)   [3][30/80]  lr: 1.9975e-04  eta: 15:21:04  time: 7.0187  data_time: 0.0032  memory: 13286  loss: 2.3037  grad_norm: 0.3405
11/18 12:03:04 - mmengine - INFO - Epoch(train)   [3][40/80]  lr: 1.9973e-04  eta: 15:19:52  time: 7.0744  data_time: 0.0036  memory: 13286  loss: 2.2095  grad_norm: 0.2908
11/18 12:04:15 - mmengine - INFO - Epoch(train)   [3][50/80]  lr: 1.9970e-04  eta: 15:18:40  time: 7.0732  data_time: 0.0036  memory: 13286  loss: 2.1770  grad_norm: 0.2583
11/18 12:05:25 - mmengine - INFO - Epoch(train)   [3][60/80]  lr: 1.9967e-04  eta: 15:17:10  time: 7.0198  data_time: 0.0035  memory: 13286  loss: 2.1379  grad_norm: 0.2583
11/18 12:06:36 - mmengine - INFO - Epoch(train)   [3][70/80]  lr: 1.9964e-04  eta: 15:15:58  time: 7.0695  data_time: 0.0034  memory: 13286  loss: 2.1367  grad_norm: 0.2278
11/18 12:07:47 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 12:07:47 - mmengine - INFO - Epoch(train)   [3][80/80]  lr: 1.9960e-04  eta: 15:14:47  time: 7.0735  data_time: 0.0034  memory: 13286  loss: 2.3012  grad_norm: 0.2170
11/18 12:07:47 - mmengine - INFO - Saving checkpoint at 3 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 12:08:59 - mmengine - INFO - Epoch(train)   [4][10/80]  lr: 1.9957e-04  eta: 15:13:06  time: 6.9756  data_time: 0.0041  memory: 13286  loss: 2.1594  grad_norm: 0.2170
11/18 12:10:11 - mmengine - INFO - Epoch(train)   [4][20/80]  lr: 1.9953e-04  eta: 15:12:16  time: 7.1367  data_time: 0.0033  memory: 13286  loss: 2.1266  grad_norm: 0.2057
11/18 12:11:21 - mmengine - INFO - Epoch(train)   [4][30/80]  lr: 1.9950e-04  eta: 15:10:48  time: 7.0124  data_time: 0.0034  memory: 13286  loss: 2.0620  grad_norm: 0.2057
11/18 12:12:31 - mmengine - INFO - Epoch(train)   [4][40/80]  lr: 1.9946e-04  eta: 15:09:36  time: 7.0639  data_time: 0.0037  memory: 13286  loss: 2.0537  grad_norm: 0.1934
11/18 12:13:42 - mmengine - INFO - Epoch(train)   [4][50/80]  lr: 1.9942e-04  eta: 15:08:27  time: 7.0778  data_time: 0.0034  memory: 13286  loss: 2.0188  grad_norm: 0.1799
11/18 12:14:52 - mmengine - INFO - Epoch(train)   [4][60/80]  lr: 1.9938e-04  eta: 15:07:03  time: 7.0172  data_time: 0.0032  memory: 13286  loss: 2.1743  grad_norm: 0.1799
11/18 12:16:03 - mmengine - INFO - Epoch(train)   [4][70/80]  lr: 1.9934e-04  eta: 15:05:54  time: 7.0740  data_time: 0.0033  memory: 13286  loss: 2.0680  grad_norm: 0.1665
11/18 12:17:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 12:17:14 - mmengine - INFO - Epoch(train)   [4][80/80]  lr: 1.9929e-04  eta: 15:04:39  time: 7.0514  data_time: 0.0032  memory: 13286  loss: 2.0443  grad_norm: 0.1595
11/18 12:17:14 - mmengine - INFO - Saving checkpoint at 4 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 12:18:27 - mmengine - INFO - Epoch(train)   [5][10/80]  lr: 1.9925e-04  eta: 15:03:16  time: 7.0144  data_time: 0.0036  memory: 13286  loss: 1.9943  grad_norm: 0.1595
11/18 12:19:38 - mmengine - INFO - Epoch(train)   [5][20/80]  lr: 1.9920e-04  eta: 15:02:23  time: 7.1424  data_time: 0.0032  memory: 13286  loss: 2.0442  grad_norm: 0.1601
11/18 12:20:48 - mmengine - INFO - Epoch(train)   [5][30/80]  lr: 1.9916e-04  eta: 15:01:00  time: 7.0148  data_time: 0.0032  memory: 13286  loss: 1.9153  grad_norm: 0.1601
11/18 12:21:59 - mmengine - INFO - Epoch(train)   [5][40/80]  lr: 1.9911e-04  eta: 14:59:51  time: 7.0728  data_time: 0.0032  memory: 13286  loss: 1.9514  grad_norm: 0.1604
11/18 12:23:10 - mmengine - INFO - Epoch(train)   [5][50/80]  lr: 1.9906e-04  eta: 14:58:42  time: 7.0783  data_time: 0.0032  memory: 13286  loss: 1.8508  grad_norm: 0.1611
11/18 12:24:20 - mmengine - INFO - Epoch(train)   [5][60/80]  lr: 1.9901e-04  eta: 14:57:21  time: 7.0150  data_time: 0.0032  memory: 13286  loss: 1.9968  grad_norm: 0.1611
11/18 12:25:31 - mmengine - INFO - Epoch(train)   [5][70/80]  lr: 1.9895e-04  eta: 14:56:12  time: 7.0728  data_time: 0.0031  memory: 13286  loss: 1.8837  grad_norm: 0.1644
11/18 12:26:41 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 12:26:41 - mmengine - INFO - Epoch(train)   [5][80/80]  lr: 1.9890e-04  eta: 14:54:56  time: 7.0359  data_time: 0.0032  memory: 13286  loss: 1.9141  grad_norm: 0.1646
11/18 12:26:41 - mmengine - INFO - Saving checkpoint at 5 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 12:27:54 - mmengine - INFO - Epoch(train)   [6][10/80]  lr: 1.9884e-04  eta: 14:53:36  time: 7.0158  data_time: 0.0038  memory: 13286  loss: 1.8946  grad_norm: 0.1646
11/18 12:29:06 - mmengine - INFO - Epoch(train)   [6][20/80]  lr: 1.9878e-04  eta: 14:52:40  time: 7.1441  data_time: 0.0033  memory: 13286  loss: 1.7468  grad_norm: 0.1633
11/18 12:30:16 - mmengine - INFO - Epoch(train)   [6][30/80]  lr: 1.9873e-04  eta: 14:51:20  time: 7.0143  data_time: 0.0032  memory: 13286  loss: 1.7684  grad_norm: 0.1633
11/18 12:31:27 - mmengine - INFO - Epoch(train)   [6][40/80]  lr: 1.9867e-04  eta: 14:50:11  time: 7.0747  data_time: 0.0032  memory: 13286  loss: 1.8035  grad_norm: 0.1649
11/18 12:32:37 - mmengine - INFO - Epoch(train)   [6][50/80]  lr: 1.9860e-04  eta: 14:49:02  time: 7.0744  data_time: 0.0032  memory: 13286  loss: 1.7701  grad_norm: 0.1687
11/18 12:33:48 - mmengine - INFO - Epoch(train)   [6][60/80]  lr: 1.9854e-04  eta: 14:47:43  time: 7.0147  data_time: 0.0033  memory: 13286  loss: 1.7377  grad_norm: 0.1687
11/18 12:34:58 - mmengine - INFO - Epoch(train)   [6][70/80]  lr: 1.9848e-04  eta: 14:46:34  time: 7.0750  data_time: 0.0032  memory: 13286  loss: 1.7247  grad_norm: 0.1743
11/18 12:36:09 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 12:36:09 - mmengine - INFO - Epoch(train)   [6][80/80]  lr: 1.9841e-04  eta: 14:45:27  time: 7.0846  data_time: 0.0032  memory: 13286  loss: 1.7550  grad_norm: 0.1819
11/18 12:36:09 - mmengine - INFO - Saving checkpoint at 6 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 12:37:23 - mmengine - INFO - Epoch(train)   [7][10/80]  lr: 1.9835e-04  eta: 14:44:09  time: 7.0183  data_time: 0.0066  memory: 13286  loss: 1.5603  grad_norm: 0.1819
11/18 12:38:34 - mmengine - INFO - Epoch(train)   [7][20/80]  lr: 1.9828e-04  eta: 14:43:11  time: 7.1467  data_time: 0.0033  memory: 13286  loss: 1.6113  grad_norm: 0.1873
11/18 12:39:45 - mmengine - INFO - Epoch(train)   [7][30/80]  lr: 1.9821e-04  eta: 14:41:53  time: 7.0173  data_time: 0.0033  memory: 13286  loss: 1.5988  grad_norm: 0.1873
11/18 12:40:55 - mmengine - INFO - Epoch(train)   [7][40/80]  lr: 1.9814e-04  eta: 14:40:44  time: 7.0760  data_time: 0.0032  memory: 13286  loss: 1.5613  grad_norm: 0.1931
11/18 12:42:06 - mmengine - INFO - Epoch(train)   [7][50/80]  lr: 1.9806e-04  eta: 14:39:36  time: 7.0815  data_time: 0.0033  memory: 13286  loss: 1.6186  grad_norm: 0.1996
11/18 12:43:16 - mmengine - INFO - Epoch(train)   [7][60/80]  lr: 1.9799e-04  eta: 14:38:18  time: 7.0146  data_time: 0.0034  memory: 13286  loss: 1.4837  grad_norm: 0.1996
11/18 12:44:27 - mmengine - INFO - Epoch(train)   [7][70/80]  lr: 1.9792e-04  eta: 14:37:09  time: 7.0735  data_time: 0.0032  memory: 13286  loss: 1.5680  grad_norm: 0.2069
11/18 12:45:38 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 12:45:38 - mmengine - INFO - Epoch(train)   [7][80/80]  lr: 1.9784e-04  eta: 14:35:59  time: 7.0748  data_time: 0.0033  memory: 13286  loss: 1.5640  grad_norm: 0.2154
11/18 12:45:38 - mmengine - INFO - Saving checkpoint at 7 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 12:46:51 - mmengine - INFO - Epoch(train)   [8][10/80]  lr: 1.9776e-04  eta: 14:34:42  time: 7.0151  data_time: 0.0037  memory: 13286  loss: 1.3465  grad_norm: 0.2154
11/18 12:48:03 - mmengine - INFO - Epoch(train)   [8][20/80]  lr: 1.9768e-04  eta: 14:33:42  time: 7.1462  data_time: 0.0033  memory: 13286  loss: 1.4269  grad_norm: 0.2243
11/18 12:49:13 - mmengine - INFO - Epoch(train)   [8][30/80]  lr: 1.9760e-04  eta: 14:32:23  time: 6.9983  data_time: 0.0034  memory: 13286  loss: 1.3761  grad_norm: 0.2243
11/18 12:50:23 - mmengine - INFO - Epoch(train)   [8][40/80]  lr: 1.9752e-04  eta: 14:31:14  time: 7.0760  data_time: 0.0032  memory: 13286  loss: 1.4222  grad_norm: 0.2323
11/18 12:51:34 - mmengine - INFO - Epoch(train)   [8][50/80]  lr: 1.9744e-04  eta: 14:30:04  time: 7.0723  data_time: 0.0032  memory: 13286  loss: 1.3204  grad_norm: 0.2438
11/18 12:52:44 - mmengine - INFO - Epoch(train)   [8][60/80]  lr: 1.9735e-04  eta: 14:28:48  time: 7.0135  data_time: 0.0032  memory: 13286  loss: 1.3009  grad_norm: 0.2438
11/18 12:53:55 - mmengine - INFO - Epoch(train)   [8][70/80]  lr: 1.9727e-04  eta: 14:27:38  time: 7.0694  data_time: 0.0032  memory: 13286  loss: 1.2717  grad_norm: 0.2565
11/18 12:55:06 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 12:55:06 - mmengine - INFO - Epoch(train)   [8][80/80]  lr: 1.9718e-04  eta: 14:26:28  time: 7.0726  data_time: 0.0032  memory: 13286  loss: 1.2288  grad_norm: 0.2701
11/18 12:55:06 - mmengine - INFO - Saving checkpoint at 8 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 12:56:19 - mmengine - INFO - Epoch(train)   [9][10/80]  lr: 1.9709e-04  eta: 14:25:13  time: 7.0225  data_time: 0.0065  memory: 13286  loss: 1.2225  grad_norm: 0.2701
11/18 12:57:31 - mmengine - INFO - Epoch(train)   [9][20/80]  lr: 1.9700e-04  eta: 14:24:11  time: 7.1413  data_time: 0.0034  memory: 13286  loss: 1.1063  grad_norm: 0.2819
11/18 12:58:41 - mmengine - INFO - Epoch(train)   [9][30/80]  lr: 1.9691e-04  eta: 14:22:55  time: 7.0155  data_time: 0.0032  memory: 13286  loss: 1.0773  grad_norm: 0.2819
11/18 12:59:52 - mmengine - INFO - Epoch(train)   [9][40/80]  lr: 1.9682e-04  eta: 14:21:46  time: 7.0777  data_time: 0.0032  memory: 13286  loss: 1.0401  grad_norm: 0.2953
11/18 13:01:02 - mmengine - INFO - Epoch(train)   [9][50/80]  lr: 1.9673e-04  eta: 14:20:36  time: 7.0730  data_time: 0.0033  memory: 13286  loss: 1.1714  grad_norm: 0.3110
11/18 13:02:13 - mmengine - INFO - Epoch(train)   [9][60/80]  lr: 1.9663e-04  eta: 14:19:21  time: 7.0180  data_time: 0.0032  memory: 13286  loss: 0.9946  grad_norm: 0.3110
11/18 13:03:23 - mmengine - INFO - Epoch(train)   [9][70/80]  lr: 1.9653e-04  eta: 14:18:12  time: 7.0792  data_time: 0.0033  memory: 13286  loss: 1.0133  grad_norm: 0.3282
11/18 13:04:34 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:04:34 - mmengine - INFO - Epoch(train)   [9][80/80]  lr: 1.9644e-04  eta: 14:17:02  time: 7.0759  data_time: 0.0032  memory: 13286  loss: 1.0514  grad_norm: 0.3473
11/18 13:04:34 - mmengine - INFO - Saving checkpoint at 9 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 13:05:48 - mmengine - INFO - Epoch(train)  [10][10/80]  lr: 1.9634e-04  eta: 14:15:47  time: 7.0193  data_time: 0.0035  memory: 13286  loss: 0.8956  grad_norm: 0.3473
11/18 13:06:59 - mmengine - INFO - Epoch(train)  [10][20/80]  lr: 1.9624e-04  eta: 14:14:45  time: 7.1471  data_time: 0.0033  memory: 13286  loss: 0.8964  grad_norm: 0.3625
11/18 13:08:09 - mmengine - INFO - Epoch(train)  [10][30/80]  lr: 1.9613e-04  eta: 14:13:30  time: 7.0188  data_time: 0.0032  memory: 13286  loss: 0.8694  grad_norm: 0.3625
11/18 13:09:20 - mmengine - INFO - Epoch(train)  [10][40/80]  lr: 1.9603e-04  eta: 14:12:20  time: 7.0732  data_time: 0.0032  memory: 13286  loss: 0.7693  grad_norm: 0.3745
11/18 13:10:31 - mmengine - INFO - Epoch(train)  [10][50/80]  lr: 1.9593e-04  eta: 14:11:11  time: 7.0739  data_time: 0.0032  memory: 13286  loss: 0.8962  grad_norm: 0.3940
11/18 13:11:41 - mmengine - INFO - Epoch(train)  [10][60/80]  lr: 1.9582e-04  eta: 14:09:56  time: 7.0189  data_time: 0.0032  memory: 13286  loss: 0.7097  grad_norm: 0.3940
11/18 13:12:52 - mmengine - INFO - Epoch(train)  [10][70/80]  lr: 1.9571e-04  eta: 14:08:46  time: 7.0761  data_time: 0.0032  memory: 13286  loss: 0.8074  grad_norm: 0.4196
11/18 13:14:03 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:14:03 - mmengine - INFO - Epoch(train)  [10][80/80]  lr: 1.9561e-04  eta: 14:07:36  time: 7.0717  data_time: 0.0033  memory: 13286  loss: 0.8661  grad_norm: 0.4407
11/18 13:14:03 - mmengine - INFO - Saving checkpoint at 10 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 13:15:16 - mmengine - INFO - Epoch(train)  [11][10/80]  lr: 1.9550e-04  eta: 14:06:22  time: 7.0166  data_time: 0.0039  memory: 13286  loss: 0.6525  grad_norm: 0.4407
11/18 13:16:28 - mmengine - INFO - Epoch(train)  [11][20/80]  lr: 1.9539e-04  eta: 14:05:18  time: 7.1481  data_time: 0.0061  memory: 13286  loss: 0.6340  grad_norm: 0.4543
11/18 13:17:38 - mmengine - INFO - Epoch(train)  [11][30/80]  lr: 1.9527e-04  eta: 14:04:04  time: 7.0156  data_time: 0.0033  memory: 13286  loss: 0.6919  grad_norm: 0.4543
11/18 13:18:49 - mmengine - INFO - Epoch(train)  [11][40/80]  lr: 1.9516e-04  eta: 14:02:54  time: 7.0732  data_time: 0.0033  memory: 13286  loss: 0.6703  grad_norm: 0.4699
11/18 13:19:59 - mmengine - INFO - Epoch(train)  [11][50/80]  lr: 1.9504e-04  eta: 14:01:44  time: 7.0750  data_time: 0.0033  memory: 13286  loss: 0.6577  grad_norm: 0.4858
11/18 13:21:10 - mmengine - INFO - Epoch(train)  [11][60/80]  lr: 1.9493e-04  eta: 14:00:30  time: 7.0207  data_time: 0.0033  memory: 13286  loss: 0.6229  grad_norm: 0.4858
11/18 13:22:20 - mmengine - INFO - Epoch(train)  [11][70/80]  lr: 1.9481e-04  eta: 13:59:21  time: 7.0777  data_time: 0.0032  memory: 13286  loss: 0.5610  grad_norm: 0.4990
11/18 13:23:31 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:23:31 - mmengine - INFO - Epoch(train)  [11][80/80]  lr: 1.9469e-04  eta: 13:58:10  time: 7.0666  data_time: 0.0033  memory: 13286  loss: 0.6056  grad_norm: 0.5063
11/18 13:23:31 - mmengine - INFO - Saving checkpoint at 11 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 13:24:44 - mmengine - INFO - Epoch(train)  [12][10/80]  lr: 1.9457e-04  eta: 13:56:55  time: 7.0099  data_time: 0.0036  memory: 13286  loss: 0.5081  grad_norm: 0.5063
11/18 13:25:56 - mmengine - INFO - Epoch(train)  [12][20/80]  lr: 1.9445e-04  eta: 13:55:51  time: 7.1449  data_time: 0.0033  memory: 13286  loss: 0.5311  grad_norm: 0.5117
11/18 13:27:06 - mmengine - INFO - Epoch(train)  [12][30/80]  lr: 1.9433e-04  eta: 13:54:37  time: 7.0176  data_time: 0.0032  memory: 13286  loss: 0.4371  grad_norm: 0.5117
11/18 13:28:17 - mmengine - INFO - Epoch(train)  [12][40/80]  lr: 1.9420e-04  eta: 13:53:28  time: 7.0801  data_time: 0.0032  memory: 13286  loss: 0.4554  grad_norm: 0.5204
11/18 13:29:27 - mmengine - INFO - Epoch(train)  [12][50/80]  lr: 1.9408e-04  eta: 13:52:18  time: 7.0712  data_time: 0.0032  memory: 13286  loss: 0.4562  grad_norm: 0.5196
11/18 13:30:37 - mmengine - INFO - Epoch(train)  [12][60/80]  lr: 1.9395e-04  eta: 13:51:03  time: 7.0164  data_time: 0.0032  memory: 13286  loss: 0.4973  grad_norm: 0.5196
11/18 13:31:48 - mmengine - INFO - Epoch(train)  [12][70/80]  lr: 1.9382e-04  eta: 13:49:53  time: 7.0718  data_time: 0.0033  memory: 13286  loss: 0.4040  grad_norm: 0.5294
11/18 13:32:59 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:32:59 - mmengine - INFO - Epoch(train)  [12][80/80]  lr: 1.9369e-04  eta: 13:48:44  time: 7.0843  data_time: 0.0032  memory: 13286  loss: 0.4685  grad_norm: 0.5280
11/18 13:32:59 - mmengine - INFO - Saving checkpoint at 12 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 13:34:12 - mmengine - INFO - Epoch(train)  [13][10/80]  lr: 1.9356e-04  eta: 13:47:28  time: 6.9887  data_time: 0.0039  memory: 13286  loss: 0.3991  grad_norm: 0.5280
11/18 13:35:24 - mmengine - INFO - Epoch(train)  [13][20/80]  lr: 1.9343e-04  eta: 13:46:23  time: 7.1416  data_time: 0.0034  memory: 13286  loss: 0.3477  grad_norm: 0.5209
11/18 13:36:34 - mmengine - INFO - Epoch(train)  [13][30/80]  lr: 1.9330e-04  eta: 13:45:09  time: 7.0175  data_time: 0.0032  memory: 13286  loss: 0.2530  grad_norm: 0.5209
11/18 13:37:44 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:37:44 - mmengine - INFO - Epoch(train)  [13][40/80]  lr: 1.9316e-04  eta: 13:43:59  time: 7.0709  data_time: 0.0032  memory: 13286  loss: 0.3908  grad_norm: 0.5195
11/18 13:38:55 - mmengine - INFO - Epoch(train)  [13][50/80]  lr: 1.9303e-04  eta: 13:42:50  time: 7.0746  data_time: 0.0032  memory: 13286  loss: 0.3351  grad_norm: 0.5110
11/18 13:40:05 - mmengine - INFO - Epoch(train)  [13][60/80]  lr: 1.9289e-04  eta: 13:41:36  time: 7.0152  data_time: 0.0032  memory: 13286  loss: 0.3337  grad_norm: 0.5110
11/18 13:41:16 - mmengine - INFO - Epoch(train)  [13][70/80]  lr: 1.9275e-04  eta: 13:40:26  time: 7.0747  data_time: 0.0032  memory: 13286  loss: 0.2714  grad_norm: 0.5120
11/18 13:42:27 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:42:27 - mmengine - INFO - Epoch(train)  [13][80/80]  lr: 1.9261e-04  eta: 13:39:16  time: 7.0746  data_time: 0.0032  memory: 13286  loss: 0.3712  grad_norm: 0.5122
11/18 13:42:27 - mmengine - INFO - Saving checkpoint at 13 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 13:43:40 - mmengine - INFO - Epoch(train)  [14][10/80]  lr: 1.9247e-04  eta: 13:38:03  time: 7.0182  data_time: 0.0037  memory: 13286  loss: 0.2803  grad_norm: 0.5122
11/18 13:44:52 - mmengine - INFO - Epoch(train)  [14][20/80]  lr: 1.9233e-04  eta: 13:36:57  time: 7.1473  data_time: 0.0033  memory: 13286  loss: 0.2452  grad_norm: 0.4970
11/18 13:46:02 - mmengine - INFO - Epoch(train)  [14][30/80]  lr: 1.9219e-04  eta: 13:35:44  time: 7.0150  data_time: 0.0032  memory: 13286  loss: 0.2350  grad_norm: 0.4970
11/18 13:47:12 - mmengine - INFO - Epoch(train)  [14][40/80]  lr: 1.9204e-04  eta: 13:34:34  time: 7.0738  data_time: 0.0032  memory: 13286  loss: 0.2170  grad_norm: 0.4889
11/18 13:48:23 - mmengine - INFO - Epoch(train)  [14][50/80]  lr: 1.9189e-04  eta: 13:33:24  time: 7.0764  data_time: 0.0032  memory: 13286  loss: 0.2193  grad_norm: 0.4756
11/18 13:49:33 - mmengine - INFO - Epoch(train)  [14][60/80]  lr: 1.9175e-04  eta: 13:32:11  time: 7.0202  data_time: 0.0033  memory: 13286  loss: 0.2443  grad_norm: 0.4756
11/18 13:50:44 - mmengine - INFO - Epoch(train)  [14][70/80]  lr: 1.9160e-04  eta: 13:31:01  time: 7.0798  data_time: 0.0034  memory: 13286  loss: 0.2218  grad_norm: 0.4623
11/18 13:51:55 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 13:51:55 - mmengine - INFO - Epoch(train)  [14][80/80]  lr: 1.9145e-04  eta: 13:29:51  time: 7.0722  data_time: 0.0032  memory: 13286  loss: 0.2278  grad_norm: 0.4492
11/18 13:51:55 - mmengine - INFO - Saving checkpoint at 14 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 13:53:09 - mmengine - INFO - Epoch(train)  [15][10/80]  lr: 1.9130e-04  eta: 13:28:38  time: 7.0153  data_time: 0.0035  memory: 13286  loss: 0.2005  grad_norm: 0.4492
11/18 13:54:20 - mmengine - INFO - Epoch(train)  [15][20/80]  lr: 1.9115e-04  eta: 13:27:32  time: 7.1473  data_time: 0.0033  memory: 13286  loss: 0.1614  grad_norm: 0.4463
11/18 13:55:30 - mmengine - INFO - Epoch(train)  [15][30/80]  lr: 1.9099e-04  eta: 13:26:19  time: 7.0148  data_time: 0.0032  memory: 13286  loss: 0.1696  grad_norm: 0.4463
11/18 13:56:41 - mmengine - INFO - Epoch(train)  [15][40/80]  lr: 1.9084e-04  eta: 13:25:09  time: 7.0728  data_time: 0.0032  memory: 13286  loss: 0.1625  grad_norm: 0.4266
11/18 13:57:52 - mmengine - INFO - Epoch(train)  [15][50/80]  lr: 1.9068e-04  eta: 13:23:59  time: 7.0761  data_time: 0.0032  memory: 13286  loss: 0.1999  grad_norm: 0.4248
11/18 13:59:02 - mmengine - INFO - Epoch(train)  [15][60/80]  lr: 1.9052e-04  eta: 13:22:46  time: 7.0181  data_time: 0.0032  memory: 13286  loss: 0.1544  grad_norm: 0.4248
11/18 14:00:13 - mmengine - INFO - Epoch(train)  [15][70/80]  lr: 1.9037e-04  eta: 13:21:36  time: 7.0742  data_time: 0.0032  memory: 13286  loss: 0.1558  grad_norm: 0.4072
11/18 14:01:23 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:01:23 - mmengine - INFO - Epoch(train)  [15][80/80]  lr: 1.9021e-04  eta: 13:20:26  time: 7.0743  data_time: 0.0032  memory: 13286  loss: 0.1531  grad_norm: 0.3894
11/18 14:01:23 - mmengine - INFO - Saving checkpoint at 15 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:02:37 - mmengine - INFO - Epoch(train)  [16][10/80]  lr: 1.9005e-04  eta: 13:19:13  time: 7.0205  data_time: 0.0038  memory: 13286  loss: 0.1374  grad_norm: 0.3894
11/18 14:03:49 - mmengine - INFO - Epoch(train)  [16][20/80]  lr: 1.8988e-04  eta: 13:18:07  time: 7.1478  data_time: 0.0033  memory: 13286  loss: 0.1373  grad_norm: 0.3872
11/18 14:04:59 - mmengine - INFO - Epoch(train)  [16][30/80]  lr: 1.8972e-04  eta: 13:16:54  time: 7.0185  data_time: 0.0032  memory: 13286  loss: 0.1046  grad_norm: 0.3872
11/18 14:06:10 - mmengine - INFO - Epoch(train)  [16][40/80]  lr: 1.8956e-04  eta: 13:15:44  time: 7.0781  data_time: 0.0033  memory: 13286  loss: 0.1167  grad_norm: 0.3792
11/18 14:07:20 - mmengine - INFO - Epoch(train)  [16][50/80]  lr: 1.8939e-04  eta: 13:14:34  time: 7.0740  data_time: 0.0032  memory: 13286  loss: 0.1228  grad_norm: 0.3818
11/18 14:08:31 - mmengine - INFO - Epoch(train)  [16][60/80]  lr: 1.8922e-04  eta: 13:13:21  time: 7.0186  data_time: 0.0032  memory: 13286  loss: 0.1264  grad_norm: 0.3818
11/18 14:09:41 - mmengine - INFO - Epoch(train)  [16][70/80]  lr: 1.8905e-04  eta: 13:12:11  time: 7.0762  data_time: 0.0033  memory: 13286  loss: 0.1461  grad_norm: 0.3752
11/18 14:10:52 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:10:52 - mmengine - INFO - Epoch(train)  [16][80/80]  lr: 1.8888e-04  eta: 13:11:01  time: 7.0770  data_time: 0.0032  memory: 13286  loss: 0.1165  grad_norm: 0.3799
11/18 14:10:52 - mmengine - INFO - Saving checkpoint at 16 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:12:06 - mmengine - INFO - Epoch(train)  [17][10/80]  lr: 1.8871e-04  eta: 13:09:49  time: 7.0261  data_time: 0.0069  memory: 13286  loss: 0.0962  grad_norm: 0.3799
11/18 14:13:17 - mmengine - INFO - Epoch(train)  [17][20/80]  lr: 1.8854e-04  eta: 13:08:42  time: 7.1466  data_time: 0.0033  memory: 13286  loss: 0.1004  grad_norm: 0.3738
11/18 14:14:27 - mmengine - INFO - Epoch(train)  [17][30/80]  lr: 1.8837e-04  eta: 13:07:29  time: 7.0162  data_time: 0.0033  memory: 13286  loss: 0.0895  grad_norm: 0.3738
11/18 14:15:38 - mmengine - INFO - Epoch(train)  [17][40/80]  lr: 1.8819e-04  eta: 13:06:19  time: 7.0749  data_time: 0.0032  memory: 13286  loss: 0.0844  grad_norm: 0.3700
11/18 14:16:49 - mmengine - INFO - Epoch(train)  [17][50/80]  lr: 1.8802e-04  eta: 13:05:09  time: 7.0803  data_time: 0.0032  memory: 13286  loss: 0.0837  grad_norm: 0.3837
11/18 14:17:59 - mmengine - INFO - Epoch(train)  [17][60/80]  lr: 1.8784e-04  eta: 13:03:56  time: 7.0152  data_time: 0.0032  memory: 13286  loss: 0.1195  grad_norm: 0.3837
11/18 14:19:10 - mmengine - INFO - Epoch(train)  [17][70/80]  lr: 1.8766e-04  eta: 13:02:46  time: 7.0706  data_time: 0.0033  memory: 13286  loss: 0.0929  grad_norm: 0.3993
11/18 14:20:21 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:20:21 - mmengine - INFO - Epoch(train)  [17][80/80]  lr: 1.8748e-04  eta: 13:01:36  time: 7.0716  data_time: 0.0034  memory: 13286  loss: 0.1114  grad_norm: 0.4336
11/18 14:20:21 - mmengine - INFO - Saving checkpoint at 17 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:21:34 - mmengine - INFO - Epoch(train)  [18][10/80]  lr: 1.8730e-04  eta: 13:00:23  time: 7.0194  data_time: 0.0036  memory: 13286  loss: 0.0755  grad_norm: 0.4336
11/18 14:22:46 - mmengine - INFO - Epoch(train)  [18][20/80]  lr: 1.8712e-04  eta: 12:59:17  time: 7.1521  data_time: 0.0032  memory: 13286  loss: 0.0765  grad_norm: 0.4332
11/18 14:23:56 - mmengine - INFO - Epoch(train)  [18][30/80]  lr: 1.8694e-04  eta: 12:58:04  time: 7.0177  data_time: 0.0032  memory: 13286  loss: 0.0836  grad_norm: 0.4332
11/18 14:25:07 - mmengine - INFO - Epoch(train)  [18][40/80]  lr: 1.8676e-04  eta: 12:56:54  time: 7.0752  data_time: 0.0033  memory: 13286  loss: 0.0691  grad_norm: 0.4605
11/18 14:26:17 - mmengine - INFO - Epoch(train)  [18][50/80]  lr: 1.8657e-04  eta: 12:55:44  time: 7.0752  data_time: 0.0033  memory: 13286  loss: 0.0802  grad_norm: 0.4550
11/18 14:27:28 - mmengine - INFO - Epoch(train)  [18][60/80]  lr: 1.8638e-04  eta: 12:54:32  time: 7.0274  data_time: 0.0032  memory: 13286  loss: 0.0816  grad_norm: 0.4550
11/18 14:28:38 - mmengine - INFO - Epoch(train)  [18][70/80]  lr: 1.8620e-04  eta: 12:53:22  time: 7.0747  data_time: 0.0034  memory: 13286  loss: 0.0747  grad_norm: 0.4548
11/18 14:29:49 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:29:49 - mmengine - INFO - Epoch(train)  [18][80/80]  lr: 1.8601e-04  eta: 12:52:12  time: 7.0736  data_time: 0.0032  memory: 13286  loss: 0.0780  grad_norm: 0.4479
11/18 14:29:49 - mmengine - INFO - Saving checkpoint at 18 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:31:03 - mmengine - INFO - Epoch(train)  [19][10/80]  lr: 1.8582e-04  eta: 12:50:59  time: 7.0134  data_time: 0.0037  memory: 13286  loss: 0.0534  grad_norm: 0.4479
11/18 14:32:14 - mmengine - INFO - Epoch(train)  [19][20/80]  lr: 1.8563e-04  eta: 12:49:52  time: 7.1422  data_time: 0.0032  memory: 13286  loss: 0.0537  grad_norm: 0.4501
11/18 14:33:24 - mmengine - INFO - Epoch(train)  [19][30/80]  lr: 1.8544e-04  eta: 12:48:39  time: 7.0147  data_time: 0.0032  memory: 13286  loss: 0.0471  grad_norm: 0.4501
11/18 14:34:35 - mmengine - INFO - Epoch(train)  [19][40/80]  lr: 1.8524e-04  eta: 12:47:29  time: 7.0725  data_time: 0.0032  memory: 13286  loss: 0.0517  grad_norm: 0.4397
11/18 14:35:46 - mmengine - INFO - Epoch(train)  [19][50/80]  lr: 1.8505e-04  eta: 12:46:19  time: 7.0816  data_time: 0.0032  memory: 13286  loss: 0.0601  grad_norm: 0.4122
11/18 14:36:56 - mmengine - INFO - Epoch(train)  [19][60/80]  lr: 1.8485e-04  eta: 12:45:06  time: 7.0165  data_time: 0.0032  memory: 13286  loss: 0.0545  grad_norm: 0.4122
11/18 14:38:07 - mmengine - INFO - Epoch(train)  [19][70/80]  lr: 1.8466e-04  eta: 12:43:56  time: 7.0687  data_time: 0.0032  memory: 13286  loss: 0.0628  grad_norm: 0.3815
11/18 14:39:17 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:39:17 - mmengine - INFO - Epoch(train)  [19][80/80]  lr: 1.8446e-04  eta: 12:42:46  time: 7.0704  data_time: 0.0032  memory: 13286  loss: 0.0691  grad_norm: 0.3389
11/18 14:39:17 - mmengine - INFO - Saving checkpoint at 19 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:40:31 - mmengine - INFO - Epoch(train)  [20][10/80]  lr: 1.8426e-04  eta: 12:41:33  time: 7.0132  data_time: 0.0035  memory: 13286  loss: 0.0412  grad_norm: 0.3389
11/18 14:41:42 - mmengine - INFO - Epoch(train)  [20][20/80]  lr: 1.8406e-04  eta: 12:40:26  time: 7.1433  data_time: 0.0032  memory: 13286  loss: 0.0441  grad_norm: 0.3266
11/18 14:42:52 - mmengine - INFO - Epoch(train)  [20][30/80]  lr: 1.8386e-04  eta: 12:39:13  time: 7.0147  data_time: 0.0032  memory: 13286  loss: 0.0407  grad_norm: 0.3266
11/18 14:44:03 - mmengine - INFO - Epoch(train)  [20][40/80]  lr: 1.8365e-04  eta: 12:38:03  time: 7.0720  data_time: 0.0031  memory: 13286  loss: 0.0373  grad_norm: 0.2853
11/18 14:45:14 - mmengine - INFO - Epoch(train)  [20][50/80]  lr: 1.8345e-04  eta: 12:36:53  time: 7.0722  data_time: 0.0032  memory: 13286  loss: 0.0413  grad_norm: 0.2737
11/18 14:46:24 - mmengine - INFO - Epoch(train)  [20][60/80]  lr: 1.8325e-04  eta: 12:35:40  time: 7.0131  data_time: 0.0032  memory: 13286  loss: 0.0414  grad_norm: 0.2737
11/18 14:47:35 - mmengine - INFO - Epoch(train)  [20][70/80]  lr: 1.8304e-04  eta: 12:34:30  time: 7.0717  data_time: 0.0032  memory: 13286  loss: 0.0508  grad_norm: 0.2473
11/18 14:48:45 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:48:45 - mmengine - INFO - Epoch(train)  [20][80/80]  lr: 1.8283e-04  eta: 12:33:19  time: 7.0703  data_time: 0.0032  memory: 13286  loss: 0.0476  grad_norm: 0.2341
11/18 14:48:45 - mmengine - INFO - Saving checkpoint at 20 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:49:59 - mmengine - INFO - Epoch(train)  [21][10/80]  lr: 1.8262e-04  eta: 12:32:07  time: 7.0185  data_time: 0.0035  memory: 13286  loss: 0.0372  grad_norm: 0.2341
11/18 14:51:10 - mmengine - INFO - Epoch(train)  [21][20/80]  lr: 1.8241e-04  eta: 12:31:00  time: 7.1440  data_time: 0.0032  memory: 13286  loss: 0.0353  grad_norm: 0.2191
11/18 14:52:21 - mmengine - INFO - Epoch(train)  [21][30/80]  lr: 1.8220e-04  eta: 12:29:47  time: 7.0161  data_time: 0.0032  memory: 13286  loss: 0.0305  grad_norm: 0.2191
11/18 14:53:31 - mmengine - INFO - Epoch(train)  [21][40/80]  lr: 1.8199e-04  eta: 12:28:37  time: 7.0759  data_time: 0.0032  memory: 13286  loss: 0.0343  grad_norm: 0.2303
11/18 14:54:42 - mmengine - INFO - Epoch(train)  [21][50/80]  lr: 1.8178e-04  eta: 12:27:27  time: 7.0714  data_time: 0.0032  memory: 13286  loss: 0.0397  grad_norm: 0.2297
11/18 14:55:52 - mmengine - INFO - Epoch(train)  [21][60/80]  lr: 1.8157e-04  eta: 12:26:14  time: 7.0182  data_time: 0.0032  memory: 13286  loss: 0.0368  grad_norm: 0.2297
11/18 14:57:03 - mmengine - INFO - Epoch(train)  [21][70/80]  lr: 1.8135e-04  eta: 12:25:04  time: 7.0754  data_time: 0.0032  memory: 13286  loss: 0.0265  grad_norm: 0.2176
11/18 14:58:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 14:58:14 - mmengine - INFO - Epoch(train)  [21][80/80]  lr: 1.8114e-04  eta: 12:23:54  time: 7.0723  data_time: 0.0032  memory: 13286  loss: 0.0381  grad_norm: 0.2041
11/18 14:58:14 - mmengine - INFO - Saving checkpoint at 21 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 14:59:27 - mmengine - INFO - Epoch(train)  [22][10/80]  lr: 1.8092e-04  eta: 12:22:42  time: 7.0211  data_time: 0.0035  memory: 13286  loss: 0.0272  grad_norm: 0.2041
11/18 15:00:39 - mmengine - INFO - Epoch(train)  [22][20/80]  lr: 1.8070e-04  eta: 12:21:34  time: 7.1468  data_time: 0.0032  memory: 13286  loss: 0.0431  grad_norm: 0.1955
11/18 15:01:49 - mmengine - INFO - Epoch(train)  [22][30/80]  lr: 1.8048e-04  eta: 12:20:22  time: 7.0175  data_time: 0.0032  memory: 13286  loss: 0.0226  grad_norm: 0.1955
11/18 15:03:00 - mmengine - INFO - Epoch(train)  [22][40/80]  lr: 1.8026e-04  eta: 12:19:12  time: 7.0712  data_time: 0.0032  memory: 13286  loss: 0.0204  grad_norm: 0.1911
11/18 15:04:10 - mmengine - INFO - Epoch(train)  [22][50/80]  lr: 1.8004e-04  eta: 12:18:02  time: 7.0764  data_time: 0.0031  memory: 13286  loss: 0.0380  grad_norm: 0.1818
11/18 15:05:21 - mmengine - INFO - Epoch(train)  [22][60/80]  lr: 1.7982e-04  eta: 12:16:50  time: 7.0184  data_time: 0.0032  memory: 13286  loss: 0.0244  grad_norm: 0.1818
11/18 15:06:31 - mmengine - INFO - Epoch(train)  [22][70/80]  lr: 1.7959e-04  eta: 12:15:39  time: 7.0773  data_time: 0.0031  memory: 13286  loss: 0.0348  grad_norm: 0.1833
11/18 15:07:42 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 15:07:42 - mmengine - INFO - Epoch(train)  [22][80/80]  lr: 1.7937e-04  eta: 12:14:29  time: 7.0754  data_time: 0.0033  memory: 13286  loss: 0.0248  grad_norm: 0.1649
11/18 15:07:42 - mmengine - INFO - Saving checkpoint at 22 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 15:08:56 - mmengine - INFO - Epoch(train)  [23][10/80]  lr: 1.7914e-04  eta: 12:13:17  time: 7.0174  data_time: 0.0066  memory: 13286  loss: 0.0219  grad_norm: 0.1649
11/18 15:10:07 - mmengine - INFO - Epoch(train)  [23][20/80]  lr: 1.7892e-04  eta: 12:12:09  time: 7.1428  data_time: 0.0033  memory: 13286  loss: 0.0289  grad_norm: 0.1654
11/18 15:11:17 - mmengine - INFO - Epoch(train)  [23][30/80]  lr: 1.7869e-04  eta: 12:10:57  time: 7.0143  data_time: 0.0033  memory: 13286  loss: 0.0308  grad_norm: 0.1654
11/18 15:12:28 - mmengine - INFO - Epoch(train)  [23][40/80]  lr: 1.7846e-04  eta: 12:09:47  time: 7.0718  data_time: 0.0033  memory: 13286  loss: 0.0226  grad_norm: 0.1459
11/18 15:13:39 - mmengine - INFO - Epoch(train)  [23][50/80]  lr: 1.7823e-04  eta: 12:08:37  time: 7.0796  data_time: 0.0032  memory: 13286  loss: 0.0251  grad_norm: 0.1229
11/18 15:14:49 - mmengine - INFO - Epoch(train)  [23][60/80]  lr: 1.7800e-04  eta: 12:07:24  time: 7.0144  data_time: 0.0032  memory: 13286  loss: 0.0211  grad_norm: 0.1229
11/18 15:16:00 - mmengine - INFO - Epoch(train)  [23][70/80]  lr: 1.7777e-04  eta: 12:06:14  time: 7.0728  data_time: 0.0033  memory: 13286  loss: 0.0256  grad_norm: 0.1216
11/18 15:17:10 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 15:17:10 - mmengine - INFO - Epoch(train)  [23][80/80]  lr: 1.7753e-04  eta: 12:05:04  time: 7.0718  data_time: 0.0033  memory: 13286  loss: 0.0180  grad_norm: 0.1284
11/18 15:17:10 - mmengine - INFO - Saving checkpoint at 23 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 15:18:24 - mmengine - INFO - Epoch(train)  [24][10/80]  lr: 1.7730e-04  eta: 12:03:52  time: 7.0217  data_time: 0.0036  memory: 13286  loss: 0.0218  grad_norm: 0.1284
11/18 15:19:35 - mmengine - INFO - Epoch(train)  [24][20/80]  lr: 1.7706e-04  eta: 12:02:44  time: 7.1511  data_time: 0.0032  memory: 13286  loss: 0.0232  grad_norm: 0.1244
11/18 15:20:45 - mmengine - INFO - Epoch(train)  [24][30/80]  lr: 1.7683e-04  eta: 12:01:32  time: 7.0152  data_time: 0.0032  memory: 13286  loss: 0.0211  grad_norm: 0.1244
11/18 15:21:56 - mmengine - INFO - Epoch(train)  [24][40/80]  lr: 1.7659e-04  eta: 12:00:22  time: 7.0728  data_time: 0.0032  memory: 13286  loss: 0.0182  grad_norm: 0.1181
11/18 15:23:07 - mmengine - INFO - Epoch(train)  [24][50/80]  lr: 1.7635e-04  eta: 11:59:11  time: 7.0743  data_time: 0.0032  memory: 13286  loss: 0.0217  grad_norm: 0.1114
11/18 15:24:17 - mmengine - INFO - Epoch(train)  [24][60/80]  lr: 1.7611e-04  eta: 11:57:59  time: 7.0168  data_time: 0.0032  memory: 13286  loss: 0.0194  grad_norm: 0.1114
11/18 15:25:28 - mmengine - INFO - Epoch(train)  [24][70/80]  lr: 1.7587e-04  eta: 11:56:49  time: 7.0749  data_time: 0.0032  memory: 13286  loss: 0.0186  grad_norm: 0.0957
11/18 15:26:38 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 15:26:38 - mmengine - INFO - Epoch(train)  [24][80/80]  lr: 1.7563e-04  eta: 11:55:39  time: 7.0730  data_time: 0.0032  memory: 13286  loss: 0.0206  grad_norm: 0.1081
11/18 15:26:38 - mmengine - INFO - Saving checkpoint at 24 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 15:27:52 - mmengine - INFO - Epoch(train)  [25][10/80]  lr: 1.7539e-04  eta: 11:54:27  time: 7.0147  data_time: 0.0037  memory: 13286  loss: 0.0247  grad_norm: 0.1081
11/18 15:29:04 - mmengine - INFO - Epoch(train)  [25][20/80]  lr: 1.7515e-04  eta: 11:53:19  time: 7.1470  data_time: 0.0033  memory: 13286  loss: 0.0214  grad_norm: 0.0985
11/18 15:30:14 - mmengine - INFO - Epoch(train)  [25][30/80]  lr: 1.7490e-04  eta: 11:52:07  time: 7.0166  data_time: 0.0031  memory: 13286  loss: 0.0192  grad_norm: 0.0985
11/18 15:31:24 - mmengine - INFO - Epoch(train)  [25][40/80]  lr: 1.7466e-04  eta: 11:50:56  time: 7.0759  data_time: 0.0032  memory: 13286  loss: 0.0158  grad_norm: 0.0977
11/18 15:32:35 - mmengine - INFO - Epoch(train)  [25][50/80]  lr: 1.7441e-04  eta: 11:49:46  time: 7.0704  data_time: 0.0031  memory: 13286  loss: 0.0206  grad_norm: 0.0990
11/18 15:33:45 - mmengine - INFO - Epoch(train)  [25][60/80]  lr: 1.7416e-04  eta: 11:48:34  time: 7.0183  data_time: 0.0031  memory: 13286  loss: 0.0191  grad_norm: 0.0990
11/18 15:34:56 - mmengine - INFO - Epoch(train)  [25][70/80]  lr: 1.7391e-04  eta: 11:47:24  time: 7.0702  data_time: 0.0032  memory: 13286  loss: 0.0170  grad_norm: 0.0983
11/18 15:36:07 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 15:36:07 - mmengine - INFO - Epoch(train)  [25][80/80]  lr: 1.7366e-04  eta: 11:46:14  time: 7.0754  data_time: 0.0031  memory: 13286  loss: 0.0154  grad_norm: 0.0849
11/18 15:36:07 - mmengine - INFO - Saving checkpoint at 25 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 15:37:20 - mmengine - INFO - Epoch(train)  [26][10/80]  lr: 1.7341e-04  eta: 11:45:02  time: 7.0189  data_time: 0.0066  memory: 13286  loss: 0.0166  grad_norm: 0.0849
11/18 15:38:32 - mmengine - INFO - Epoch(train)  [26][20/80]  lr: 1.7316e-04  eta: 11:43:54  time: 7.1492  data_time: 0.0033  memory: 13286  loss: 0.0128  grad_norm: 0.0836
11/18 15:39:42 - mmengine - INFO - Epoch(train)  [26][30/80]  lr: 1.7291e-04  eta: 11:42:42  time: 7.0157  data_time: 0.0032  memory: 13286  loss: 0.0206  grad_norm: 0.0836
11/18 15:40:52 - mmengine - INFO - Epoch(train)  [26][40/80]  lr: 1.7266e-04  eta: 11:41:31  time: 7.0738  data_time: 0.0032  memory: 13286  loss: 0.0146  grad_norm: 0.0827
11/18 15:42:03 - mmengine - INFO - Epoch(train)  [26][50/80]  lr: 1.7240e-04  eta: 11:40:21  time: 7.0802  data_time: 0.0032  memory: 13286  loss: 0.0142  grad_norm: 0.0827
11/18 15:43:13 - mmengine - INFO - Epoch(train)  [26][60/80]  lr: 1.7215e-04  eta: 11:39:09  time: 7.0127  data_time: 0.0032  memory: 13286  loss: 0.0186  grad_norm: 0.0827
11/18 15:44:24 - mmengine - INFO - Epoch(train)  [26][70/80]  lr: 1.7189e-04  eta: 11:37:59  time: 7.0794  data_time: 0.0032  memory: 13286  loss: 0.0161  grad_norm: 0.0791
11/18 15:45:35 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 15:45:35 - mmengine - INFO - Epoch(train)  [26][80/80]  lr: 1.7163e-04  eta: 11:36:49  time: 7.0733  data_time: 0.0032  memory: 13286  loss: 0.0194  grad_norm: 0.0617
11/18 15:45:35 - mmengine - INFO - Saving checkpoint at 26 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 15:46:48 - mmengine - INFO - Epoch(train)  [27][10/80]  lr: 1.7138e-04  eta: 11:35:37  time: 7.0154  data_time: 0.0042  memory: 13286  loss: 0.0210  grad_norm: 0.0617
11/18 15:48:00 - mmengine - INFO - Epoch(train)  [27][20/80]  lr: 1.7112e-04  eta: 11:34:28  time: 7.1474  data_time: 0.0033  memory: 13286  loss: 0.0100  grad_norm: 0.0581
11/18 15:49:10 - mmengine - INFO - Epoch(train)  [27][30/80]  lr: 1.7086e-04  eta: 11:33:16  time: 7.0118  data_time: 0.0032  memory: 13286  loss: 0.0135  grad_norm: 0.0581
11/18 15:50:20 - mmengine - INFO - Epoch(train)  [27][40/80]  lr: 1.7060e-04  eta: 11:32:06  time: 7.0729  data_time: 0.0031  memory: 13286  loss: 0.0156  grad_norm: 0.0564
11/18 15:51:31 - mmengine - INFO - Epoch(train)  [27][50/80]  lr: 1.7034e-04  eta: 11:30:56  time: 7.0730  data_time: 0.0032  memory: 13286  loss: 0.0177  grad_norm: 0.0514
11/18 15:52:41 - mmengine - INFO - Epoch(train)  [27][60/80]  lr: 1.7007e-04  eta: 11:29:44  time: 7.0106  data_time: 0.0032  memory: 13286  loss: 0.0121  grad_norm: 0.0514
11/18 15:53:52 - mmengine - INFO - Epoch(train)  [27][70/80]  lr: 1.6981e-04  eta: 11:28:33  time: 7.0711  data_time: 0.0034  memory: 13286  loss: 0.0130  grad_norm: 0.0438
11/18 15:55:03 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 15:55:03 - mmengine - INFO - Epoch(train)  [27][80/80]  lr: 1.6954e-04  eta: 11:27:23  time: 7.0686  data_time: 0.0032  memory: 13286  loss: 0.0160  grad_norm: 0.0404
11/18 15:55:03 - mmengine - INFO - Saving checkpoint at 27 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 15:56:16 - mmengine - INFO - Epoch(train)  [28][10/80]  lr: 1.6928e-04  eta: 11:26:11  time: 7.0174  data_time: 0.0067  memory: 13286  loss: 0.0154  grad_norm: 0.0404
11/18 15:57:27 - mmengine - INFO - Epoch(train)  [28][20/80]  lr: 1.6901e-04  eta: 11:25:03  time: 7.1438  data_time: 0.0032  memory: 13286  loss: 0.0132  grad_norm: 0.0383
11/18 15:58:38 - mmengine - INFO - Epoch(train)  [28][30/80]  lr: 1.6875e-04  eta: 11:23:51  time: 7.0147  data_time: 0.0032  memory: 13286  loss: 0.0101  grad_norm: 0.0383
11/18 15:59:48 - mmengine - INFO - Epoch(train)  [28][40/80]  lr: 1.6848e-04  eta: 11:22:40  time: 7.0729  data_time: 0.0031  memory: 13286  loss: 0.0104  grad_norm: 0.0355
11/18 16:00:59 - mmengine - INFO - Epoch(train)  [28][50/80]  lr: 1.6821e-04  eta: 11:21:30  time: 7.0738  data_time: 0.0031  memory: 13286  loss: 0.0091  grad_norm: 0.0325
11/18 16:02:09 - mmengine - INFO - Epoch(train)  [28][60/80]  lr: 1.6794e-04  eta: 11:20:18  time: 7.0131  data_time: 0.0031  memory: 13286  loss: 0.0119  grad_norm: 0.0325
11/18 16:03:20 - mmengine - INFO - Epoch(train)  [28][70/80]  lr: 1.6767e-04  eta: 11:19:08  time: 7.0747  data_time: 0.0032  memory: 13286  loss: 0.0168  grad_norm: 0.0354
11/18 16:04:31 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 16:04:31 - mmengine - INFO - Epoch(train)  [28][80/80]  lr: 1.6740e-04  eta: 11:17:58  time: 7.0786  data_time: 0.0031  memory: 13286  loss: 0.0201  grad_norm: 0.0373
11/18 16:04:31 - mmengine - INFO - Saving checkpoint at 28 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 16:05:44 - mmengine - INFO - Epoch(train)  [29][10/80]  lr: 1.6712e-04  eta: 11:16:46  time: 7.0091  data_time: 0.0037  memory: 13286  loss: 0.0077  grad_norm: 0.0373
11/18 16:06:55 - mmengine - INFO - Epoch(train)  [29][20/80]  lr: 1.6685e-04  eta: 11:15:37  time: 7.1422  data_time: 0.0033  memory: 13286  loss: 0.0084  grad_norm: 0.0331
11/18 16:08:05 - mmengine - INFO - Epoch(train)  [29][30/80]  lr: 1.6657e-04  eta: 11:14:25  time: 7.0119  data_time: 0.0032  memory: 13286  loss: 0.0124  grad_norm: 0.0331
11/18 16:09:16 - mmengine - INFO - Epoch(train)  [29][40/80]  lr: 1.6630e-04  eta: 11:13:15  time: 7.0691  data_time: 0.0033  memory: 13286  loss: 0.0170  grad_norm: 0.0306
11/18 16:10:27 - mmengine - INFO - Epoch(train)  [29][50/80]  lr: 1.6602e-04  eta: 11:12:04  time: 7.0678  data_time: 0.0032  memory: 13286  loss: 0.0119  grad_norm: 0.0310
11/18 16:11:37 - mmengine - INFO - Epoch(train)  [29][60/80]  lr: 1.6575e-04  eta: 11:10:52  time: 7.0114  data_time: 0.0032  memory: 13286  loss: 0.0151  grad_norm: 0.0310
11/18 16:12:48 - mmengine - INFO - Epoch(train)  [29][70/80]  lr: 1.6547e-04  eta: 11:09:42  time: 7.0703  data_time: 0.0032  memory: 13286  loss: 0.0142  grad_norm: 0.0335
11/18 16:13:58 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 16:13:58 - mmengine - INFO - Epoch(train)  [29][80/80]  lr: 1.6519e-04  eta: 11:08:32  time: 7.0753  data_time: 0.0032  memory: 13286  loss: 0.0128  grad_norm: 0.0338
11/18 16:13:58 - mmengine - INFO - Saving checkpoint at 29 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 16:15:12 - mmengine - INFO - Epoch(train)  [30][10/80]  lr: 1.6491e-04  eta: 11:07:20  time: 7.0102  data_time: 0.0036  memory: 13286  loss: 0.0093  grad_norm: 0.0338
11/18 16:16:23 - mmengine - INFO - Epoch(train)  [30][20/80]  lr: 1.6463e-04  eta: 11:06:11  time: 7.1417  data_time: 0.0034  memory: 13286  loss: 0.0100  grad_norm: 0.0332
11/18 16:17:33 - mmengine - INFO - Epoch(train)  [30][30/80]  lr: 1.6435e-04  eta: 11:04:59  time: 7.0147  data_time: 0.0033  memory: 13286  loss: 0.0137  grad_norm: 0.0332
11/18 16:18:44 - mmengine - INFO - Epoch(train)  [30][40/80]  lr: 1.6407e-04  eta: 11:03:49  time: 7.0706  data_time: 0.0033  memory: 13286  loss: 0.0131  grad_norm: 0.0346
11/18 16:19:55 - mmengine - INFO - Epoch(train)  [30][50/80]  lr: 1.6378e-04  eta: 11:02:39  time: 7.0713  data_time: 0.0032  memory: 13286  loss: 0.0101  grad_norm: 0.0341
11/18 16:21:05 - mmengine - INFO - Epoch(train)  [30][60/80]  lr: 1.6350e-04  eta: 11:01:27  time: 7.0098  data_time: 0.0032  memory: 13286  loss: 0.0128  grad_norm: 0.0341
11/18 16:22:16 - mmengine - INFO - Epoch(train)  [30][70/80]  lr: 1.6321e-04  eta: 11:00:16  time: 7.0774  data_time: 0.0032  memory: 13286  loss: 0.0104  grad_norm: 0.0294
11/18 16:23:26 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 16:23:26 - mmengine - INFO - Epoch(train)  [30][80/80]  lr: 1.6293e-04  eta: 10:59:06  time: 7.0740  data_time: 0.0032  memory: 13286  loss: 0.0089  grad_norm: 0.0253
11/18 16:23:26 - mmengine - INFO - Saving checkpoint at 30 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 16:24:40 - mmengine - INFO - Epoch(train)  [31][10/80]  lr: 1.6264e-04  eta: 10:57:55  time: 7.0186  data_time: 0.0037  memory: 13286  loss: 0.0092  grad_norm: 0.0253
11/18 16:25:51 - mmengine - INFO - Epoch(train)  [31][20/80]  lr: 1.6236e-04  eta: 10:56:46  time: 7.1494  data_time: 0.0032  memory: 13286  loss: 0.0084  grad_norm: 0.0252
11/18 16:27:02 - mmengine - INFO - Epoch(train)  [31][30/80]  lr: 1.6207e-04  eta: 10:55:34  time: 7.0150  data_time: 0.0032  memory: 13286  loss: 0.0105  grad_norm: 0.0252
11/18 16:28:12 - mmengine - INFO - Epoch(train)  [31][40/80]  lr: 1.6178e-04  eta: 10:54:24  time: 7.0770  data_time: 0.0033  memory: 13286  loss: 0.0128  grad_norm: 0.0250
11/18 16:29:23 - mmengine - INFO - Epoch(train)  [31][50/80]  lr: 1.6149e-04  eta: 10:53:14  time: 7.0746  data_time: 0.0031  memory: 13286  loss: 0.0102  grad_norm: 0.0334
11/18 16:30:33 - mmengine - INFO - Epoch(train)  [31][60/80]  lr: 1.6120e-04  eta: 10:52:02  time: 7.0173  data_time: 0.0031  memory: 13286  loss: 0.0104  grad_norm: 0.0334
11/18 16:31:44 - mmengine - INFO - Epoch(train)  [31][70/80]  lr: 1.6091e-04  eta: 10:50:52  time: 7.0800  data_time: 0.0031  memory: 13286  loss: 0.0129  grad_norm: 0.0423
11/18 16:32:55 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 16:32:55 - mmengine - INFO - Epoch(train)  [31][80/80]  lr: 1.6062e-04  eta: 10:49:41  time: 7.0711  data_time: 0.0032  memory: 13286  loss: 0.0100  grad_norm: 0.0424
11/18 16:32:55 - mmengine - INFO - Saving checkpoint at 31 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 16:34:08 - mmengine - INFO - Epoch(train)  [32][10/80]  lr: 1.6032e-04  eta: 10:48:30  time: 7.0149  data_time: 0.0037  memory: 13286  loss: 0.0062  grad_norm: 0.0424
11/18 16:35:19 - mmengine - INFO - Epoch(train)  [32][20/80]  lr: 1.6003e-04  eta: 10:47:21  time: 7.1407  data_time: 0.0033  memory: 13286  loss: 0.0080  grad_norm: 0.0420
11/18 16:36:29 - mmengine - INFO - Epoch(train)  [32][30/80]  lr: 1.5974e-04  eta: 10:46:09  time: 7.0118  data_time: 0.0032  memory: 13286  loss: 0.0106  grad_norm: 0.0420
11/18 16:37:40 - mmengine - INFO - Epoch(train)  [32][40/80]  lr: 1.5944e-04  eta: 10:44:59  time: 7.0738  data_time: 0.0032  memory: 13286  loss: 0.0094  grad_norm: 0.0397
11/18 16:38:51 - mmengine - INFO - Epoch(train)  [32][50/80]  lr: 1.5915e-04  eta: 10:43:48  time: 7.0714  data_time: 0.0032  memory: 13286  loss: 0.0139  grad_norm: 0.0394
11/18 16:40:01 - mmengine - INFO - Epoch(train)  [32][60/80]  lr: 1.5885e-04  eta: 10:42:37  time: 7.0144  data_time: 0.0032  memory: 13286  loss: 0.0089  grad_norm: 0.0394
11/18 16:41:12 - mmengine - INFO - Epoch(train)  [32][70/80]  lr: 1.5855e-04  eta: 10:41:26  time: 7.0714  data_time: 0.0032  memory: 13286  loss: 0.0118  grad_norm: 0.0395
11/18 16:42:22 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 16:42:22 - mmengine - INFO - Epoch(train)  [32][80/80]  lr: 1.5825e-04  eta: 10:40:16  time: 7.0709  data_time: 0.0032  memory: 13286  loss: 0.0068  grad_norm: 0.0385
11/18 16:42:22 - mmengine - INFO - Saving checkpoint at 32 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 16:43:36 - mmengine - INFO - Epoch(train)  [33][10/80]  lr: 1.5796e-04  eta: 10:39:04  time: 7.0117  data_time: 0.0039  memory: 13286  loss: 0.0099  grad_norm: 0.0385
11/18 16:44:47 - mmengine - INFO - Epoch(train)  [33][20/80]  lr: 1.5766e-04  eta: 10:37:55  time: 7.1428  data_time: 0.0032  memory: 13286  loss: 0.0095  grad_norm: 0.0387
11/18 16:45:57 - mmengine - INFO - Epoch(train)  [33][30/80]  lr: 1.5736e-04  eta: 10:36:44  time: 7.0089  data_time: 0.0032  memory: 13286  loss: 0.0131  grad_norm: 0.0387
11/18 16:47:08 - mmengine - INFO - Epoch(train)  [33][40/80]  lr: 1.5706e-04  eta: 10:35:33  time: 7.0740  data_time: 0.0032  memory: 13286  loss: 0.0106  grad_norm: 0.0528
11/18 16:48:19 - mmengine - INFO - Epoch(train)  [33][50/80]  lr: 1.5675e-04  eta: 10:34:23  time: 7.0730  data_time: 0.0032  memory: 13286  loss: 0.0067  grad_norm: 0.0417
11/18 16:49:29 - mmengine - INFO - Epoch(train)  [33][60/80]  lr: 1.5645e-04  eta: 10:33:11  time: 7.0123  data_time: 0.0032  memory: 13286  loss: 0.0086  grad_norm: 0.0417
11/18 16:50:40 - mmengine - INFO - Epoch(train)  [33][70/80]  lr: 1.5615e-04  eta: 10:32:01  time: 7.0713  data_time: 0.0032  memory: 13286  loss: 0.0096  grad_norm: 0.0289
11/18 16:51:50 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 16:51:50 - mmengine - INFO - Epoch(train)  [33][80/80]  lr: 1.5584e-04  eta: 10:30:50  time: 7.0727  data_time: 0.0032  memory: 13286  loss: 0.0067  grad_norm: 0.0270
11/18 16:51:50 - mmengine - INFO - Saving checkpoint at 33 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 16:53:04 - mmengine - INFO - Epoch(train)  [34][10/80]  lr: 1.5554e-04  eta: 10:29:39  time: 7.0090  data_time: 0.0035  memory: 13286  loss: 0.0152  grad_norm: 0.0270
11/18 16:54:15 - mmengine - INFO - Epoch(train)  [34][20/80]  lr: 1.5523e-04  eta: 10:28:30  time: 7.1396  data_time: 0.0032  memory: 13286  loss: 0.0076  grad_norm: 0.0319
11/18 16:55:25 - mmengine - INFO - Epoch(train)  [34][30/80]  lr: 1.5493e-04  eta: 10:27:18  time: 7.0111  data_time: 0.0032  memory: 13286  loss: 0.0161  grad_norm: 0.0319
11/18 16:56:36 - mmengine - INFO - Epoch(train)  [34][40/80]  lr: 1.5462e-04  eta: 10:26:08  time: 7.0696  data_time: 0.0031  memory: 13286  loss: 0.0109  grad_norm: 0.0338
11/18 16:57:46 - mmengine - INFO - Epoch(train)  [34][50/80]  lr: 1.5431e-04  eta: 10:24:57  time: 7.0710  data_time: 0.0033  memory: 13286  loss: 0.0112  grad_norm: 0.0431
11/18 16:58:57 - mmengine - INFO - Epoch(train)  [34][60/80]  lr: 1.5401e-04  eta: 10:23:45  time: 7.0113  data_time: 0.0031  memory: 13286  loss: 0.0099  grad_norm: 0.0431
11/18 17:00:07 - mmengine - INFO - Epoch(train)  [34][70/80]  lr: 1.5370e-04  eta: 10:22:35  time: 7.0695  data_time: 0.0031  memory: 13286  loss: 0.0111  grad_norm: 0.0496
11/18 17:01:18 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:01:18 - mmengine - INFO - Epoch(train)  [34][80/80]  lr: 1.5339e-04  eta: 10:21:25  time: 7.0718  data_time: 0.0031  memory: 13286  loss: 0.0065  grad_norm: 0.0511
11/18 17:01:18 - mmengine - INFO - Saving checkpoint at 34 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:02:31 - mmengine - INFO - Epoch(train)  [35][10/80]  lr: 1.5308e-04  eta: 10:20:13  time: 7.0164  data_time: 0.0037  memory: 13286  loss: 0.0091  grad_norm: 0.0511
11/18 17:03:43 - mmengine - INFO - Epoch(train)  [35][20/80]  lr: 1.5277e-04  eta: 10:19:04  time: 7.1439  data_time: 0.0033  memory: 13286  loss: 0.0111  grad_norm: 0.0530
11/18 17:04:53 - mmengine - INFO - Epoch(train)  [35][30/80]  lr: 1.5246e-04  eta: 10:17:53  time: 7.0125  data_time: 0.0032  memory: 13286  loss: 0.0120  grad_norm: 0.0530
11/18 17:06:04 - mmengine - INFO - Epoch(train)  [35][40/80]  lr: 1.5214e-04  eta: 10:16:42  time: 7.0693  data_time: 0.0032  memory: 13286  loss: 0.0156  grad_norm: 0.0534
11/18 17:07:14 - mmengine - INFO - Epoch(train)  [35][50/80]  lr: 1.5183e-04  eta: 10:15:32  time: 7.0712  data_time: 0.0032  memory: 13286  loss: 0.0085  grad_norm: 0.0551
11/18 17:08:25 - mmengine - INFO - Epoch(train)  [35][60/80]  lr: 1.5152e-04  eta: 10:14:20  time: 7.0117  data_time: 0.0032  memory: 13286  loss: 0.0084  grad_norm: 0.0551
11/18 17:09:35 - mmengine - INFO - Epoch(train)  [35][70/80]  lr: 1.5121e-04  eta: 10:13:10  time: 7.0736  data_time: 0.0033  memory: 13286  loss: 0.0089  grad_norm: 0.0556
11/18 17:10:46 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:10:46 - mmengine - INFO - Epoch(train)  [35][80/80]  lr: 1.5089e-04  eta: 10:11:59  time: 7.0734  data_time: 0.0032  memory: 13286  loss: 0.0085  grad_norm: 0.0560
11/18 17:10:46 - mmengine - INFO - Saving checkpoint at 35 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:11:59 - mmengine - INFO - Epoch(train)  [36][10/80]  lr: 1.5058e-04  eta: 10:10:48  time: 7.0085  data_time: 0.0040  memory: 13286  loss: 0.0087  grad_norm: 0.0560
11/18 17:13:11 - mmengine - INFO - Epoch(train)  [36][20/80]  lr: 1.5026e-04  eta: 10:09:38  time: 7.1362  data_time: 0.0032  memory: 13286  loss: 0.0057  grad_norm: 0.0516
11/18 17:14:21 - mmengine - INFO - Epoch(train)  [36][30/80]  lr: 1.4994e-04  eta: 10:08:27  time: 7.0085  data_time: 0.0032  memory: 13286  loss: 0.0079  grad_norm: 0.0516
11/18 17:15:31 - mmengine - INFO - Epoch(train)  [36][40/80]  lr: 1.4963e-04  eta: 10:07:16  time: 7.0677  data_time: 0.0032  memory: 13286  loss: 0.0067  grad_norm: 0.0503
11/18 17:16:42 - mmengine - INFO - Epoch(train)  [36][50/80]  lr: 1.4931e-04  eta: 10:06:06  time: 7.0741  data_time: 0.0032  memory: 13286  loss: 0.0134  grad_norm: 0.0437
11/18 17:17:52 - mmengine - INFO - Epoch(train)  [36][60/80]  lr: 1.4899e-04  eta: 10:04:54  time: 7.0079  data_time: 0.0032  memory: 13286  loss: 0.0103  grad_norm: 0.0437
11/18 17:19:03 - mmengine - INFO - Epoch(train)  [36][70/80]  lr: 1.4867e-04  eta: 10:03:44  time: 7.0671  data_time: 0.0032  memory: 13286  loss: 0.0154  grad_norm: 0.0399
11/18 17:20:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:20:14 - mmengine - INFO - Epoch(train)  [36][80/80]  lr: 1.4835e-04  eta: 10:02:33  time: 7.0700  data_time: 0.0032  memory: 13286  loss: 0.0105  grad_norm: 0.0402
11/18 17:20:14 - mmengine - INFO - Saving checkpoint at 36 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:21:27 - mmengine - INFO - Epoch(train)  [37][10/80]  lr: 1.4803e-04  eta: 10:01:22  time: 7.0150  data_time: 0.0037  memory: 13286  loss: 0.0081  grad_norm: 0.0402
11/18 17:22:38 - mmengine - INFO - Epoch(train)  [37][20/80]  lr: 1.4771e-04  eta: 10:00:13  time: 7.1430  data_time: 0.0033  memory: 13286  loss: 0.0185  grad_norm: 0.0463
11/18 17:23:48 - mmengine - INFO - Epoch(train)  [37][30/80]  lr: 1.4739e-04  eta: 9:59:01  time: 7.0157  data_time: 0.0032  memory: 13286  loss: 0.0095  grad_norm: 0.0463
11/18 17:24:59 - mmengine - INFO - Epoch(train)  [37][40/80]  lr: 1.4707e-04  eta: 9:57:51  time: 7.0689  data_time: 0.0031  memory: 13286  loss: 0.0051  grad_norm: 0.0338
11/18 17:26:10 - mmengine - INFO - Epoch(train)  [37][50/80]  lr: 1.4675e-04  eta: 9:56:40  time: 7.0698  data_time: 0.0031  memory: 13286  loss: 0.0066  grad_norm: 0.0379
11/18 17:27:20 - mmengine - INFO - Epoch(train)  [37][60/80]  lr: 1.4642e-04  eta: 9:55:29  time: 7.0097  data_time: 0.0032  memory: 13286  loss: 0.0065  grad_norm: 0.0379
11/18 17:28:31 - mmengine - INFO - Epoch(train)  [37][70/80]  lr: 1.4610e-04  eta: 9:54:19  time: 7.0713  data_time: 0.0032  memory: 13286  loss: 0.0112  grad_norm: 0.0389
11/18 17:29:41 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:29:41 - mmengine - INFO - Epoch(train)  [37][80/80]  lr: 1.4578e-04  eta: 9:53:08  time: 7.0675  data_time: 0.0031  memory: 13286  loss: 0.0071  grad_norm: 0.0395
11/18 17:29:41 - mmengine - INFO - Saving checkpoint at 37 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:30:55 - mmengine - INFO - Epoch(train)  [38][10/80]  lr: 1.4545e-04  eta: 9:51:57  time: 7.0125  data_time: 0.0037  memory: 13286  loss: 0.0110  grad_norm: 0.0395
11/18 17:32:06 - mmengine - INFO - Epoch(train)  [38][20/80]  lr: 1.4513e-04  eta: 9:50:47  time: 7.1409  data_time: 0.0032  memory: 13286  loss: 0.0071  grad_norm: 0.0414
11/18 17:33:16 - mmengine - INFO - Epoch(train)  [38][30/80]  lr: 1.4480e-04  eta: 9:49:36  time: 7.0093  data_time: 0.0031  memory: 13286  loss: 0.0060  grad_norm: 0.0414
11/18 17:34:27 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:34:27 - mmengine - INFO - Epoch(train)  [38][40/80]  lr: 1.4447e-04  eta: 9:48:25  time: 7.0656  data_time: 0.0031  memory: 13286  loss: 0.0128  grad_norm: 0.0418
11/18 17:35:37 - mmengine - INFO - Epoch(train)  [38][50/80]  lr: 1.4415e-04  eta: 9:47:15  time: 7.0651  data_time: 0.0031  memory: 13286  loss: 0.0053  grad_norm: 0.0415
11/18 17:36:47 - mmengine - INFO - Epoch(train)  [38][60/80]  lr: 1.4382e-04  eta: 9:46:03  time: 7.0077  data_time: 0.0031  memory: 13286  loss: 0.0100  grad_norm: 0.0415
11/18 17:37:58 - mmengine - INFO - Epoch(train)  [38][70/80]  lr: 1.4349e-04  eta: 9:44:53  time: 7.0674  data_time: 0.0031  memory: 13286  loss: 0.0058  grad_norm: 0.0380
11/18 17:39:09 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:39:09 - mmengine - INFO - Epoch(train)  [38][80/80]  lr: 1.4316e-04  eta: 9:43:42  time: 7.0650  data_time: 0.0031  memory: 13286  loss: 0.0056  grad_norm: 0.0364
11/18 17:39:09 - mmengine - INFO - Saving checkpoint at 38 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:40:22 - mmengine - INFO - Epoch(train)  [39][10/80]  lr: 1.4284e-04  eta: 9:42:31  time: 7.0161  data_time: 0.0037  memory: 13286  loss: 0.0055  grad_norm: 0.0364
11/18 17:41:34 - mmengine - INFO - Epoch(train)  [39][20/80]  lr: 1.4251e-04  eta: 9:41:22  time: 7.1443  data_time: 0.0033  memory: 13286  loss: 0.0096  grad_norm: 0.0280
11/18 17:42:44 - mmengine - INFO - Epoch(train)  [39][30/80]  lr: 1.4218e-04  eta: 9:40:10  time: 7.0144  data_time: 0.0032  memory: 13286  loss: 0.0114  grad_norm: 0.0280
11/18 17:43:55 - mmengine - INFO - Epoch(train)  [39][40/80]  lr: 1.4185e-04  eta: 9:39:00  time: 7.0734  data_time: 0.0032  memory: 13286  loss: 0.0082  grad_norm: 0.0276
11/18 17:45:05 - mmengine - INFO - Epoch(train)  [39][50/80]  lr: 1.4151e-04  eta: 9:37:49  time: 7.0721  data_time: 0.0032  memory: 13286  loss: 0.0072  grad_norm: 0.0218
11/18 17:46:15 - mmengine - INFO - Epoch(train)  [39][60/80]  lr: 1.4118e-04  eta: 9:36:38  time: 7.0159  data_time: 0.0032  memory: 13286  loss: 0.0051  grad_norm: 0.0218
11/18 17:47:26 - mmengine - INFO - Epoch(train)  [39][70/80]  lr: 1.4085e-04  eta: 9:35:28  time: 7.0715  data_time: 0.0032  memory: 13286  loss: 0.0053  grad_norm: 0.0202
11/18 17:48:37 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:48:37 - mmengine - INFO - Epoch(train)  [39][80/80]  lr: 1.4052e-04  eta: 9:34:17  time: 7.0697  data_time: 0.0032  memory: 13286  loss: 0.0064  grad_norm: 0.0190
11/18 17:48:37 - mmengine - INFO - Saving checkpoint at 39 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:49:50 - mmengine - INFO - Epoch(train)  [40][10/80]  lr: 1.4019e-04  eta: 9:33:06  time: 7.0114  data_time: 0.0036  memory: 13286  loss: 0.0095  grad_norm: 0.0190
11/18 17:51:02 - mmengine - INFO - Epoch(train)  [40][20/80]  lr: 1.3985e-04  eta: 9:31:56  time: 7.1439  data_time: 0.0032  memory: 13286  loss: 0.0059  grad_norm: 0.0176
11/18 17:52:12 - mmengine - INFO - Epoch(train)  [40][30/80]  lr: 1.3952e-04  eta: 9:30:45  time: 7.0126  data_time: 0.0032  memory: 13286  loss: 0.0051  grad_norm: 0.0176
11/18 17:53:22 - mmengine - INFO - Epoch(train)  [40][40/80]  lr: 1.3919e-04  eta: 9:29:35  time: 7.0651  data_time: 0.0032  memory: 13286  loss: 0.0086  grad_norm: 0.0161
11/18 17:54:33 - mmengine - INFO - Epoch(train)  [40][50/80]  lr: 1.3885e-04  eta: 9:28:24  time: 7.0693  data_time: 0.0033  memory: 13286  loss: 0.0053  grad_norm: 0.0135
11/18 17:55:43 - mmengine - INFO - Epoch(train)  [40][60/80]  lr: 1.3852e-04  eta: 9:27:13  time: 7.0100  data_time: 0.0032  memory: 13286  loss: 0.0066  grad_norm: 0.0135
11/18 17:56:54 - mmengine - INFO - Epoch(train)  [40][70/80]  lr: 1.3818e-04  eta: 9:26:02  time: 7.0685  data_time: 0.0032  memory: 13286  loss: 0.0065  grad_norm: 0.0140
11/18 17:58:04 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 17:58:04 - mmengine - INFO - Epoch(train)  [40][80/80]  lr: 1.3785e-04  eta: 9:24:52  time: 7.0682  data_time: 0.0032  memory: 13286  loss: 0.0079  grad_norm: 0.0137
11/18 17:58:04 - mmengine - INFO - Saving checkpoint at 40 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 17:59:18 - mmengine - INFO - Epoch(train)  [41][10/80]  lr: 1.3751e-04  eta: 9:23:40  time: 7.0100  data_time: 0.0038  memory: 13286  loss: 0.0049  grad_norm: 0.0137
11/18 18:00:29 - mmengine - INFO - Epoch(train)  [41][20/80]  lr: 1.3717e-04  eta: 9:22:31  time: 7.1398  data_time: 0.0032  memory: 13286  loss: 0.0053  grad_norm: 0.0135
11/18 18:01:39 - mmengine - INFO - Epoch(train)  [41][30/80]  lr: 1.3683e-04  eta: 9:21:20  time: 7.0093  data_time: 0.0032  memory: 13286  loss: 0.0061  grad_norm: 0.0135
11/18 18:02:50 - mmengine - INFO - Epoch(train)  [41][40/80]  lr: 1.3650e-04  eta: 9:20:09  time: 7.0682  data_time: 0.0031  memory: 13286  loss: 0.0085  grad_norm: 0.0117
11/18 18:04:01 - mmengine - INFO - Epoch(train)  [41][50/80]  lr: 1.3616e-04  eta: 9:18:59  time: 7.0653  data_time: 0.0031  memory: 13286  loss: 0.0083  grad_norm: 0.0119
11/18 18:05:11 - mmengine - INFO - Epoch(train)  [41][60/80]  lr: 1.3582e-04  eta: 9:17:47  time: 7.0095  data_time: 0.0031  memory: 13286  loss: 0.0066  grad_norm: 0.0119
11/18 18:06:21 - mmengine - INFO - Epoch(train)  [41][70/80]  lr: 1.3548e-04  eta: 9:16:37  time: 7.0694  data_time: 0.0033  memory: 13286  loss: 0.0055  grad_norm: 0.0146
11/18 18:07:32 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 18:07:32 - mmengine - INFO - Epoch(train)  [41][80/80]  lr: 1.3514e-04  eta: 9:15:26  time: 7.0707  data_time: 0.0031  memory: 13286  loss: 0.0067  grad_norm: 0.0145
11/18 18:07:32 - mmengine - INFO - Saving checkpoint at 41 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 18:08:45 - mmengine - INFO - Epoch(train)  [42][10/80]  lr: 1.3480e-04  eta: 9:14:15  time: 7.0117  data_time: 0.0036  memory: 13286  loss: 0.0094  grad_norm: 0.0145
11/18 18:09:57 - mmengine - INFO - Epoch(train)  [42][20/80]  lr: 1.3446e-04  eta: 9:13:05  time: 7.1437  data_time: 0.0033  memory: 13286  loss: 0.0047  grad_norm: 0.0134
11/18 18:11:07 - mmengine - INFO - Epoch(train)  [42][30/80]  lr: 1.3412e-04  eta: 9:11:54  time: 7.0077  data_time: 0.0032  memory: 13286  loss: 0.0059  grad_norm: 0.0134
11/18 18:12:18 - mmengine - INFO - Epoch(train)  [42][40/80]  lr: 1.3378e-04  eta: 9:10:44  time: 7.0673  data_time: 0.0032  memory: 13286  loss: 0.0054  grad_norm: 0.0134
11/18 18:13:28 - mmengine - INFO - Epoch(train)  [42][50/80]  lr: 1.3344e-04  eta: 9:09:33  time: 7.0646  data_time: 0.0032  memory: 13286  loss: 0.0043  grad_norm: 0.0134
11/18 18:14:38 - mmengine - INFO - Epoch(train)  [42][60/80]  lr: 1.3310e-04  eta: 9:08:22  time: 7.0095  data_time: 0.0032  memory: 13286  loss: 0.0048  grad_norm: 0.0134
11/18 18:15:49 - mmengine - INFO - Epoch(train)  [42][70/80]  lr: 1.3276e-04  eta: 9:07:11  time: 7.0747  data_time: 0.0031  memory: 13286  loss: 0.0058  grad_norm: 0.0130
11/18 18:17:00 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 18:17:00 - mmengine - INFO - Epoch(train)  [42][80/80]  lr: 1.3242e-04  eta: 9:06:01  time: 7.0675  data_time: 0.0033  memory: 13286  loss: 0.0088  grad_norm: 0.0129
11/18 18:17:00 - mmengine - INFO - Saving checkpoint at 42 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 18:18:13 - mmengine - INFO - Epoch(train)  [43][10/80]  lr: 1.3207e-04  eta: 9:04:50  time: 7.0160  data_time: 0.0037  memory: 13286  loss: 0.0068  grad_norm: 0.0129
11/18 18:19:24 - mmengine - INFO - Epoch(train)  [43][20/80]  lr: 1.3173e-04  eta: 9:03:40  time: 7.1465  data_time: 0.0034  memory: 13286  loss: 0.0067  grad_norm: 0.0130
11/18 18:20:35 - mmengine - INFO - Epoch(train)  [43][30/80]  lr: 1.3139e-04  eta: 9:02:29  time: 7.0148  data_time: 0.0032  memory: 13286  loss: 0.0078  grad_norm: 0.0130
11/18 18:21:45 - mmengine - INFO - Epoch(train)  [43][40/80]  lr: 1.3104e-04  eta: 9:01:18  time: 7.0674  data_time: 0.0032  memory: 13286  loss: 0.0042  grad_norm: 0.0127
11/18 18:22:56 - mmengine - INFO - Epoch(train)  [43][50/80]  lr: 1.3070e-04  eta: 9:00:08  time: 7.0766  data_time: 0.0032  memory: 13286  loss: 0.0066  grad_norm: 0.0120
11/18 18:24:06 - mmengine - INFO - Epoch(train)  [43][60/80]  lr: 1.3036e-04  eta: 8:58:57  time: 7.0142  data_time: 0.0031  memory: 13286  loss: 0.0054  grad_norm: 0.0120
11/18 18:25:17 - mmengine - INFO - Epoch(train)  [43][70/80]  lr: 1.3001e-04  eta: 8:57:46  time: 7.0674  data_time: 0.0031  memory: 13286  loss: 0.0044  grad_norm: 0.0094
11/18 18:26:28 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 18:26:28 - mmengine - INFO - Epoch(train)  [43][80/80]  lr: 1.2967e-04  eta: 8:56:36  time: 7.0753  data_time: 0.0032  memory: 13286  loss: 0.0049  grad_norm: 0.0093
11/18 18:26:28 - mmengine - INFO - Saving checkpoint at 43 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 18:27:41 - mmengine - INFO - Epoch(train)  [44][10/80]  lr: 1.2932e-04  eta: 8:55:25  time: 7.0084  data_time: 0.0037  memory: 13286  loss: 0.0047  grad_norm: 0.0093
11/18 18:28:52 - mmengine - INFO - Epoch(train)  [44][20/80]  lr: 1.2898e-04  eta: 8:54:15  time: 7.1407  data_time: 0.0033  memory: 13286  loss: 0.0054  grad_norm: 0.0091
11/18 18:30:02 - mmengine - INFO - Epoch(train)  [44][30/80]  lr: 1.2863e-04  eta: 8:53:04  time: 7.0065  data_time: 0.0032  memory: 13286  loss: 0.0051  grad_norm: 0.0091
11/18 18:31:13 - mmengine - INFO - Epoch(train)  [44][40/80]  lr: 1.2829e-04  eta: 8:51:53  time: 7.0699  data_time: 0.0032  memory: 13286  loss: 0.0051  grad_norm: 0.0089
11/18 18:32:24 - mmengine - INFO - Epoch(train)  [44][50/80]  lr: 1.2794e-04  eta: 8:50:43  time: 7.0738  data_time: 0.0032  memory: 13286  loss: 0.0074  grad_norm: 0.0090
11/18 18:33:34 - mmengine - INFO - Epoch(train)  [44][60/80]  lr: 1.2759e-04  eta: 8:49:32  time: 7.0082  data_time: 0.0032  memory: 13286  loss: 0.0046  grad_norm: 0.0090
11/18 18:34:45 - mmengine - INFO - Epoch(train)  [44][70/80]  lr: 1.2725e-04  eta: 8:48:21  time: 7.0672  data_time: 0.0032  memory: 13286  loss: 0.0058  grad_norm: 0.0085
11/18 18:35:55 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 18:35:55 - mmengine - INFO - Epoch(train)  [44][80/80]  lr: 1.2690e-04  eta: 8:47:11  time: 7.0770  data_time: 0.0031  memory: 13286  loss: 0.0066  grad_norm: 0.0085
11/18 18:35:55 - mmengine - INFO - Saving checkpoint at 44 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 18:37:09 - mmengine - INFO - Epoch(train)  [45][10/80]  lr: 1.2655e-04  eta: 8:45:59  time: 7.0130  data_time: 0.0038  memory: 13286  loss: 0.0056  grad_norm: 0.0085
11/18 18:38:20 - mmengine - INFO - Epoch(train)  [45][20/80]  lr: 1.2620e-04  eta: 8:44:50  time: 7.1447  data_time: 0.0033  memory: 13286  loss: 0.0057  grad_norm: 0.0083
11/18 18:39:30 - mmengine - INFO - Epoch(train)  [45][30/80]  lr: 1.2586e-04  eta: 8:43:39  time: 7.0092  data_time: 0.0032  memory: 13286  loss: 0.0053  grad_norm: 0.0083
11/18 18:40:41 - mmengine - INFO - Epoch(train)  [45][40/80]  lr: 1.2551e-04  eta: 8:42:28  time: 7.0711  data_time: 0.0032  memory: 13286  loss: 0.0048  grad_norm: 0.0080
11/18 18:41:52 - mmengine - INFO - Epoch(train)  [45][50/80]  lr: 1.2516e-04  eta: 8:41:18  time: 7.0751  data_time: 0.0031  memory: 13286  loss: 0.0058  grad_norm: 0.0082
11/18 18:43:02 - mmengine - INFO - Epoch(train)  [45][60/80]  lr: 1.2481e-04  eta: 8:40:06  time: 7.0113  data_time: 0.0032  memory: 13286  loss: 0.0061  grad_norm: 0.0082
11/18 18:44:13 - mmengine - INFO - Epoch(train)  [45][70/80]  lr: 1.2446e-04  eta: 8:38:56  time: 7.0657  data_time: 0.0032  memory: 13286  loss: 0.0057  grad_norm: 0.0078
11/18 18:45:23 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 18:45:23 - mmengine - INFO - Epoch(train)  [45][80/80]  lr: 1.2411e-04  eta: 8:37:45  time: 7.0702  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0078
11/18 18:45:23 - mmengine - INFO - Saving checkpoint at 45 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 18:46:36 - mmengine - INFO - Epoch(train)  [46][10/80]  lr: 1.2376e-04  eta: 8:36:34  time: 7.0072  data_time: 0.0037  memory: 13286  loss: 0.0039  grad_norm: 0.0078
11/18 18:47:48 - mmengine - INFO - Epoch(train)  [46][20/80]  lr: 1.2342e-04  eta: 8:35:25  time: 7.1349  data_time: 0.0032  memory: 13286  loss: 0.0059  grad_norm: 0.0078
11/18 18:48:58 - mmengine - INFO - Epoch(train)  [46][30/80]  lr: 1.2307e-04  eta: 8:34:13  time: 7.0056  data_time: 0.0031  memory: 13286  loss: 0.0043  grad_norm: 0.0078
11/18 18:50:08 - mmengine - INFO - Epoch(train)  [46][40/80]  lr: 1.2272e-04  eta: 8:33:03  time: 7.0675  data_time: 0.0031  memory: 13286  loss: 0.0057  grad_norm: 0.0077
11/18 18:51:19 - mmengine - INFO - Epoch(train)  [46][50/80]  lr: 1.2237e-04  eta: 8:31:52  time: 7.0640  data_time: 0.0032  memory: 13286  loss: 0.0055  grad_norm: 0.0075
11/18 18:52:29 - mmengine - INFO - Epoch(train)  [46][60/80]  lr: 1.2202e-04  eta: 8:30:41  time: 7.0082  data_time: 0.0032  memory: 13286  loss: 0.0073  grad_norm: 0.0075
11/18 18:53:40 - mmengine - INFO - Epoch(train)  [46][70/80]  lr: 1.2167e-04  eta: 8:29:30  time: 7.0681  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0076
11/18 18:54:51 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 18:54:51 - mmengine - INFO - Epoch(train)  [46][80/80]  lr: 1.2132e-04  eta: 8:28:20  time: 7.0612  data_time: 0.0031  memory: 13286  loss: 0.0046  grad_norm: 0.0073
11/18 18:54:51 - mmengine - INFO - Saving checkpoint at 46 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 18:56:04 - mmengine - INFO - Epoch(train)  [47][10/80]  lr: 1.2096e-04  eta: 8:27:09  time: 7.0080  data_time: 0.0037  memory: 13286  loss: 0.0065  grad_norm: 0.0073
11/18 18:57:15 - mmengine - INFO - Epoch(train)  [47][20/80]  lr: 1.2061e-04  eta: 8:25:59  time: 7.1415  data_time: 0.0033  memory: 13286  loss: 0.0039  grad_norm: 0.0072
11/18 18:58:25 - mmengine - INFO - Epoch(train)  [47][30/80]  lr: 1.2026e-04  eta: 8:24:48  time: 7.0098  data_time: 0.0032  memory: 13286  loss: 0.0051  grad_norm: 0.0072
11/18 18:59:36 - mmengine - INFO - Epoch(train)  [47][40/80]  lr: 1.1991e-04  eta: 8:23:37  time: 7.0682  data_time: 0.0031  memory: 13286  loss: 0.0046  grad_norm: 0.0071
11/18 19:00:47 - mmengine - INFO - Epoch(train)  [47][50/80]  lr: 1.1956e-04  eta: 8:22:27  time: 7.0669  data_time: 0.0032  memory: 13286  loss: 0.0043  grad_norm: 0.0069
11/18 19:01:57 - mmengine - INFO - Epoch(train)  [47][60/80]  lr: 1.1921e-04  eta: 8:21:16  time: 7.0112  data_time: 0.0031  memory: 13286  loss: 0.0054  grad_norm: 0.0069
11/18 19:03:07 - mmengine - INFO - Epoch(train)  [47][70/80]  lr: 1.1886e-04  eta: 8:20:05  time: 7.0660  data_time: 0.0031  memory: 13286  loss: 0.0061  grad_norm: 0.0070
11/18 19:04:18 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 19:04:18 - mmengine - INFO - Epoch(train)  [47][80/80]  lr: 1.1850e-04  eta: 8:18:55  time: 7.0673  data_time: 0.0031  memory: 13286  loss: 0.0038  grad_norm: 0.0073
11/18 19:04:18 - mmengine - INFO - Saving checkpoint at 47 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 19:05:31 - mmengine - INFO - Epoch(train)  [48][10/80]  lr: 1.1815e-04  eta: 8:17:43  time: 7.0141  data_time: 0.0037  memory: 13286  loss: 0.0043  grad_norm: 0.0073
11/18 19:06:43 - mmengine - INFO - Epoch(train)  [48][20/80]  lr: 1.1780e-04  eta: 8:16:34  time: 7.1451  data_time: 0.0032  memory: 13286  loss: 0.0048  grad_norm: 0.0073
11/18 19:07:53 - mmengine - INFO - Epoch(train)  [48][30/80]  lr: 1.1745e-04  eta: 8:15:23  time: 7.0140  data_time: 0.0032  memory: 13286  loss: 0.0048  grad_norm: 0.0073
11/18 19:09:04 - mmengine - INFO - Epoch(train)  [48][40/80]  lr: 1.1710e-04  eta: 8:14:12  time: 7.0712  data_time: 0.0032  memory: 13286  loss: 0.0040  grad_norm: 0.0073
11/18 19:10:14 - mmengine - INFO - Epoch(train)  [48][50/80]  lr: 1.1674e-04  eta: 8:13:02  time: 7.0699  data_time: 0.0032  memory: 13286  loss: 0.0052  grad_norm: 0.0073
11/18 19:11:24 - mmengine - INFO - Epoch(train)  [48][60/80]  lr: 1.1639e-04  eta: 8:11:51  time: 7.0112  data_time: 0.0032  memory: 13286  loss: 0.0046  grad_norm: 0.0073
11/18 19:12:35 - mmengine - INFO - Epoch(train)  [48][70/80]  lr: 1.1604e-04  eta: 8:10:40  time: 7.0706  data_time: 0.0032  memory: 13286  loss: 0.0071  grad_norm: 0.0072
11/18 19:13:46 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 19:13:46 - mmengine - INFO - Epoch(train)  [48][80/80]  lr: 1.1569e-04  eta: 8:09:30  time: 7.0716  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0073
11/18 19:13:46 - mmengine - INFO - Saving checkpoint at 48 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 19:14:59 - mmengine - INFO - Epoch(train)  [49][10/80]  lr: 1.1533e-04  eta: 8:08:18  time: 7.0070  data_time: 0.0036  memory: 13286  loss: 0.0058  grad_norm: 0.0073
11/18 19:16:11 - mmengine - INFO - Epoch(train)  [49][20/80]  lr: 1.1498e-04  eta: 8:07:09  time: 7.1394  data_time: 0.0034  memory: 13286  loss: 0.0042  grad_norm: 0.0071
11/18 19:17:21 - mmengine - INFO - Epoch(train)  [49][30/80]  lr: 1.1463e-04  eta: 8:05:57  time: 7.0087  data_time: 0.0032  memory: 13286  loss: 0.0038  grad_norm: 0.0071
11/18 19:18:31 - mmengine - INFO - Epoch(train)  [49][40/80]  lr: 1.1427e-04  eta: 8:04:47  time: 7.0723  data_time: 0.0032  memory: 13286  loss: 0.0042  grad_norm: 0.0071
11/18 19:19:42 - mmengine - INFO - Epoch(train)  [49][50/80]  lr: 1.1392e-04  eta: 8:03:36  time: 7.0643  data_time: 0.0032  memory: 13286  loss: 0.0050  grad_norm: 0.0075
11/18 19:20:52 - mmengine - INFO - Epoch(train)  [49][60/80]  lr: 1.1357e-04  eta: 8:02:25  time: 7.0091  data_time: 0.0033  memory: 13286  loss: 0.0048  grad_norm: 0.0075
11/18 19:22:03 - mmengine - INFO - Epoch(train)  [49][70/80]  lr: 1.1322e-04  eta: 8:01:15  time: 7.0645  data_time: 0.0032  memory: 13286  loss: 0.0046  grad_norm: 0.0075
11/18 19:23:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 19:23:14 - mmengine - INFO - Epoch(train)  [49][80/80]  lr: 1.1286e-04  eta: 8:00:04  time: 7.0672  data_time: 0.0032  memory: 13286  loss: 0.0049  grad_norm: 0.0071
11/18 19:23:14 - mmengine - INFO - Saving checkpoint at 49 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 19:24:27 - mmengine - INFO - Epoch(train)  [50][10/80]  lr: 1.1251e-04  eta: 7:58:53  time: 7.0117  data_time: 0.0038  memory: 13286  loss: 0.0056  grad_norm: 0.0071
11/18 19:25:38 - mmengine - INFO - Epoch(train)  [50][20/80]  lr: 1.1216e-04  eta: 7:57:43  time: 7.1410  data_time: 0.0033  memory: 13286  loss: 0.0043  grad_norm: 0.0071
11/18 19:26:48 - mmengine - INFO - Epoch(train)  [50][30/80]  lr: 1.1180e-04  eta: 7:56:32  time: 7.0116  data_time: 0.0032  memory: 13286  loss: 0.0034  grad_norm: 0.0071
11/18 19:27:59 - mmengine - INFO - Epoch(train)  [50][40/80]  lr: 1.1145e-04  eta: 7:55:22  time: 7.0712  data_time: 0.0032  memory: 13286  loss: 0.0041  grad_norm: 0.0071
11/18 19:29:10 - mmengine - INFO - Epoch(train)  [50][50/80]  lr: 1.1110e-04  eta: 7:54:11  time: 7.0716  data_time: 0.0032  memory: 13286  loss: 0.0055  grad_norm: 0.0070
11/18 19:30:20 - mmengine - INFO - Epoch(train)  [50][60/80]  lr: 1.1074e-04  eta: 7:53:00  time: 7.0101  data_time: 0.0032  memory: 13286  loss: 0.0048  grad_norm: 0.0070
11/18 19:31:31 - mmengine - INFO - Epoch(train)  [50][70/80]  lr: 1.1039e-04  eta: 7:51:50  time: 7.0693  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0069
11/18 19:32:41 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 19:32:41 - mmengine - INFO - Epoch(train)  [50][80/80]  lr: 1.1004e-04  eta: 7:50:39  time: 7.0679  data_time: 0.0032  memory: 13286  loss: 0.0044  grad_norm: 0.0069
11/18 19:32:41 - mmengine - INFO - Saving checkpoint at 50 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 19:33:54 - mmengine - INFO - Epoch(train)  [51][10/80]  lr: 1.0968e-04  eta: 7:49:28  time: 7.0064  data_time: 0.0039  memory: 13286  loss: 0.0036  grad_norm: 0.0069
11/18 19:35:06 - mmengine - INFO - Epoch(train)  [51][20/80]  lr: 1.0933e-04  eta: 7:48:18  time: 7.1402  data_time: 0.0032  memory: 13286  loss: 0.0056  grad_norm: 0.0070
11/18 19:36:16 - mmengine - INFO - Epoch(train)  [51][30/80]  lr: 1.0898e-04  eta: 7:47:07  time: 7.0094  data_time: 0.0032  memory: 13286  loss: 0.0037  grad_norm: 0.0070
11/18 19:37:27 - mmengine - INFO - Epoch(train)  [51][40/80]  lr: 1.0862e-04  eta: 7:45:57  time: 7.0647  data_time: 0.0031  memory: 13286  loss: 0.0044  grad_norm: 0.0071
11/18 19:38:37 - mmengine - INFO - Epoch(train)  [51][50/80]  lr: 1.0827e-04  eta: 7:44:46  time: 7.0651  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0066
11/18 19:39:47 - mmengine - INFO - Epoch(train)  [51][60/80]  lr: 1.0791e-04  eta: 7:43:35  time: 7.0092  data_time: 0.0032  memory: 13286  loss: 0.0064  grad_norm: 0.0066
11/18 19:40:58 - mmengine - INFO - Epoch(train)  [51][70/80]  lr: 1.0756e-04  eta: 7:42:24  time: 7.0671  data_time: 0.0032  memory: 13286  loss: 0.0042  grad_norm: 0.0066
11/18 19:42:09 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 19:42:09 - mmengine - INFO - Epoch(train)  [51][80/80]  lr: 1.0721e-04  eta: 7:41:14  time: 7.0663  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0066
11/18 19:42:09 - mmengine - INFO - Saving checkpoint at 51 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 19:43:22 - mmengine - INFO - Epoch(train)  [52][10/80]  lr: 1.0686e-04  eta: 7:40:03  time: 7.0104  data_time: 0.0068  memory: 13286  loss: 0.0040  grad_norm: 0.0066
11/18 19:44:33 - mmengine - INFO - Epoch(train)  [52][20/80]  lr: 1.0650e-04  eta: 7:38:53  time: 7.1400  data_time: 0.0032  memory: 13286  loss: 0.0039  grad_norm: 0.0066
11/18 19:45:43 - mmengine - INFO - Epoch(train)  [52][30/80]  lr: 1.0615e-04  eta: 7:37:42  time: 7.0073  data_time: 0.0032  memory: 13286  loss: 0.0062  grad_norm: 0.0066
11/18 19:46:54 - mmengine - INFO - Epoch(train)  [52][40/80]  lr: 1.0580e-04  eta: 7:36:31  time: 7.0702  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0067
11/18 19:48:05 - mmengine - INFO - Epoch(train)  [52][50/80]  lr: 1.0544e-04  eta: 7:35:21  time: 7.0691  data_time: 0.0033  memory: 13286  loss: 0.0038  grad_norm: 0.0066
11/18 19:49:15 - mmengine - INFO - Epoch(train)  [52][60/80]  lr: 1.0509e-04  eta: 7:34:10  time: 7.0078  data_time: 0.0032  memory: 13286  loss: 0.0046  grad_norm: 0.0066
11/18 19:50:25 - mmengine - INFO - Epoch(train)  [52][70/80]  lr: 1.0474e-04  eta: 7:32:59  time: 7.0655  data_time: 0.0032  memory: 13286  loss: 0.0037  grad_norm: 0.0066
11/18 19:51:36 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 19:51:36 - mmengine - INFO - Epoch(train)  [52][80/80]  lr: 1.0438e-04  eta: 7:31:49  time: 7.0663  data_time: 0.0032  memory: 13286  loss: 0.0044  grad_norm: 0.0065
11/18 19:51:36 - mmengine - INFO - Saving checkpoint at 52 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 19:52:50 - mmengine - INFO - Epoch(train)  [53][10/80]  lr: 1.0403e-04  eta: 7:30:38  time: 7.0095  data_time: 0.0067  memory: 13286  loss: 0.0032  grad_norm: 0.0065
11/18 19:54:01 - mmengine - INFO - Epoch(train)  [53][20/80]  lr: 1.0368e-04  eta: 7:29:28  time: 7.1391  data_time: 0.0033  memory: 13286  loss: 0.0037  grad_norm: 0.0062
11/18 19:55:11 - mmengine - INFO - Epoch(train)  [53][30/80]  lr: 1.0333e-04  eta: 7:28:17  time: 7.0098  data_time: 0.0033  memory: 13286  loss: 0.0061  grad_norm: 0.0062
11/18 19:56:22 - mmengine - INFO - Epoch(train)  [53][40/80]  lr: 1.0297e-04  eta: 7:27:06  time: 7.0650  data_time: 0.0032  memory: 13286  loss: 0.0040  grad_norm: 0.0062
11/18 19:57:33 - mmengine - INFO - Epoch(train)  [53][50/80]  lr: 1.0262e-04  eta: 7:25:56  time: 7.0688  data_time: 0.0032  memory: 13286  loss: 0.0035  grad_norm: 0.0063
11/18 19:58:43 - mmengine - INFO - Epoch(train)  [53][60/80]  lr: 1.0227e-04  eta: 7:24:45  time: 7.0098  data_time: 0.0032  memory: 13286  loss: 0.0038  grad_norm: 0.0063
11/18 19:59:53 - mmengine - INFO - Epoch(train)  [53][70/80]  lr: 1.0192e-04  eta: 7:23:34  time: 7.0665  data_time: 0.0031  memory: 13286  loss: 0.0045  grad_norm: 0.0064
11/18 20:01:04 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:01:04 - mmengine - INFO - Epoch(train)  [53][80/80]  lr: 1.0157e-04  eta: 7:22:24  time: 7.0661  data_time: 0.0031  memory: 13286  loss: 0.0053  grad_norm: 0.0063
11/18 20:01:04 - mmengine - INFO - Saving checkpoint at 53 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:02:17 - mmengine - INFO - Epoch(train)  [54][10/80]  lr: 1.0121e-04  eta: 7:21:13  time: 7.0089  data_time: 0.0037  memory: 13286  loss: 0.0050  grad_norm: 0.0063
11/18 20:03:29 - mmengine - INFO - Epoch(train)  [54][20/80]  lr: 1.0086e-04  eta: 7:20:03  time: 7.1413  data_time: 0.0032  memory: 13286  loss: 0.0046  grad_norm: 0.0063
11/18 20:04:39 - mmengine - INFO - Epoch(train)  [54][30/80]  lr: 1.0051e-04  eta: 7:18:52  time: 7.0104  data_time: 0.0031  memory: 13286  loss: 0.0052  grad_norm: 0.0063
11/18 20:05:49 - mmengine - INFO - Epoch(train)  [54][40/80]  lr: 1.0016e-04  eta: 7:17:41  time: 7.0700  data_time: 0.0032  memory: 13286  loss: 0.0043  grad_norm: 0.0063
11/18 20:07:00 - mmengine - INFO - Epoch(train)  [54][50/80]  lr: 9.9808e-05  eta: 7:16:31  time: 7.0665  data_time: 0.0031  memory: 13286  loss: 0.0046  grad_norm: 0.0062
11/18 20:08:10 - mmengine - INFO - Epoch(train)  [54][60/80]  lr: 9.9457e-05  eta: 7:15:20  time: 7.0047  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0062
11/18 20:09:21 - mmengine - INFO - Epoch(train)  [54][70/80]  lr: 9.9106e-05  eta: 7:14:09  time: 7.0615  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0062
11/18 20:10:31 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:10:31 - mmengine - INFO - Epoch(train)  [54][80/80]  lr: 9.8755e-05  eta: 7:12:58  time: 7.0633  data_time: 0.0031  memory: 13286  loss: 0.0038  grad_norm: 0.0063
11/18 20:10:31 - mmengine - INFO - Saving checkpoint at 54 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:11:45 - mmengine - INFO - Epoch(train)  [55][10/80]  lr: 9.8405e-05  eta: 7:11:47  time: 7.0086  data_time: 0.0037  memory: 13286  loss: 0.0038  grad_norm: 0.0063
11/18 20:12:56 - mmengine - INFO - Epoch(train)  [55][20/80]  lr: 9.8054e-05  eta: 7:10:37  time: 7.1412  data_time: 0.0034  memory: 13286  loss: 0.0032  grad_norm: 0.0064
11/18 20:14:06 - mmengine - INFO - Epoch(train)  [55][30/80]  lr: 9.7704e-05  eta: 7:09:26  time: 7.0097  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0064
11/18 20:15:17 - mmengine - INFO - Epoch(train)  [55][40/80]  lr: 9.7354e-05  eta: 7:08:16  time: 7.0693  data_time: 0.0031  memory: 13286  loss: 0.0050  grad_norm: 0.0063
11/18 20:16:27 - mmengine - INFO - Epoch(train)  [55][50/80]  lr: 9.7004e-05  eta: 7:07:05  time: 7.0679  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0063
11/18 20:17:38 - mmengine - INFO - Epoch(train)  [55][60/80]  lr: 9.6654e-05  eta: 7:05:54  time: 7.0094  data_time: 0.0031  memory: 13286  loss: 0.0050  grad_norm: 0.0063
11/18 20:18:48 - mmengine - INFO - Epoch(train)  [55][70/80]  lr: 9.6305e-05  eta: 7:04:44  time: 7.0671  data_time: 0.0031  memory: 13286  loss: 0.0047  grad_norm: 0.0061
11/18 20:19:59 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:19:59 - mmengine - INFO - Epoch(train)  [55][80/80]  lr: 9.5956e-05  eta: 7:03:33  time: 7.0686  data_time: 0.0031  memory: 13286  loss: 0.0043  grad_norm: 0.0061
11/18 20:19:59 - mmengine - INFO - Saving checkpoint at 55 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:21:12 - mmengine - INFO - Epoch(train)  [56][10/80]  lr: 9.5607e-05  eta: 7:02:22  time: 7.0098  data_time: 0.0037  memory: 13286  loss: 0.0047  grad_norm: 0.0061
11/18 20:22:24 - mmengine - INFO - Epoch(train)  [56][20/80]  lr: 9.5258e-05  eta: 7:01:12  time: 7.1398  data_time: 0.0032  memory: 13286  loss: 0.0033  grad_norm: 0.0061
11/18 20:23:34 - mmengine - INFO - Epoch(train)  [56][30/80]  lr: 9.4910e-05  eta: 7:00:01  time: 7.0050  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0061
11/18 20:24:44 - mmengine - INFO - Epoch(train)  [56][40/80]  lr: 9.4561e-05  eta: 6:58:51  time: 7.0649  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0061
11/18 20:25:55 - mmengine - INFO - Epoch(train)  [56][50/80]  lr: 9.4213e-05  eta: 6:57:40  time: 7.0669  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0061
11/18 20:27:05 - mmengine - INFO - Epoch(train)  [56][60/80]  lr: 9.3865e-05  eta: 6:56:29  time: 7.0089  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0061
11/18 20:28:16 - mmengine - INFO - Epoch(train)  [56][70/80]  lr: 9.3518e-05  eta: 6:55:19  time: 7.0803  data_time: 0.0031  memory: 13286  loss: 0.0046  grad_norm: 0.0060
11/18 20:29:27 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:29:27 - mmengine - INFO - Epoch(train)  [56][80/80]  lr: 9.3170e-05  eta: 6:54:08  time: 7.0692  data_time: 0.0031  memory: 13286  loss: 0.0066  grad_norm: 0.0060
11/18 20:29:27 - mmengine - INFO - Saving checkpoint at 56 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:30:40 - mmengine - INFO - Epoch(train)  [57][10/80]  lr: 9.2823e-05  eta: 6:52:57  time: 7.0046  data_time: 0.0036  memory: 13286  loss: 0.0040  grad_norm: 0.0060
11/18 20:31:51 - mmengine - INFO - Epoch(train)  [57][20/80]  lr: 9.2477e-05  eta: 6:51:47  time: 7.1369  data_time: 0.0032  memory: 13286  loss: 0.0054  grad_norm: 0.0061
11/18 20:33:01 - mmengine - INFO - Epoch(train)  [57][30/80]  lr: 9.2130e-05  eta: 6:50:36  time: 7.0032  data_time: 0.0032  memory: 13286  loss: 0.0029  grad_norm: 0.0061
11/18 20:34:12 - mmengine - INFO - Epoch(train)  [57][40/80]  lr: 9.1784e-05  eta: 6:49:26  time: 7.0649  data_time: 0.0032  memory: 13286  loss: 0.0035  grad_norm: 0.0061
11/18 20:35:22 - mmengine - INFO - Epoch(train)  [57][50/80]  lr: 9.1438e-05  eta: 6:48:15  time: 7.0621  data_time: 0.0032  memory: 13286  loss: 0.0038  grad_norm: 0.0060
11/18 20:36:32 - mmengine - INFO - Epoch(train)  [57][60/80]  lr: 9.1092e-05  eta: 6:47:04  time: 7.0062  data_time: 0.0032  memory: 13286  loss: 0.0043  grad_norm: 0.0060
11/18 20:37:43 - mmengine - INFO - Epoch(train)  [57][70/80]  lr: 9.0747e-05  eta: 6:45:54  time: 7.0664  data_time: 0.0031  memory: 13286  loss: 0.0048  grad_norm: 0.0061
11/18 20:38:54 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:38:54 - mmengine - INFO - Epoch(train)  [57][80/80]  lr: 9.0402e-05  eta: 6:44:43  time: 7.0722  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0059
11/18 20:38:54 - mmengine - INFO - Saving checkpoint at 57 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:40:07 - mmengine - INFO - Epoch(train)  [58][10/80]  lr: 9.0057e-05  eta: 6:43:32  time: 7.0116  data_time: 0.0035  memory: 13286  loss: 0.0045  grad_norm: 0.0059
11/18 20:41:18 - mmengine - INFO - Epoch(train)  [58][20/80]  lr: 8.9712e-05  eta: 6:42:22  time: 7.1424  data_time: 0.0033  memory: 13286  loss: 0.0044  grad_norm: 0.0059
11/18 20:42:29 - mmengine - INFO - Epoch(train)  [58][30/80]  lr: 8.9368e-05  eta: 6:41:11  time: 7.0117  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0059
11/18 20:43:39 - mmengine - INFO - Epoch(train)  [58][40/80]  lr: 8.9024e-05  eta: 6:40:01  time: 7.0710  data_time: 0.0031  memory: 13286  loss: 0.0038  grad_norm: 0.0057
11/18 20:44:50 - mmengine - INFO - Epoch(train)  [58][50/80]  lr: 8.8681e-05  eta: 6:38:50  time: 7.0688  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0057
11/18 20:46:00 - mmengine - INFO - Epoch(train)  [58][60/80]  lr: 8.8338e-05  eta: 6:37:39  time: 7.0116  data_time: 0.0031  memory: 13286  loss: 0.0049  grad_norm: 0.0057
11/18 20:47:11 - mmengine - INFO - Epoch(train)  [58][70/80]  lr: 8.7995e-05  eta: 6:36:29  time: 7.0683  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0059
11/18 20:48:21 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:48:21 - mmengine - INFO - Epoch(train)  [58][80/80]  lr: 8.7652e-05  eta: 6:35:18  time: 7.0671  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0058
11/18 20:48:21 - mmengine - INFO - Saving checkpoint at 58 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:49:35 - mmengine - INFO - Epoch(train)  [59][10/80]  lr: 8.7310e-05  eta: 6:34:07  time: 7.0074  data_time: 0.0035  memory: 13286  loss: 0.0040  grad_norm: 0.0058
11/18 20:50:46 - mmengine - INFO - Epoch(train)  [59][20/80]  lr: 8.6968e-05  eta: 6:32:57  time: 7.1407  data_time: 0.0031  memory: 13286  loss: 0.0051  grad_norm: 0.0056
11/18 20:51:56 - mmengine - INFO - Epoch(train)  [59][30/80]  lr: 8.6627e-05  eta: 6:31:46  time: 7.0085  data_time: 0.0031  memory: 13286  loss: 0.0037  grad_norm: 0.0056
11/18 20:53:07 - mmengine - INFO - Epoch(train)  [59][40/80]  lr: 8.6286e-05  eta: 6:30:36  time: 7.0644  data_time: 0.0030  memory: 13286  loss: 0.0033  grad_norm: 0.0058
11/18 20:54:17 - mmengine - INFO - Epoch(train)  [59][50/80]  lr: 8.5945e-05  eta: 6:29:25  time: 7.0646  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0058
11/18 20:55:27 - mmengine - INFO - Epoch(train)  [59][60/80]  lr: 8.5604e-05  eta: 6:28:14  time: 7.0063  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0058
11/18 20:56:38 - mmengine - INFO - Epoch(train)  [59][70/80]  lr: 8.5264e-05  eta: 6:27:04  time: 7.0656  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0057
11/18 20:57:49 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 20:57:49 - mmengine - INFO - Epoch(train)  [59][80/80]  lr: 8.4925e-05  eta: 6:25:53  time: 7.0653  data_time: 0.0032  memory: 13286  loss: 0.0032  grad_norm: 0.0058
11/18 20:57:49 - mmengine - INFO - Saving checkpoint at 59 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 20:59:02 - mmengine - INFO - Epoch(train)  [60][10/80]  lr: 8.4586e-05  eta: 6:24:42  time: 7.0070  data_time: 0.0035  memory: 13286  loss: 0.0035  grad_norm: 0.0058
11/18 21:00:14 - mmengine - INFO - Epoch(train)  [60][20/80]  lr: 8.4247e-05  eta: 6:23:32  time: 7.1412  data_time: 0.0032  memory: 13286  loss: 0.0031  grad_norm: 0.0058
11/18 21:01:24 - mmengine - INFO - Epoch(train)  [60][30/80]  lr: 8.3908e-05  eta: 6:22:21  time: 7.0086  data_time: 0.0031  memory: 13286  loss: 0.0049  grad_norm: 0.0058
11/18 21:02:34 - mmengine - INFO - Epoch(train)  [60][40/80]  lr: 8.3570e-05  eta: 6:21:11  time: 7.0685  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0058
11/18 21:03:45 - mmengine - INFO - Epoch(train)  [60][50/80]  lr: 8.3233e-05  eta: 6:20:00  time: 7.0656  data_time: 0.0032  memory: 13286  loss: 0.0054  grad_norm: 0.0058
11/18 21:04:55 - mmengine - INFO - Epoch(train)  [60][60/80]  lr: 8.2895e-05  eta: 6:18:49  time: 7.0075  data_time: 0.0031  memory: 13286  loss: 0.0048  grad_norm: 0.0058
11/18 21:06:06 - mmengine - INFO - Epoch(train)  [60][70/80]  lr: 8.2558e-05  eta: 6:17:39  time: 7.0675  data_time: 0.0030  memory: 13286  loss: 0.0030  grad_norm: 0.0057
11/18 21:07:16 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:07:16 - mmengine - INFO - Epoch(train)  [60][80/80]  lr: 8.2222e-05  eta: 6:16:28  time: 7.0665  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0056
11/18 21:07:16 - mmengine - INFO - Saving checkpoint at 60 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 21:08:30 - mmengine - INFO - Epoch(train)  [61][10/80]  lr: 8.1886e-05  eta: 6:15:17  time: 7.0068  data_time: 0.0035  memory: 13286  loss: 0.0044  grad_norm: 0.0056
11/18 21:09:41 - mmengine - INFO - Epoch(train)  [61][20/80]  lr: 8.1551e-05  eta: 6:14:07  time: 7.1397  data_time: 0.0032  memory: 13286  loss: 0.0035  grad_norm: 0.0056
11/18 21:10:51 - mmengine - INFO - Epoch(train)  [61][30/80]  lr: 8.1216e-05  eta: 6:12:56  time: 7.0085  data_time: 0.0031  memory: 13286  loss: 0.0033  grad_norm: 0.0056
11/18 21:12:02 - mmengine - INFO - Epoch(train)  [61][40/80]  lr: 8.0881e-05  eta: 6:11:46  time: 7.0724  data_time: 0.0030  memory: 13286  loss: 0.0040  grad_norm: 0.0054
11/18 21:13:13 - mmengine - INFO - Epoch(train)  [61][50/80]  lr: 8.0547e-05  eta: 6:10:35  time: 7.0687  data_time: 0.0031  memory: 13286  loss: 0.0043  grad_norm: 0.0055
11/18 21:14:23 - mmengine - INFO - Epoch(train)  [61][60/80]  lr: 8.0213e-05  eta: 6:09:24  time: 7.0072  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0055
11/18 21:15:33 - mmengine - INFO - Epoch(train)  [61][70/80]  lr: 7.9880e-05  eta: 6:08:14  time: 7.0684  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0054
11/18 21:16:44 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:16:44 - mmengine - INFO - Epoch(train)  [61][80/80]  lr: 7.9547e-05  eta: 6:07:03  time: 7.0675  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0054
11/18 21:16:44 - mmengine - INFO - Saving checkpoint at 61 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 21:17:57 - mmengine - INFO - Epoch(train)  [62][10/80]  lr: 7.9214e-05  eta: 6:05:52  time: 7.0085  data_time: 0.0036  memory: 13286  loss: 0.0028  grad_norm: 0.0054
11/18 21:19:09 - mmengine - INFO - Epoch(train)  [62][20/80]  lr: 7.8883e-05  eta: 6:04:42  time: 7.1371  data_time: 0.0032  memory: 13286  loss: 0.0052  grad_norm: 0.0054
11/18 21:20:19 - mmengine - INFO - Epoch(train)  [62][30/80]  lr: 7.8551e-05  eta: 6:03:31  time: 7.0065  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0054
11/18 21:21:29 - mmengine - INFO - Epoch(train)  [62][40/80]  lr: 7.8220e-05  eta: 6:02:21  time: 7.0661  data_time: 0.0031  memory: 13286  loss: 0.0043  grad_norm: 0.0054
11/18 21:22:40 - mmengine - INFO - Epoch(train)  [62][50/80]  lr: 7.7890e-05  eta: 6:01:10  time: 7.0678  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0053
11/18 21:23:50 - mmengine - INFO - Epoch(train)  [62][60/80]  lr: 7.7560e-05  eta: 5:59:59  time: 7.0077  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0053
11/18 21:25:01 - mmengine - INFO - Epoch(train)  [62][70/80]  lr: 7.7231e-05  eta: 5:58:49  time: 7.0661  data_time: 0.0031  memory: 13286  loss: 0.0044  grad_norm: 0.0053
11/18 21:26:12 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:26:12 - mmengine - INFO - Epoch(train)  [62][80/80]  lr: 7.6902e-05  eta: 5:57:38  time: 7.0714  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0055
11/18 21:26:12 - mmengine - INFO - Saving checkpoint at 62 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 21:27:25 - mmengine - INFO - Epoch(train)  [63][10/80]  lr: 7.6573e-05  eta: 5:56:27  time: 7.0102  data_time: 0.0035  memory: 13286  loss: 0.0041  grad_norm: 0.0055
11/18 21:28:36 - mmengine - INFO - Epoch(train)  [63][20/80]  lr: 7.6245e-05  eta: 5:55:17  time: 7.1383  data_time: 0.0032  memory: 13286  loss: 0.0043  grad_norm: 0.0054
11/18 21:29:46 - mmengine - INFO - Epoch(train)  [63][30/80]  lr: 7.5918e-05  eta: 5:54:06  time: 7.0068  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0054
11/18 21:30:57 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:30:57 - mmengine - INFO - Epoch(train)  [63][40/80]  lr: 7.5591e-05  eta: 5:52:56  time: 7.0660  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0055
11/18 21:32:08 - mmengine - INFO - Epoch(train)  [63][50/80]  lr: 7.5265e-05  eta: 5:51:45  time: 7.0666  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0055
11/18 21:33:18 - mmengine - INFO - Epoch(train)  [63][60/80]  lr: 7.4939e-05  eta: 5:50:34  time: 7.0080  data_time: 0.0031  memory: 13286  loss: 0.0044  grad_norm: 0.0055
11/18 21:34:28 - mmengine - INFO - Epoch(train)  [63][70/80]  lr: 7.4614e-05  eta: 5:49:24  time: 7.0653  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0055
11/18 21:35:39 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:35:39 - mmengine - INFO - Epoch(train)  [63][80/80]  lr: 7.4289e-05  eta: 5:48:13  time: 7.0638  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0055
11/18 21:35:39 - mmengine - INFO - Saving checkpoint at 63 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 21:36:52 - mmengine - INFO - Epoch(train)  [64][10/80]  lr: 7.3965e-05  eta: 5:47:02  time: 7.0110  data_time: 0.0062  memory: 13286  loss: 0.0044  grad_norm: 0.0055
11/18 21:38:04 - mmengine - INFO - Epoch(train)  [64][20/80]  lr: 7.3641e-05  eta: 5:45:52  time: 7.1343  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0055
11/18 21:39:14 - mmengine - INFO - Epoch(train)  [64][30/80]  lr: 7.3318e-05  eta: 5:44:41  time: 7.0077  data_time: 0.0031  memory: 13286  loss: 0.0038  grad_norm: 0.0055
11/18 21:40:24 - mmengine - INFO - Epoch(train)  [64][40/80]  lr: 7.2996e-05  eta: 5:43:31  time: 7.0689  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0054
11/18 21:41:35 - mmengine - INFO - Epoch(train)  [64][50/80]  lr: 7.2674e-05  eta: 5:42:20  time: 7.0671  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0054
11/18 21:42:45 - mmengine - INFO - Epoch(train)  [64][60/80]  lr: 7.2353e-05  eta: 5:41:10  time: 7.0066  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0054
11/18 21:43:56 - mmengine - INFO - Epoch(train)  [64][70/80]  lr: 7.2032e-05  eta: 5:39:59  time: 7.0653  data_time: 0.0030  memory: 13286  loss: 0.0042  grad_norm: 0.0055
11/18 21:45:06 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:45:06 - mmengine - INFO - Epoch(train)  [64][80/80]  lr: 7.1712e-05  eta: 5:38:48  time: 7.0660  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0055
11/18 21:45:06 - mmengine - INFO - Saving checkpoint at 64 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 21:46:20 - mmengine - INFO - Epoch(train)  [65][10/80]  lr: 7.1392e-05  eta: 5:37:38  time: 7.0108  data_time: 0.0064  memory: 13286  loss: 0.0035  grad_norm: 0.0055
11/18 21:47:31 - mmengine - INFO - Epoch(train)  [65][20/80]  lr: 7.1073e-05  eta: 5:36:27  time: 7.1424  data_time: 0.0032  memory: 13286  loss: 0.0038  grad_norm: 0.0054
11/18 21:48:41 - mmengine - INFO - Epoch(train)  [65][30/80]  lr: 7.0755e-05  eta: 5:35:17  time: 7.0080  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0054
11/18 21:49:52 - mmengine - INFO - Epoch(train)  [65][40/80]  lr: 7.0437e-05  eta: 5:34:06  time: 7.0633  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0053
11/18 21:51:02 - mmengine - INFO - Epoch(train)  [65][50/80]  lr: 7.0120e-05  eta: 5:32:55  time: 7.0636  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0053
11/18 21:52:12 - mmengine - INFO - Epoch(train)  [65][60/80]  lr: 6.9804e-05  eta: 5:31:45  time: 7.0069  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0053
11/18 21:53:23 - mmengine - INFO - Epoch(train)  [65][70/80]  lr: 6.9488e-05  eta: 5:30:34  time: 7.0685  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0054
11/18 21:54:34 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 21:54:34 - mmengine - INFO - Epoch(train)  [65][80/80]  lr: 6.9172e-05  eta: 5:29:24  time: 7.0669  data_time: 0.0032  memory: 13286  loss: 0.0027  grad_norm: 0.0053
11/18 21:54:34 - mmengine - INFO - Saving checkpoint at 65 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 21:55:47 - mmengine - INFO - Epoch(train)  [66][10/80]  lr: 6.8858e-05  eta: 5:28:13  time: 7.0083  data_time: 0.0035  memory: 13286  loss: 0.0039  grad_norm: 0.0053
11/18 21:56:58 - mmengine - INFO - Epoch(train)  [66][20/80]  lr: 6.8544e-05  eta: 5:27:03  time: 7.1415  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0052
11/18 21:58:09 - mmengine - INFO - Epoch(train)  [66][30/80]  lr: 6.8230e-05  eta: 5:25:52  time: 7.0104  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0052
11/18 21:59:19 - mmengine - INFO - Epoch(train)  [66][40/80]  lr: 6.7918e-05  eta: 5:24:41  time: 7.0705  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0053
11/18 22:00:30 - mmengine - INFO - Epoch(train)  [66][50/80]  lr: 6.7605e-05  eta: 5:23:31  time: 7.0730  data_time: 0.0031  memory: 13286  loss: 0.0026  grad_norm: 0.0053
11/18 22:01:40 - mmengine - INFO - Epoch(train)  [66][60/80]  lr: 6.7294e-05  eta: 5:22:20  time: 7.0065  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0053
11/18 22:02:51 - mmengine - INFO - Epoch(train)  [66][70/80]  lr: 6.6983e-05  eta: 5:21:09  time: 7.0688  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0052
11/18 22:04:01 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 22:04:01 - mmengine - INFO - Epoch(train)  [66][80/80]  lr: 6.6673e-05  eta: 5:19:59  time: 7.0694  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0053
11/18 22:04:01 - mmengine - INFO - Saving checkpoint at 66 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 22:05:15 - mmengine - INFO - Epoch(train)  [67][10/80]  lr: 6.6364e-05  eta: 5:18:48  time: 7.0110  data_time: 0.0036  memory: 13286  loss: 0.0038  grad_norm: 0.0053
11/18 22:06:26 - mmengine - INFO - Epoch(train)  [67][20/80]  lr: 6.6055e-05  eta: 5:17:38  time: 7.1400  data_time: 0.0032  memory: 13286  loss: 0.0036  grad_norm: 0.0054
11/18 22:07:36 - mmengine - INFO - Epoch(train)  [67][30/80]  lr: 6.5747e-05  eta: 5:16:27  time: 7.0116  data_time: 0.0032  memory: 13286  loss: 0.0036  grad_norm: 0.0054
11/18 22:08:47 - mmengine - INFO - Epoch(train)  [67][40/80]  lr: 6.5439e-05  eta: 5:15:16  time: 7.0712  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0055
11/18 22:09:58 - mmengine - INFO - Epoch(train)  [67][50/80]  lr: 6.5133e-05  eta: 5:14:06  time: 7.0716  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0055
11/18 22:11:08 - mmengine - INFO - Epoch(train)  [67][60/80]  lr: 6.4827e-05  eta: 5:12:55  time: 7.0114  data_time: 0.0031  memory: 13286  loss: 0.0026  grad_norm: 0.0055
11/18 22:12:19 - mmengine - INFO - Epoch(train)  [67][70/80]  lr: 6.4521e-05  eta: 5:11:45  time: 7.0696  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0053
11/18 22:13:29 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 22:13:29 - mmengine - INFO - Epoch(train)  [67][80/80]  lr: 6.4217e-05  eta: 5:10:34  time: 7.0715  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0055
11/18 22:13:29 - mmengine - INFO - Saving checkpoint at 67 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 22:14:43 - mmengine - INFO - Epoch(train)  [68][10/80]  lr: 6.3913e-05  eta: 5:09:23  time: 7.0065  data_time: 0.0036  memory: 13286  loss: 0.0028  grad_norm: 0.0055
11/18 22:15:54 - mmengine - INFO - Epoch(train)  [68][20/80]  lr: 6.3610e-05  eta: 5:08:13  time: 7.1356  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0055
11/18 22:17:04 - mmengine - INFO - Epoch(train)  [68][30/80]  lr: 6.3307e-05  eta: 5:07:02  time: 7.0077  data_time: 0.0031  memory: 13286  loss: 0.0043  grad_norm: 0.0055
11/18 22:18:15 - mmengine - INFO - Epoch(train)  [68][40/80]  lr: 6.3005e-05  eta: 5:05:52  time: 7.0686  data_time: 0.0032  memory: 13286  loss: 0.0039  grad_norm: 0.0055
11/18 22:19:25 - mmengine - INFO - Epoch(train)  [68][50/80]  lr: 6.2704e-05  eta: 5:04:41  time: 7.0707  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0056
11/18 22:20:36 - mmengine - INFO - Epoch(train)  [68][60/80]  lr: 6.2404e-05  eta: 5:03:30  time: 7.0086  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0056
11/18 22:21:46 - mmengine - INFO - Epoch(train)  [68][70/80]  lr: 6.2104e-05  eta: 5:02:20  time: 7.0645  data_time: 0.0031  memory: 13286  loss: 0.0038  grad_norm: 0.0055
11/18 22:22:57 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 22:22:57 - mmengine - INFO - Epoch(train)  [68][80/80]  lr: 6.1805e-05  eta: 5:01:09  time: 7.0646  data_time: 0.0031  memory: 13286  loss: 0.0037  grad_norm: 0.0054
11/18 22:22:57 - mmengine - INFO - Saving checkpoint at 68 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 22:24:10 - mmengine - INFO - Epoch(train)  [69][10/80]  lr: 6.1507e-05  eta: 4:59:58  time: 7.0046  data_time: 0.0036  memory: 13286  loss: 0.0030  grad_norm: 0.0054
11/18 22:25:21 - mmengine - INFO - Epoch(train)  [69][20/80]  lr: 6.1210e-05  eta: 4:58:48  time: 7.1353  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0053
11/18 22:26:31 - mmengine - INFO - Epoch(train)  [69][30/80]  lr: 6.0913e-05  eta: 4:57:37  time: 7.0078  data_time: 0.0031  memory: 13286  loss: 0.0033  grad_norm: 0.0053
11/18 22:27:42 - mmengine - INFO - Epoch(train)  [69][40/80]  lr: 6.0617e-05  eta: 4:56:27  time: 7.0625  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0051
11/18 22:28:53 - mmengine - INFO - Epoch(train)  [69][50/80]  lr: 6.0322e-05  eta: 4:55:16  time: 7.0648  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0052
11/18 22:30:03 - mmengine - INFO - Epoch(train)  [69][60/80]  lr: 6.0028e-05  eta: 4:54:05  time: 7.0064  data_time: 0.0031  memory: 13286  loss: 0.0037  grad_norm: 0.0052
11/18 22:31:13 - mmengine - INFO - Epoch(train)  [69][70/80]  lr: 5.9735e-05  eta: 4:52:55  time: 7.0680  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0053
11/18 22:32:24 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 22:32:24 - mmengine - INFO - Epoch(train)  [69][80/80]  lr: 5.9442e-05  eta: 4:51:44  time: 7.0656  data_time: 0.0030  memory: 13286  loss: 0.0045  grad_norm: 0.0052
11/18 22:32:24 - mmengine - INFO - Saving checkpoint at 69 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 22:33:37 - mmengine - INFO - Epoch(train)  [70][10/80]  lr: 5.9150e-05  eta: 4:50:33  time: 7.0134  data_time: 0.0064  memory: 13286  loss: 0.0026  grad_norm: 0.0052
11/18 22:34:49 - mmengine - INFO - Epoch(train)  [70][20/80]  lr: 5.8859e-05  eta: 4:49:23  time: 7.1410  data_time: 0.0032  memory: 13286  loss: 0.0025  grad_norm: 0.0051
11/18 22:35:59 - mmengine - INFO - Epoch(train)  [70][30/80]  lr: 5.8568e-05  eta: 4:48:12  time: 7.0080  data_time: 0.0031  memory: 13286  loss: 0.0026  grad_norm: 0.0051
11/18 22:37:10 - mmengine - INFO - Epoch(train)  [70][40/80]  lr: 5.8278e-05  eta: 4:47:02  time: 7.0707  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0050
11/18 22:38:20 - mmengine - INFO - Epoch(train)  [70][50/80]  lr: 5.7990e-05  eta: 4:45:51  time: 7.0693  data_time: 0.0031  memory: 13286  loss: 0.0038  grad_norm: 0.0050
11/18 22:39:30 - mmengine - INFO - Epoch(train)  [70][60/80]  lr: 5.7702e-05  eta: 4:44:41  time: 7.0129  data_time: 0.0032  memory: 13286  loss: 0.0038  grad_norm: 0.0050
11/18 22:40:41 - mmengine - INFO - Epoch(train)  [70][70/80]  lr: 5.7414e-05  eta: 4:43:30  time: 7.0689  data_time: 0.0031  memory: 13286  loss: 0.0043  grad_norm: 0.0051
11/18 22:41:52 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 22:41:52 - mmengine - INFO - Epoch(train)  [70][80/80]  lr: 5.7128e-05  eta: 4:42:19  time: 7.0678  data_time: 0.0032  memory: 13286  loss: 0.0039  grad_norm: 0.0051
11/18 22:41:52 - mmengine - INFO - Saving checkpoint at 70 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 22:43:05 - mmengine - INFO - Epoch(train)  [71][10/80]  lr: 5.6842e-05  eta: 4:41:09  time: 7.0135  data_time: 0.0065  memory: 13286  loss: 0.0038  grad_norm: 0.0051
11/18 22:44:16 - mmengine - INFO - Epoch(train)  [71][20/80]  lr: 5.6558e-05  eta: 4:39:58  time: 7.1362  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0051
11/18 22:45:26 - mmengine - INFO - Epoch(train)  [71][30/80]  lr: 5.6274e-05  eta: 4:38:48  time: 7.0083  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0051
11/18 22:46:37 - mmengine - INFO - Epoch(train)  [71][40/80]  lr: 5.5990e-05  eta: 4:37:37  time: 7.0686  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0052
11/18 22:47:48 - mmengine - INFO - Epoch(train)  [71][50/80]  lr: 5.5708e-05  eta: 4:36:27  time: 7.0676  data_time: 0.0032  memory: 13286  loss: 0.0034  grad_norm: 0.0051
11/18 22:48:58 - mmengine - INFO - Epoch(train)  [71][60/80]  lr: 5.5427e-05  eta: 4:35:16  time: 7.0049  data_time: 0.0031  memory: 13286  loss: 0.0026  grad_norm: 0.0051
11/18 22:50:09 - mmengine - INFO - Epoch(train)  [71][70/80]  lr: 5.5146e-05  eta: 4:34:05  time: 7.0657  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0052
11/18 22:51:19 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 22:51:19 - mmengine - INFO - Epoch(train)  [71][80/80]  lr: 5.4866e-05  eta: 4:32:55  time: 7.0679  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0051
11/18 22:51:19 - mmengine - INFO - Saving checkpoint at 71 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 22:52:33 - mmengine - INFO - Epoch(train)  [72][10/80]  lr: 5.4587e-05  eta: 4:31:44  time: 7.0111  data_time: 0.0036  memory: 13286  loss: 0.0035  grad_norm: 0.0051
11/18 22:53:44 - mmengine - INFO - Epoch(train)  [72][20/80]  lr: 5.4309e-05  eta: 4:30:34  time: 7.1392  data_time: 0.0032  memory: 13286  loss: 0.0026  grad_norm: 0.0051
11/18 22:54:54 - mmengine - INFO - Epoch(train)  [72][30/80]  lr: 5.4032e-05  eta: 4:29:23  time: 7.0082  data_time: 0.0032  memory: 13286  loss: 0.0039  grad_norm: 0.0051
11/18 22:56:05 - mmengine - INFO - Epoch(train)  [72][40/80]  lr: 5.3756e-05  eta: 4:28:12  time: 7.0657  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0052
11/18 22:57:15 - mmengine - INFO - Epoch(train)  [72][50/80]  lr: 5.3480e-05  eta: 4:27:02  time: 7.0671  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0052
11/18 22:58:25 - mmengine - INFO - Epoch(train)  [72][60/80]  lr: 5.3206e-05  eta: 4:25:51  time: 7.0069  data_time: 0.0032  memory: 13286  loss: 0.0030  grad_norm: 0.0052
11/18 22:59:36 - mmengine - INFO - Epoch(train)  [72][70/80]  lr: 5.2932e-05  eta: 4:24:40  time: 7.0676  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0050
11/18 23:00:47 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:00:47 - mmengine - INFO - Epoch(train)  [72][80/80]  lr: 5.2659e-05  eta: 4:23:30  time: 7.0706  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0052
11/18 23:00:47 - mmengine - INFO - Saving checkpoint at 72 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:02:00 - mmengine - INFO - Epoch(train)  [73][10/80]  lr: 5.2387e-05  eta: 4:22:19  time: 7.0089  data_time: 0.0035  memory: 13286  loss: 0.0035  grad_norm: 0.0052
11/18 23:03:11 - mmengine - INFO - Epoch(train)  [73][20/80]  lr: 5.2116e-05  eta: 4:21:09  time: 7.1385  data_time: 0.0032  memory: 13286  loss: 0.0033  grad_norm: 0.0052
11/18 23:04:21 - mmengine - INFO - Epoch(train)  [73][30/80]  lr: 5.1846e-05  eta: 4:19:58  time: 7.0110  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0052
11/18 23:05:32 - mmengine - INFO - Epoch(train)  [73][40/80]  lr: 5.1577e-05  eta: 4:18:48  time: 7.0671  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0051
11/18 23:06:43 - mmengine - INFO - Epoch(train)  [73][50/80]  lr: 5.1308e-05  eta: 4:17:37  time: 7.0674  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0051
11/18 23:07:53 - mmengine - INFO - Epoch(train)  [73][60/80]  lr: 5.1041e-05  eta: 4:16:26  time: 7.0068  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0051
11/18 23:09:04 - mmengine - INFO - Epoch(train)  [73][70/80]  lr: 5.0774e-05  eta: 4:15:16  time: 7.0712  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0051
11/18 23:10:14 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:10:14 - mmengine - INFO - Epoch(train)  [73][80/80]  lr: 5.0508e-05  eta: 4:14:05  time: 7.0695  data_time: 0.0032  memory: 13286  loss: 0.0028  grad_norm: 0.0051
11/18 23:10:14 - mmengine - INFO - Saving checkpoint at 73 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:11:28 - mmengine - INFO - Epoch(train)  [74][10/80]  lr: 5.0244e-05  eta: 4:12:54  time: 7.0063  data_time: 0.0036  memory: 13286  loss: 0.0046  grad_norm: 0.0051
11/18 23:12:39 - mmengine - INFO - Epoch(train)  [74][20/80]  lr: 4.9980e-05  eta: 4:11:44  time: 7.1352  data_time: 0.0032  memory: 13286  loss: 0.0027  grad_norm: 0.0052
11/18 23:13:49 - mmengine - INFO - Epoch(train)  [74][30/80]  lr: 4.9717e-05  eta: 4:10:33  time: 7.0055  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0052
11/18 23:15:00 - mmengine - INFO - Epoch(train)  [74][40/80]  lr: 4.9455e-05  eta: 4:09:23  time: 7.0655  data_time: 0.0032  memory: 13286  loss: 0.0032  grad_norm: 0.0052
11/18 23:16:10 - mmengine - INFO - Epoch(train)  [74][50/80]  lr: 4.9194e-05  eta: 4:08:12  time: 7.0643  data_time: 0.0032  memory: 13286  loss: 0.0031  grad_norm: 0.0051
11/18 23:17:20 - mmengine - INFO - Epoch(train)  [74][60/80]  lr: 4.8934e-05  eta: 4:07:01  time: 7.0037  data_time: 0.0032  memory: 13286  loss: 0.0033  grad_norm: 0.0051
11/18 23:18:31 - mmengine - INFO - Epoch(train)  [74][70/80]  lr: 4.8675e-05  eta: 4:05:51  time: 7.0673  data_time: 0.0032  memory: 13286  loss: 0.0036  grad_norm: 0.0052
11/18 23:19:42 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:19:42 - mmengine - INFO - Epoch(train)  [74][80/80]  lr: 4.8417e-05  eta: 4:04:40  time: 7.0628  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0051
11/18 23:19:42 - mmengine - INFO - Saving checkpoint at 74 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:20:55 - mmengine - INFO - Epoch(train)  [75][10/80]  lr: 4.8159e-05  eta: 4:03:30  time: 7.0076  data_time: 0.0036  memory: 13286  loss: 0.0037  grad_norm: 0.0051
11/18 23:22:06 - mmengine - INFO - Epoch(train)  [75][20/80]  lr: 4.7903e-05  eta: 4:02:19  time: 7.1404  data_time: 0.0032  memory: 13286  loss: 0.0025  grad_norm: 0.0050
11/18 23:23:16 - mmengine - INFO - Epoch(train)  [75][30/80]  lr: 4.7648e-05  eta: 4:01:08  time: 7.0089  data_time: 0.0032  memory: 13286  loss: 0.0025  grad_norm: 0.0050
11/18 23:24:27 - mmengine - INFO - Epoch(train)  [75][40/80]  lr: 4.7393e-05  eta: 3:59:58  time: 7.0695  data_time: 0.0032  memory: 13286  loss: 0.0025  grad_norm: 0.0050
11/18 23:25:38 - mmengine - INFO - Epoch(train)  [75][50/80]  lr: 4.7140e-05  eta: 3:58:47  time: 7.0727  data_time: 0.0034  memory: 13286  loss: 0.0029  grad_norm: 0.0050
11/18 23:26:48 - mmengine - INFO - Epoch(train)  [75][60/80]  lr: 4.6887e-05  eta: 3:57:37  time: 7.0134  data_time: 0.0033  memory: 13286  loss: 0.0043  grad_norm: 0.0050
11/18 23:27:59 - mmengine - INFO - Epoch(train)  [75][70/80]  lr: 4.6636e-05  eta: 3:56:26  time: 7.0736  data_time: 0.0031  memory: 13286  loss: 0.0040  grad_norm: 0.0050
11/18 23:29:09 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:29:09 - mmengine - INFO - Epoch(train)  [75][80/80]  lr: 4.6385e-05  eta: 3:55:16  time: 7.0735  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0050
11/18 23:29:09 - mmengine - INFO - Saving checkpoint at 75 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:30:23 - mmengine - INFO - Epoch(train)  [76][10/80]  lr: 4.6136e-05  eta: 3:54:05  time: 7.0165  data_time: 0.0057  memory: 13286  loss: 0.0044  grad_norm: 0.0050
11/18 23:31:34 - mmengine - INFO - Epoch(train)  [76][20/80]  lr: 4.5887e-05  eta: 3:52:55  time: 7.1430  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0049
11/18 23:32:44 - mmengine - INFO - Epoch(train)  [76][30/80]  lr: 4.5640e-05  eta: 3:51:44  time: 7.0125  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0049
11/18 23:33:55 - mmengine - INFO - Epoch(train)  [76][40/80]  lr: 4.5393e-05  eta: 3:50:33  time: 7.0697  data_time: 0.0032  memory: 13286  loss: 0.0028  grad_norm: 0.0048
11/18 23:35:06 - mmengine - INFO - Epoch(train)  [76][50/80]  lr: 4.5148e-05  eta: 3:49:23  time: 7.0740  data_time: 0.0034  memory: 13286  loss: 0.0026  grad_norm: 0.0049
11/18 23:36:16 - mmengine - INFO - Epoch(train)  [76][60/80]  lr: 4.4903e-05  eta: 3:48:12  time: 7.0085  data_time: 0.0033  memory: 13286  loss: 0.0029  grad_norm: 0.0049
11/18 23:37:27 - mmengine - INFO - Epoch(train)  [76][70/80]  lr: 4.4660e-05  eta: 3:47:02  time: 7.0742  data_time: 0.0032  memory: 13286  loss: 0.0045  grad_norm: 0.0048
11/18 23:38:37 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:38:37 - mmengine - INFO - Epoch(train)  [76][80/80]  lr: 4.4417e-05  eta: 3:45:51  time: 7.0732  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0049
11/18 23:38:37 - mmengine - INFO - Saving checkpoint at 76 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:39:51 - mmengine - INFO - Epoch(train)  [77][10/80]  lr: 4.4175e-05  eta: 3:44:40  time: 7.0134  data_time: 0.0035  memory: 13286  loss: 0.0034  grad_norm: 0.0049
11/18 23:41:02 - mmengine - INFO - Epoch(train)  [77][20/80]  lr: 4.3935e-05  eta: 3:43:30  time: 7.1443  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0049
11/18 23:42:12 - mmengine - INFO - Epoch(train)  [77][30/80]  lr: 4.3695e-05  eta: 3:42:19  time: 7.0115  data_time: 0.0030  memory: 13286  loss: 0.0027  grad_norm: 0.0049
11/18 23:43:23 - mmengine - INFO - Epoch(train)  [77][40/80]  lr: 4.3457e-05  eta: 3:41:09  time: 7.0688  data_time: 0.0030  memory: 13286  loss: 0.0033  grad_norm: 0.0051
11/18 23:44:34 - mmengine - INFO - Epoch(train)  [77][50/80]  lr: 4.3220e-05  eta: 3:39:58  time: 7.0744  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0051
11/18 23:45:44 - mmengine - INFO - Epoch(train)  [77][60/80]  lr: 4.2983e-05  eta: 3:38:47  time: 7.0152  data_time: 0.0030  memory: 13286  loss: 0.0036  grad_norm: 0.0051
11/18 23:46:54 - mmengine - INFO - Epoch(train)  [77][70/80]  lr: 4.2748e-05  eta: 3:37:37  time: 7.0694  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0050
11/18 23:48:05 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:48:05 - mmengine - INFO - Epoch(train)  [77][80/80]  lr: 4.2513e-05  eta: 3:36:26  time: 7.0694  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0050
11/18 23:48:05 - mmengine - INFO - Saving checkpoint at 77 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:49:18 - mmengine - INFO - Epoch(train)  [78][10/80]  lr: 4.2280e-05  eta: 3:35:16  time: 7.0071  data_time: 0.0056  memory: 13286  loss: 0.0037  grad_norm: 0.0050
11/18 23:50:30 - mmengine - INFO - Epoch(train)  [78][20/80]  lr: 4.2048e-05  eta: 3:34:05  time: 7.1368  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0051
11/18 23:51:40 - mmengine - INFO - Epoch(train)  [78][30/80]  lr: 4.1817e-05  eta: 3:32:55  time: 7.0091  data_time: 0.0030  memory: 13286  loss: 0.0030  grad_norm: 0.0051
11/18 23:52:50 - mmengine - INFO - Epoch(train)  [78][40/80]  lr: 4.1586e-05  eta: 3:31:44  time: 7.0598  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0051
11/18 23:54:01 - mmengine - INFO - Epoch(train)  [78][50/80]  lr: 4.1357e-05  eta: 3:30:33  time: 7.0653  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0051
11/18 23:55:11 - mmengine - INFO - Epoch(train)  [78][60/80]  lr: 4.1129e-05  eta: 3:29:23  time: 7.0077  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0051
11/18 23:56:22 - mmengine - INFO - Epoch(train)  [78][70/80]  lr: 4.0902e-05  eta: 3:28:12  time: 7.0670  data_time: 0.0031  memory: 13286  loss: 0.0042  grad_norm: 0.0051
11/18 23:57:32 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/18 23:57:32 - mmengine - INFO - Epoch(train)  [78][80/80]  lr: 4.0676e-05  eta: 3:27:02  time: 7.0699  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0050
11/18 23:57:32 - mmengine - INFO - Saving checkpoint at 78 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/18 23:58:45 - mmengine - INFO - Epoch(train)  [79][10/80]  lr: 4.0451e-05  eta: 3:25:51  time: 7.0150  data_time: 0.0034  memory: 13286  loss: 0.0033  grad_norm: 0.0050
11/18 23:59:57 - mmengine - INFO - Epoch(train)  [79][20/80]  lr: 4.0228e-05  eta: 3:24:41  time: 7.1459  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0050
11/19 00:01:07 - mmengine - INFO - Epoch(train)  [79][30/80]  lr: 4.0005e-05  eta: 3:23:30  time: 7.0100  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0050
11/19 00:02:18 - mmengine - INFO - Epoch(train)  [79][40/80]  lr: 3.9783e-05  eta: 3:22:19  time: 7.0700  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0050
11/19 00:03:28 - mmengine - INFO - Epoch(train)  [79][50/80]  lr: 3.9563e-05  eta: 3:21:09  time: 7.0720  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0048
11/19 00:04:39 - mmengine - INFO - Epoch(train)  [79][60/80]  lr: 3.9343e-05  eta: 3:19:58  time: 7.0120  data_time: 0.0030  memory: 13286  loss: 0.0035  grad_norm: 0.0048
11/19 00:05:49 - mmengine - INFO - Epoch(train)  [79][70/80]  lr: 3.9125e-05  eta: 3:18:48  time: 7.0728  data_time: 0.0030  memory: 13286  loss: 0.0036  grad_norm: 0.0049
11/19 00:07:00 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 00:07:00 - mmengine - INFO - Epoch(train)  [79][80/80]  lr: 3.8908e-05  eta: 3:17:37  time: 7.0815  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0048
11/19 00:07:00 - mmengine - INFO - Saving checkpoint at 79 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 00:08:13 - mmengine - INFO - Epoch(train)  [80][10/80]  lr: 3.8692e-05  eta: 3:16:26  time: 7.0090  data_time: 0.0034  memory: 13286  loss: 0.0035  grad_norm: 0.0048
11/19 00:09:25 - mmengine - INFO - Epoch(train)  [80][20/80]  lr: 3.8476e-05  eta: 3:15:16  time: 7.1417  data_time: 0.0030  memory: 13286  loss: 0.0037  grad_norm: 0.0048
11/19 00:10:35 - mmengine - INFO - Epoch(train)  [80][30/80]  lr: 3.8262e-05  eta: 3:14:05  time: 7.0096  data_time: 0.0031  memory: 13286  loss: 0.0026  grad_norm: 0.0048
11/19 00:11:45 - mmengine - INFO - Epoch(train)  [80][40/80]  lr: 3.8050e-05  eta: 3:12:55  time: 7.0678  data_time: 0.0030  memory: 13286  loss: 0.0037  grad_norm: 0.0049
11/19 00:12:56 - mmengine - INFO - Epoch(train)  [80][50/80]  lr: 3.7838e-05  eta: 3:11:44  time: 7.0710  data_time: 0.0030  memory: 13286  loss: 0.0032  grad_norm: 0.0051
11/19 00:14:06 - mmengine - INFO - Epoch(train)  [80][60/80]  lr: 3.7627e-05  eta: 3:10:33  time: 7.0079  data_time: 0.0030  memory: 13286  loss: 0.0023  grad_norm: 0.0051
11/19 00:15:17 - mmengine - INFO - Epoch(train)  [80][70/80]  lr: 3.7418e-05  eta: 3:09:23  time: 7.0643  data_time: 0.0030  memory: 13286  loss: 0.0031  grad_norm: 0.0049
11/19 00:16:28 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 00:16:28 - mmengine - INFO - Epoch(train)  [80][80/80]  lr: 3.7209e-05  eta: 3:08:12  time: 7.0664  data_time: 0.0030  memory: 13286  loss: 0.0030  grad_norm: 0.0049
11/19 00:16:28 - mmengine - INFO - Saving checkpoint at 80 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 00:17:41 - mmengine - INFO - Epoch(train)  [81][10/80]  lr: 3.7002e-05  eta: 3:07:02  time: 7.0070  data_time: 0.0034  memory: 13286  loss: 0.0028  grad_norm: 0.0049
11/19 00:18:52 - mmengine - INFO - Epoch(train)  [81][20/80]  lr: 3.6796e-05  eta: 3:05:51  time: 7.1368  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0048
11/19 00:20:02 - mmengine - INFO - Epoch(train)  [81][30/80]  lr: 3.6591e-05  eta: 3:04:40  time: 7.0072  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0048
11/19 00:21:13 - mmengine - INFO - Epoch(train)  [81][40/80]  lr: 3.6387e-05  eta: 3:03:30  time: 7.0683  data_time: 0.0031  memory: 13286  loss: 0.0044  grad_norm: 0.0050
11/19 00:22:24 - mmengine - INFO - Epoch(train)  [81][50/80]  lr: 3.6184e-05  eta: 3:02:19  time: 7.0667  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0051
11/19 00:23:34 - mmengine - INFO - Epoch(train)  [81][60/80]  lr: 3.5982e-05  eta: 3:01:09  time: 7.0070  data_time: 0.0032  memory: 13286  loss: 0.0024  grad_norm: 0.0051
11/19 00:24:45 - mmengine - INFO - Epoch(train)  [81][70/80]  lr: 3.5782e-05  eta: 2:59:58  time: 7.0713  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0050
11/19 00:25:55 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 00:25:55 - mmengine - INFO - Epoch(train)  [81][80/80]  lr: 3.5583e-05  eta: 2:58:48  time: 7.0711  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0050
11/19 00:25:55 - mmengine - INFO - Saving checkpoint at 81 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 00:27:08 - mmengine - INFO - Epoch(train)  [82][10/80]  lr: 3.5384e-05  eta: 2:57:37  time: 7.0081  data_time: 0.0034  memory: 13286  loss: 0.0029  grad_norm: 0.0050
11/19 00:28:20 - mmengine - INFO - Epoch(train)  [82][20/80]  lr: 3.5187e-05  eta: 2:56:27  time: 7.1447  data_time: 0.0030  memory: 13286  loss: 0.0046  grad_norm: 0.0051
11/19 00:29:30 - mmengine - INFO - Epoch(train)  [82][30/80]  lr: 3.4991e-05  eta: 2:55:16  time: 7.0033  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0051
11/19 00:30:40 - mmengine - INFO - Epoch(train)  [82][40/80]  lr: 3.4797e-05  eta: 2:54:05  time: 7.0654  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0052
11/19 00:31:51 - mmengine - INFO - Epoch(train)  [82][50/80]  lr: 3.4603e-05  eta: 2:52:55  time: 7.0659  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0049
11/19 00:33:01 - mmengine - INFO - Epoch(train)  [82][60/80]  lr: 3.4411e-05  eta: 2:51:44  time: 7.0063  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0049
11/19 00:34:12 - mmengine - INFO - Epoch(train)  [82][70/80]  lr: 3.4220e-05  eta: 2:50:33  time: 7.0647  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0049
11/19 00:35:22 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 00:35:22 - mmengine - INFO - Epoch(train)  [82][80/80]  lr: 3.4029e-05  eta: 2:49:23  time: 7.0641  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0049
11/19 00:35:22 - mmengine - INFO - Saving checkpoint at 82 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 00:36:36 - mmengine - INFO - Epoch(train)  [83][10/80]  lr: 3.3841e-05  eta: 2:48:12  time: 7.0083  data_time: 0.0034  memory: 13286  loss: 0.0034  grad_norm: 0.0049
11/19 00:37:47 - mmengine - INFO - Epoch(train)  [83][20/80]  lr: 3.3653e-05  eta: 2:47:02  time: 7.1418  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0050
11/19 00:38:57 - mmengine - INFO - Epoch(train)  [83][30/80]  lr: 3.3466e-05  eta: 2:45:51  time: 7.0124  data_time: 0.0031  memory: 13286  loss: 0.0022  grad_norm: 0.0050
11/19 00:40:08 - mmengine - INFO - Epoch(train)  [83][40/80]  lr: 3.3281e-05  eta: 2:44:41  time: 7.0770  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0048
11/19 00:41:19 - mmengine - INFO - Epoch(train)  [83][50/80]  lr: 3.3097e-05  eta: 2:43:30  time: 7.0713  data_time: 0.0030  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 00:42:29 - mmengine - INFO - Epoch(train)  [83][60/80]  lr: 3.2914e-05  eta: 2:42:19  time: 7.0146  data_time: 0.0030  memory: 13286  loss: 0.0038  grad_norm: 0.0047
11/19 00:43:39 - mmengine - INFO - Epoch(train)  [83][70/80]  lr: 3.2732e-05  eta: 2:41:09  time: 7.0691  data_time: 0.0030  memory: 13286  loss: 0.0027  grad_norm: 0.0048
11/19 00:44:50 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 00:44:50 - mmengine - INFO - Epoch(train)  [83][80/80]  lr: 3.2551e-05  eta: 2:39:58  time: 7.0673  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0048
11/19 00:44:50 - mmengine - INFO - Saving checkpoint at 83 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 00:46:03 - mmengine - INFO - Epoch(train)  [84][10/80]  lr: 3.2372e-05  eta: 2:38:48  time: 7.0105  data_time: 0.0034  memory: 13286  loss: 0.0040  grad_norm: 0.0048
11/19 00:47:15 - mmengine - INFO - Epoch(train)  [84][20/80]  lr: 3.2194e-05  eta: 2:37:37  time: 7.1374  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0047
11/19 00:48:25 - mmengine - INFO - Epoch(train)  [84][30/80]  lr: 3.2017e-05  eta: 2:36:26  time: 7.0088  data_time: 0.0030  memory: 13286  loss: 0.0025  grad_norm: 0.0047
11/19 00:49:35 - mmengine - INFO - Epoch(train)  [84][40/80]  lr: 3.1841e-05  eta: 2:35:16  time: 7.0720  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0046
11/19 00:50:46 - mmengine - INFO - Epoch(train)  [84][50/80]  lr: 3.1666e-05  eta: 2:34:05  time: 7.0661  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0047
11/19 00:51:56 - mmengine - INFO - Epoch(train)  [84][60/80]  lr: 3.1493e-05  eta: 2:32:55  time: 7.0096  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0047
11/19 00:53:07 - mmengine - INFO - Epoch(train)  [84][70/80]  lr: 3.1320e-05  eta: 2:31:44  time: 7.0711  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0048
11/19 00:54:18 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 00:54:18 - mmengine - INFO - Epoch(train)  [84][80/80]  lr: 3.1149e-05  eta: 2:30:34  time: 7.0675  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0048
11/19 00:54:18 - mmengine - INFO - Saving checkpoint at 84 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 00:55:31 - mmengine - INFO - Epoch(train)  [85][10/80]  lr: 3.0980e-05  eta: 2:29:23  time: 7.0041  data_time: 0.0034  memory: 13286  loss: 0.0036  grad_norm: 0.0048
11/19 00:56:42 - mmengine - INFO - Epoch(train)  [85][20/80]  lr: 3.0811e-05  eta: 2:28:12  time: 7.1313  data_time: 0.0030  memory: 13286  loss: 0.0028  grad_norm: 0.0046
11/19 00:57:52 - mmengine - INFO - Epoch(train)  [85][30/80]  lr: 3.0644e-05  eta: 2:27:02  time: 7.0043  data_time: 0.0030  memory: 13286  loss: 0.0032  grad_norm: 0.0046
11/19 00:59:03 - mmengine - INFO - Epoch(train)  [85][40/80]  lr: 3.0478e-05  eta: 2:25:51  time: 7.0632  data_time: 0.0030  memory: 13286  loss: 0.0033  grad_norm: 0.0048
11/19 01:00:13 - mmengine - INFO - Epoch(train)  [85][50/80]  lr: 3.0313e-05  eta: 2:24:41  time: 7.0624  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0049
11/19 01:01:23 - mmengine - INFO - Epoch(train)  [85][60/80]  lr: 3.0149e-05  eta: 2:23:30  time: 7.0051  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0049
11/19 01:02:34 - mmengine - INFO - Epoch(train)  [85][70/80]  lr: 2.9987e-05  eta: 2:22:19  time: 7.0590  data_time: 0.0030  memory: 13286  loss: 0.0028  grad_norm: 0.0049
11/19 01:03:44 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:03:44 - mmengine - INFO - Epoch(train)  [85][80/80]  lr: 2.9825e-05  eta: 2:21:09  time: 7.0658  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0048
11/19 01:03:44 - mmengine - INFO - Saving checkpoint at 85 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 01:04:58 - mmengine - INFO - Epoch(train)  [86][10/80]  lr: 2.9666e-05  eta: 2:19:58  time: 7.0092  data_time: 0.0034  memory: 13286  loss: 0.0030  grad_norm: 0.0048
11/19 01:06:09 - mmengine - INFO - Epoch(train)  [86][20/80]  lr: 2.9507e-05  eta: 2:18:48  time: 7.1364  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 01:07:19 - mmengine - INFO - Epoch(train)  [86][30/80]  lr: 2.9349e-05  eta: 2:17:37  time: 7.0092  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0047
11/19 01:08:30 - mmengine - INFO - Epoch(train)  [86][40/80]  lr: 2.9193e-05  eta: 2:16:26  time: 7.0720  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0047
11/19 01:09:41 - mmengine - INFO - Epoch(train)  [86][50/80]  lr: 2.9038e-05  eta: 2:15:16  time: 7.0665  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0047
11/19 01:10:51 - mmengine - INFO - Epoch(train)  [86][60/80]  lr: 2.8884e-05  eta: 2:14:05  time: 7.0090  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0047
11/19 01:12:01 - mmengine - INFO - Epoch(train)  [86][70/80]  lr: 2.8732e-05  eta: 2:12:55  time: 7.0691  data_time: 0.0030  memory: 13286  loss: 0.0036  grad_norm: 0.0048
11/19 01:13:12 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:13:12 - mmengine - INFO - Epoch(train)  [86][80/80]  lr: 2.8581e-05  eta: 2:11:44  time: 7.0686  data_time: 0.0030  memory: 13286  loss: 0.0031  grad_norm: 0.0048
11/19 01:13:12 - mmengine - INFO - Saving checkpoint at 86 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 01:14:25 - mmengine - INFO - Epoch(train)  [87][10/80]  lr: 2.8431e-05  eta: 2:10:34  time: 7.0136  data_time: 0.0035  memory: 13286  loss: 0.0036  grad_norm: 0.0048
11/19 01:15:37 - mmengine - INFO - Epoch(train)  [87][20/80]  lr: 2.8282e-05  eta: 2:09:23  time: 7.1369  data_time: 0.0032  memory: 13286  loss: 0.0028  grad_norm: 0.0048
11/19 01:16:47 - mmengine - INFO - Epoch(train)  [87][30/80]  lr: 2.8134e-05  eta: 2:08:12  time: 7.0104  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0048
11/19 01:17:57 - mmengine - INFO - Epoch(train)  [87][40/80]  lr: 2.7988e-05  eta: 2:07:02  time: 7.0706  data_time: 0.0031  memory: 13286  loss: 0.0022  grad_norm: 0.0049
11/19 01:19:08 - mmengine - INFO - Epoch(train)  [87][50/80]  lr: 2.7843e-05  eta: 2:05:51  time: 7.0678  data_time: 0.0031  memory: 13286  loss: 0.0024  grad_norm: 0.0046
11/19 01:20:18 - mmengine - INFO - Epoch(train)  [87][60/80]  lr: 2.7700e-05  eta: 2:04:41  time: 7.0129  data_time: 0.0031  memory: 13286  loss: 0.0033  grad_norm: 0.0046
11/19 01:21:29 - mmengine - INFO - Epoch(train)  [87][70/80]  lr: 2.7557e-05  eta: 2:03:30  time: 7.0696  data_time: 0.0030  memory: 13286  loss: 0.0030  grad_norm: 0.0046
11/19 01:22:40 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:22:40 - mmengine - INFO - Epoch(train)  [87][80/80]  lr: 2.7416e-05  eta: 2:02:20  time: 7.0771  data_time: 0.0031  memory: 13286  loss: 0.0037  grad_norm: 0.0047
11/19 01:22:40 - mmengine - INFO - Saving checkpoint at 87 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 01:23:53 - mmengine - INFO - Epoch(train)  [88][10/80]  lr: 2.7276e-05  eta: 2:01:09  time: 7.0162  data_time: 0.0034  memory: 13286  loss: 0.0034  grad_norm: 0.0047
11/19 01:25:04 - mmengine - INFO - Epoch(train)  [88][20/80]  lr: 2.7138e-05  eta: 1:59:58  time: 7.1473  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0049
11/19 01:26:14 - mmengine - INFO - Epoch(train)  [88][30/80]  lr: 2.7000e-05  eta: 1:58:48  time: 7.0138  data_time: 0.0031  memory: 13286  loss: 0.0034  grad_norm: 0.0049
11/19 01:27:25 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:27:25 - mmengine - INFO - Epoch(train)  [88][40/80]  lr: 2.6864e-05  eta: 1:57:37  time: 7.0684  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0048
11/19 01:28:36 - mmengine - INFO - Epoch(train)  [88][50/80]  lr: 2.6730e-05  eta: 1:56:27  time: 7.0774  data_time: 0.0030  memory: 13286  loss: 0.0041  grad_norm: 0.0048
11/19 01:29:46 - mmengine - INFO - Epoch(train)  [88][60/80]  lr: 2.6596e-05  eta: 1:55:16  time: 7.0104  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0048
11/19 01:30:57 - mmengine - INFO - Epoch(train)  [88][70/80]  lr: 2.6464e-05  eta: 1:54:05  time: 7.0678  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0048
11/19 01:32:07 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:32:07 - mmengine - INFO - Epoch(train)  [88][80/80]  lr: 2.6333e-05  eta: 1:52:55  time: 7.0752  data_time: 0.0030  memory: 13286  loss: 0.0027  grad_norm: 0.0047
11/19 01:32:07 - mmengine - INFO - Saving checkpoint at 88 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 01:33:21 - mmengine - INFO - Epoch(train)  [89][10/80]  lr: 2.6204e-05  eta: 1:51:44  time: 7.0100  data_time: 0.0056  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 01:34:32 - mmengine - INFO - Epoch(train)  [89][20/80]  lr: 2.6075e-05  eta: 1:50:34  time: 7.1355  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0047
11/19 01:35:42 - mmengine - INFO - Epoch(train)  [89][30/80]  lr: 2.5948e-05  eta: 1:49:23  time: 7.0036  data_time: 0.0031  memory: 13286  loss: 0.0022  grad_norm: 0.0047
11/19 01:36:53 - mmengine - INFO - Epoch(train)  [89][40/80]  lr: 2.5823e-05  eta: 1:48:13  time: 7.0612  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0047
11/19 01:38:03 - mmengine - INFO - Epoch(train)  [89][50/80]  lr: 2.5698e-05  eta: 1:47:02  time: 7.0661  data_time: 0.0030  memory: 13286  loss: 0.0027  grad_norm: 0.0048
11/19 01:39:13 - mmengine - INFO - Epoch(train)  [89][60/80]  lr: 2.5575e-05  eta: 1:45:51  time: 7.0059  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0048
11/19 01:40:24 - mmengine - INFO - Epoch(train)  [89][70/80]  lr: 2.5453e-05  eta: 1:44:41  time: 7.0649  data_time: 0.0030  memory: 13286  loss: 0.0040  grad_norm: 0.0048
11/19 01:41:35 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:41:35 - mmengine - INFO - Epoch(train)  [89][80/80]  lr: 2.5333e-05  eta: 1:43:30  time: 7.0673  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0048
11/19 01:41:35 - mmengine - INFO - Saving checkpoint at 89 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 01:42:48 - mmengine - INFO - Epoch(train)  [90][10/80]  lr: 2.5214e-05  eta: 1:42:20  time: 7.0129  data_time: 0.0034  memory: 13286  loss: 0.0027  grad_norm: 0.0048
11/19 01:43:59 - mmengine - INFO - Epoch(train)  [90][20/80]  lr: 2.5096e-05  eta: 1:41:09  time: 7.1391  data_time: 0.0031  memory: 13286  loss: 0.0048  grad_norm: 0.0048
11/19 01:45:09 - mmengine - INFO - Epoch(train)  [90][30/80]  lr: 2.4979e-05  eta: 1:39:59  time: 7.0064  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0048
11/19 01:46:20 - mmengine - INFO - Epoch(train)  [90][40/80]  lr: 2.4864e-05  eta: 1:38:48  time: 7.0717  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0050
11/19 01:47:31 - mmengine - INFO - Epoch(train)  [90][50/80]  lr: 2.4750e-05  eta: 1:37:37  time: 7.0690  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0049
11/19 01:48:41 - mmengine - INFO - Epoch(train)  [90][60/80]  lr: 2.4637e-05  eta: 1:36:27  time: 7.0056  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0049
11/19 01:49:52 - mmengine - INFO - Epoch(train)  [90][70/80]  lr: 2.4526e-05  eta: 1:35:16  time: 7.0723  data_time: 0.0030  memory: 13286  loss: 0.0021  grad_norm: 0.0048
11/19 01:51:02 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 01:51:02 - mmengine - INFO - Epoch(train)  [90][80/80]  lr: 2.4416e-05  eta: 1:34:06  time: 7.0719  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0048
11/19 01:51:02 - mmengine - INFO - Saving checkpoint at 90 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 01:52:16 - mmengine - INFO - Epoch(train)  [91][10/80]  lr: 2.4307e-05  eta: 1:32:55  time: 7.0079  data_time: 0.0034  memory: 13286  loss: 0.0032  grad_norm: 0.0048
11/19 01:53:27 - mmengine - INFO - Epoch(train)  [91][20/80]  lr: 2.4200e-05  eta: 1:31:45  time: 7.1373  data_time: 0.0030  memory: 13286  loss: 0.0032  grad_norm: 0.0048
11/19 01:54:37 - mmengine - INFO - Epoch(train)  [91][30/80]  lr: 2.4094e-05  eta: 1:30:34  time: 7.0076  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0048
11/19 01:55:48 - mmengine - INFO - Epoch(train)  [91][40/80]  lr: 2.3989e-05  eta: 1:29:23  time: 7.0725  data_time: 0.0030  memory: 13286  loss: 0.0022  grad_norm: 0.0049
11/19 01:56:58 - mmengine - INFO - Epoch(train)  [91][50/80]  lr: 2.3886e-05  eta: 1:28:13  time: 7.0655  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0050
11/19 01:58:08 - mmengine - INFO - Epoch(train)  [91][60/80]  lr: 2.3784e-05  eta: 1:27:02  time: 7.0076  data_time: 0.0030  memory: 13286  loss: 0.0031  grad_norm: 0.0050
11/19 01:59:19 - mmengine - INFO - Epoch(train)  [91][70/80]  lr: 2.3683e-05  eta: 1:25:52  time: 7.0726  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0048
11/19 02:00:30 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:00:30 - mmengine - INFO - Epoch(train)  [91][80/80]  lr: 2.3583e-05  eta: 1:24:41  time: 7.0629  data_time: 0.0030  memory: 13286  loss: 0.0030  grad_norm: 0.0048
11/19 02:00:30 - mmengine - INFO - Saving checkpoint at 91 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:01:43 - mmengine - INFO - Epoch(train)  [92][10/80]  lr: 2.3485e-05  eta: 1:23:30  time: 7.0133  data_time: 0.0034  memory: 13286  loss: 0.0020  grad_norm: 0.0048
11/19 02:02:55 - mmengine - INFO - Epoch(train)  [92][20/80]  lr: 2.3389e-05  eta: 1:22:20  time: 7.1385  data_time: 0.0031  memory: 13286  loss: 0.0039  grad_norm: 0.0048
11/19 02:04:05 - mmengine - INFO - Epoch(train)  [92][30/80]  lr: 2.3293e-05  eta: 1:21:09  time: 7.0111  data_time: 0.0030  memory: 13286  loss: 0.0028  grad_norm: 0.0048
11/19 02:05:15 - mmengine - INFO - Epoch(train)  [92][40/80]  lr: 2.3199e-05  eta: 1:19:59  time: 7.0720  data_time: 0.0030  memory: 13286  loss: 0.0033  grad_norm: 0.0047
11/19 02:06:26 - mmengine - INFO - Epoch(train)  [92][50/80]  lr: 2.3106e-05  eta: 1:18:48  time: 7.0671  data_time: 0.0030  memory: 13286  loss: 0.0027  grad_norm: 0.0047
11/19 02:07:36 - mmengine - INFO - Epoch(train)  [92][60/80]  lr: 2.3015e-05  eta: 1:17:38  time: 7.0119  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0047
11/19 02:08:47 - mmengine - INFO - Epoch(train)  [92][70/80]  lr: 2.2925e-05  eta: 1:16:27  time: 7.0699  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0046
11/19 02:09:58 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:09:58 - mmengine - INFO - Epoch(train)  [92][80/80]  lr: 2.2836e-05  eta: 1:15:16  time: 7.0732  data_time: 0.0030  memory: 13286  loss: 0.0036  grad_norm: 0.0048
11/19 02:09:58 - mmengine - INFO - Saving checkpoint at 92 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:11:11 - mmengine - INFO - Epoch(train)  [93][10/80]  lr: 2.2749e-05  eta: 1:14:06  time: 7.0085  data_time: 0.0034  memory: 13286  loss: 0.0029  grad_norm: 0.0048
11/19 02:12:22 - mmengine - INFO - Epoch(train)  [93][20/80]  lr: 2.2663e-05  eta: 1:12:55  time: 7.1361  data_time: 0.0030  memory: 13286  loss: 0.0033  grad_norm: 0.0048
11/19 02:13:32 - mmengine - INFO - Epoch(train)  [93][30/80]  lr: 2.2578e-05  eta: 1:11:45  time: 7.0080  data_time: 0.0030  memory: 13286  loss: 0.0028  grad_norm: 0.0048
11/19 02:14:43 - mmengine - INFO - Epoch(train)  [93][40/80]  lr: 2.2495e-05  eta: 1:10:34  time: 7.0692  data_time: 0.0030  memory: 13286  loss: 0.0035  grad_norm: 0.0048
11/19 02:15:54 - mmengine - INFO - Epoch(train)  [93][50/80]  lr: 2.2413e-05  eta: 1:09:23  time: 7.0641  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0047
11/19 02:17:04 - mmengine - INFO - Epoch(train)  [93][60/80]  lr: 2.2332e-05  eta: 1:08:13  time: 7.0054  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0047
11/19 02:18:14 - mmengine - INFO - Epoch(train)  [93][70/80]  lr: 2.2253e-05  eta: 1:07:02  time: 7.0695  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0048
11/19 02:19:25 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:19:25 - mmengine - INFO - Epoch(train)  [93][80/80]  lr: 2.2175e-05  eta: 1:05:52  time: 7.0612  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0048
11/19 02:19:25 - mmengine - INFO - Saving checkpoint at 93 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:20:38 - mmengine - INFO - Epoch(train)  [94][10/80]  lr: 2.2099e-05  eta: 1:04:41  time: 7.0078  data_time: 0.0034  memory: 13286  loss: 0.0028  grad_norm: 0.0048
11/19 02:21:50 - mmengine - INFO - Epoch(train)  [94][20/80]  lr: 2.2023e-05  eta: 1:03:31  time: 7.1381  data_time: 0.0030  memory: 13286  loss: 0.0023  grad_norm: 0.0047
11/19 02:23:00 - mmengine - INFO - Epoch(train)  [94][30/80]  lr: 2.1950e-05  eta: 1:02:20  time: 7.0065  data_time: 0.0030  memory: 13286  loss: 0.0037  grad_norm: 0.0047
11/19 02:24:10 - mmengine - INFO - Epoch(train)  [94][40/80]  lr: 2.1877e-05  eta: 1:01:09  time: 7.0688  data_time: 0.0031  memory: 13286  loss: 0.0024  grad_norm: 0.0048
11/19 02:25:21 - mmengine - INFO - Epoch(train)  [94][50/80]  lr: 2.1806e-05  eta: 0:59:59  time: 7.0626  data_time: 0.0030  memory: 13286  loss: 0.0021  grad_norm: 0.0047
11/19 02:26:31 - mmengine - INFO - Epoch(train)  [94][60/80]  lr: 2.1736e-05  eta: 0:58:48  time: 7.0031  data_time: 0.0031  memory: 13286  loss: 0.0047  grad_norm: 0.0047
11/19 02:27:42 - mmengine - INFO - Epoch(train)  [94][70/80]  lr: 2.1668e-05  eta: 0:57:38  time: 7.0649  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0049
11/19 02:28:52 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:28:52 - mmengine - INFO - Epoch(train)  [94][80/80]  lr: 2.1601e-05  eta: 0:56:27  time: 7.0671  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0047
11/19 02:28:52 - mmengine - INFO - Saving checkpoint at 94 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:30:05 - mmengine - INFO - Epoch(train)  [95][10/80]  lr: 2.1535e-05  eta: 0:55:16  time: 7.0056  data_time: 0.0057  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 02:31:17 - mmengine - INFO - Epoch(train)  [95][20/80]  lr: 2.1471e-05  eta: 0:54:06  time: 7.1368  data_time: 0.0031  memory: 13286  loss: 0.0022  grad_norm: 0.0047
11/19 02:32:27 - mmengine - INFO - Epoch(train)  [95][30/80]  lr: 2.1408e-05  eta: 0:52:55  time: 7.0052  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0047
11/19 02:33:38 - mmengine - INFO - Epoch(train)  [95][40/80]  lr: 2.1346e-05  eta: 0:51:45  time: 7.0649  data_time: 0.0031  memory: 13286  loss: 0.0022  grad_norm: 0.0047
11/19 02:34:48 - mmengine - INFO - Epoch(train)  [95][50/80]  lr: 2.1286e-05  eta: 0:50:34  time: 7.0688  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0047
11/19 02:35:58 - mmengine - INFO - Epoch(train)  [95][60/80]  lr: 2.1227e-05  eta: 0:49:24  time: 7.0094  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 02:37:09 - mmengine - INFO - Epoch(train)  [95][70/80]  lr: 2.1170e-05  eta: 0:48:13  time: 7.0622  data_time: 0.0030  memory: 13286  loss: 0.0031  grad_norm: 0.0047
11/19 02:38:20 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:38:20 - mmengine - INFO - Epoch(train)  [95][80/80]  lr: 2.1114e-05  eta: 0:47:02  time: 7.0659  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0047
11/19 02:38:20 - mmengine - INFO - Saving checkpoint at 95 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:39:33 - mmengine - INFO - Epoch(train)  [96][10/80]  lr: 2.1059e-05  eta: 0:45:52  time: 7.0134  data_time: 0.0034  memory: 13286  loss: 0.0021  grad_norm: 0.0047
11/19 02:40:44 - mmengine - INFO - Epoch(train)  [96][20/80]  lr: 2.1005e-05  eta: 0:44:41  time: 7.1400  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0046
11/19 02:41:54 - mmengine - INFO - Epoch(train)  [96][30/80]  lr: 2.0953e-05  eta: 0:43:31  time: 7.0111  data_time: 0.0031  memory: 13286  loss: 0.0033  grad_norm: 0.0046
11/19 02:43:05 - mmengine - INFO - Epoch(train)  [96][40/80]  lr: 2.0903e-05  eta: 0:42:20  time: 7.0673  data_time: 0.0031  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 02:44:16 - mmengine - INFO - Epoch(train)  [96][50/80]  lr: 2.0854e-05  eta: 0:41:10  time: 7.0686  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0047
11/19 02:45:26 - mmengine - INFO - Epoch(train)  [96][60/80]  lr: 2.0806e-05  eta: 0:39:59  time: 7.0085  data_time: 0.0030  memory: 13286  loss: 0.0024  grad_norm: 0.0047
11/19 02:46:36 - mmengine - INFO - Epoch(train)  [96][70/80]  lr: 2.0759e-05  eta: 0:38:48  time: 7.0647  data_time: 0.0031  memory: 13286  loss: 0.0031  grad_norm: 0.0046
11/19 02:47:47 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:47:47 - mmengine - INFO - Epoch(train)  [96][80/80]  lr: 2.0714e-05  eta: 0:37:38  time: 7.0696  data_time: 0.0030  memory: 13286  loss: 0.0040  grad_norm: 0.0047
11/19 02:47:47 - mmengine - INFO - Saving checkpoint at 96 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:49:00 - mmengine - INFO - Epoch(train)  [97][10/80]  lr: 2.0670e-05  eta: 0:36:27  time: 7.0131  data_time: 0.0034  memory: 13286  loss: 0.0030  grad_norm: 0.0047
11/19 02:50:12 - mmengine - INFO - Epoch(train)  [97][20/80]  lr: 2.0628e-05  eta: 0:35:17  time: 7.1400  data_time: 0.0030  memory: 13286  loss: 0.0025  grad_norm: 0.0047
11/19 02:51:22 - mmengine - INFO - Epoch(train)  [97][30/80]  lr: 2.0587e-05  eta: 0:34:06  time: 7.0129  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0047
11/19 02:52:33 - mmengine - INFO - Epoch(train)  [97][40/80]  lr: 2.0547e-05  eta: 0:32:56  time: 7.0680  data_time: 0.0030  memory: 13286  loss: 0.0036  grad_norm: 0.0047
11/19 02:53:43 - mmengine - INFO - Epoch(train)  [97][50/80]  lr: 2.0509e-05  eta: 0:31:45  time: 7.0741  data_time: 0.0030  memory: 13286  loss: 0.0022  grad_norm: 0.0047
11/19 02:54:53 - mmengine - INFO - Epoch(train)  [97][60/80]  lr: 2.0472e-05  eta: 0:30:34  time: 7.0104  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0047
11/19 02:56:04 - mmengine - INFO - Epoch(train)  [97][70/80]  lr: 2.0437e-05  eta: 0:29:24  time: 7.0657  data_time: 0.0030  memory: 13286  loss: 0.0032  grad_norm: 0.0047
11/19 02:57:15 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 02:57:15 - mmengine - INFO - Epoch(train)  [97][80/80]  lr: 2.0403e-05  eta: 0:28:13  time: 7.0690  data_time: 0.0030  memory: 13286  loss: 0.0027  grad_norm: 0.0048
11/19 02:57:15 - mmengine - INFO - Saving checkpoint at 97 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 02:58:28 - mmengine - INFO - Epoch(train)  [98][10/80]  lr: 2.0370e-05  eta: 0:27:03  time: 7.0109  data_time: 0.0034  memory: 13286  loss: 0.0024  grad_norm: 0.0048
11/19 02:59:39 - mmengine - INFO - Epoch(train)  [98][20/80]  lr: 2.0339e-05  eta: 0:25:52  time: 7.1415  data_time: 0.0030  memory: 13286  loss: 0.0034  grad_norm: 0.0048
11/19 03:00:49 - mmengine - INFO - Epoch(train)  [98][30/80]  lr: 2.0309e-05  eta: 0:24:42  time: 7.0040  data_time: 0.0031  memory: 13286  loss: 0.0036  grad_norm: 0.0048
11/19 03:02:00 - mmengine - INFO - Epoch(train)  [98][40/80]  lr: 2.0280e-05  eta: 0:23:31  time: 7.0643  data_time: 0.0031  memory: 13286  loss: 0.0023  grad_norm: 0.0046
11/19 03:03:11 - mmengine - INFO - Epoch(train)  [98][50/80]  lr: 2.0253e-05  eta: 0:22:20  time: 7.0612  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0047
11/19 03:04:21 - mmengine - INFO - Epoch(train)  [98][60/80]  lr: 2.0227e-05  eta: 0:21:10  time: 7.0083  data_time: 0.0030  memory: 13286  loss: 0.0026  grad_norm: 0.0047
11/19 03:05:31 - mmengine - INFO - Epoch(train)  [98][70/80]  lr: 2.0203e-05  eta: 0:19:59  time: 7.0642  data_time: 0.0030  memory: 13286  loss: 0.0032  grad_norm: 0.0048
11/19 03:06:42 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 03:06:42 - mmengine - INFO - Epoch(train)  [98][80/80]  lr: 2.0180e-05  eta: 0:18:49  time: 7.0756  data_time: 0.0030  memory: 13286  loss: 0.0030  grad_norm: 0.0046
11/19 03:06:42 - mmengine - INFO - Saving checkpoint at 98 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 03:07:56 - mmengine - INFO - Epoch(train)  [99][10/80]  lr: 2.0158e-05  eta: 0:17:38  time: 7.0101  data_time: 0.0034  memory: 13286  loss: 0.0034  grad_norm: 0.0046
11/19 03:09:07 - mmengine - INFO - Epoch(train)  [99][20/80]  lr: 2.0138e-05  eta: 0:16:28  time: 7.1419  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0045
11/19 03:10:17 - mmengine - INFO - Epoch(train)  [99][30/80]  lr: 2.0119e-05  eta: 0:15:17  time: 7.0068  data_time: 0.0031  memory: 13286  loss: 0.0029  grad_norm: 0.0045
11/19 03:11:28 - mmengine - INFO - Epoch(train)  [99][40/80]  lr: 2.0102e-05  eta: 0:14:06  time: 7.0650  data_time: 0.0031  memory: 13286  loss: 0.0041  grad_norm: 0.0046
11/19 03:12:38 - mmengine - INFO - Epoch(train)  [99][50/80]  lr: 2.0085e-05  eta: 0:12:56  time: 7.0662  data_time: 0.0031  memory: 13286  loss: 0.0025  grad_norm: 0.0046
11/19 03:13:49 - mmengine - INFO - Epoch(train)  [99][60/80]  lr: 2.0071e-05  eta: 0:11:45  time: 7.0056  data_time: 0.0031  memory: 13286  loss: 0.0035  grad_norm: 0.0046
11/19 03:14:59 - mmengine - INFO - Epoch(train)  [99][70/80]  lr: 2.0057e-05  eta: 0:10:35  time: 7.0690  data_time: 0.0031  memory: 13286  loss: 0.0022  grad_norm: 0.0047
11/19 03:16:10 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 03:16:10 - mmengine - INFO - Epoch(train)  [99][80/80]  lr: 2.0046e-05  eta: 0:09:24  time: 7.0663  data_time: 0.0031  memory: 13286  loss: 0.0024  grad_norm: 0.0046
11/19 03:16:10 - mmengine - INFO - Saving checkpoint at 99 epochs
/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
11/19 03:17:23 - mmengine - INFO - Epoch(train) [100][10/80]  lr: 2.0035e-05  eta: 0:08:14  time: 7.0061  data_time: 0.0057  memory: 13286  loss: 0.0028  grad_norm: 0.0046
11/19 03:18:34 - mmengine - INFO - Epoch(train) [100][20/80]  lr: 2.0026e-05  eta: 0:07:03  time: 7.1315  data_time: 0.0031  memory: 13286  loss: 0.0030  grad_norm: 0.0045
11/19 03:19:44 - mmengine - INFO - Epoch(train) [100][30/80]  lr: 2.0018e-05  eta: 0:05:52  time: 7.0046  data_time: 0.0031  memory: 13286  loss: 0.0027  grad_norm: 0.0045
11/19 03:20:55 - mmengine - INFO - Epoch(train) [100][40/80]  lr: 2.0012e-05  eta: 0:04:42  time: 7.0672  data_time: 0.0031  memory: 13286  loss: 0.0044  grad_norm: 0.0047
11/19 03:22:06 - mmengine - INFO - Epoch(train) [100][50/80]  lr: 2.0007e-05  eta: 0:03:31  time: 7.0660  data_time: 0.0030  memory: 13286  loss: 0.0021  grad_norm: 0.0047
11/19 03:23:16 - mmengine - INFO - Epoch(train) [100][60/80]  lr: 2.0003e-05  eta: 0:02:21  time: 7.0053  data_time: 0.0031  memory: 13286  loss: 0.0028  grad_norm: 0.0047
11/19 03:24:27 - mmengine - INFO - Epoch(train) [100][70/80]  lr: 2.0001e-05  eta: 0:01:10  time: 7.0721  data_time: 0.0030  memory: 13286  loss: 0.0025  grad_norm: 0.0047
11/19 03:25:37 - mmengine - INFO - Exp name: baichuan_xtuner_config_20231118_113741
11/19 03:25:37 - mmengine - INFO - Epoch(train) [100][80/80]  lr: 2.0000e-05  eta: 0:00:00  time: 7.0652  data_time: 0.0030  memory: 13286  loss: 0.0029  grad_norm: 0.0048
11/19 03:25:37 - mmengine - INFO - Saving checkpoint at 100 epochs
+ test
+ xtuner_config=/data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py
+ pth_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.pth
+ hf_model_path=/data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.hf
+ xtuner convert pth_to_hf /data/yinxiaoln/code/Deep4Everything/Baichuan2/fine-tune/xtuner/baichuan_xtuner_config.py /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.pth /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.hf
[2023-11-19 03:25:52,116] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-19 03:25:56,478] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
quantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.80s/it]
11/19 03:26:10 - mmengine - INFO - dispatch baichuan2 NormHead forward
11/19 03:26:10 - mmengine - INFO - dispatch baichuan2-7B attn forward
11/19 03:26:10 - mmengine - WARNING - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
11/19 03:26:10 - mmengine - INFO - dispatch baichuan2-13B attn forward
Load PTH model from /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.pth
Convert weights to float16
Saving HuggingFace model to /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.hf
All done!
+ python merge.py /data/yinxiaoln/pre_models/Baichuan2-7B-Chat /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/epoch_100.hf /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/merged
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.69s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.61s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]
Saving to /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/merged...
All done!
+ python xtuner_test.py /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/output.json /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/merged
2023-11-19 03:29:44,160 - modelscope - INFO - PyTorch version 2.1.0 Found.
2023-11-19 03:29:44,161 - modelscope - INFO - Loading ast index from /home/yinxiaoln/.cache/modelscope/ast_indexer
2023-11-19 03:29:44,208 - modelscope - INFO - Loading done! Current index file version is 1.9.2, with md5 3cdb0bb552f9ef7c5b6c350d835395e7 and a total number of 941 components indexed
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
/data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/merged
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:12,  1.79s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:03<00:10,  1.69s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:05<00:08,  1.66s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:06<00:06,  1.64s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:08<00:04,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:09<00:03,  1.66s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:11<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.44s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.57s/it]
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Dumping model to file cache /tmp/jieba.cache
Dump cache file failed.
Traceback (most recent call last):
  File "/data/yinxiaoln/apps/miniconda3/envs/baichuan2/lib/python3.11/site-packages/jieba/__init__.py", line 154, in initialize
    _replace_file(fpath, cache_file)
PermissionError: [Errno 1] Operation not permitted: '/tmp/tmp80n0dwis' -> '/tmp/jieba.cache'
Loading model cost 0.810 seconds.
Prefix dict has been built successfully.
tot_cnt 585
0 / 585 Sun Nov 19 03:30:00 2023
正确。 正确。
tensor([[4283,   66]]) tensor([[4283,   66]])
100 / 585 Sun Nov 19 03:30:43 2023
基准轴或基准面 尺寸线的延长线;中心要素
tensor([[28249, 94427, 92600, 28249, 92454]]) tensor([[12014, 11684,  9535, 92597, 92399,  1925, 13813]])
200 / 585 Sun Nov 19 03:31:18 2023
车削 镗孔
tensor([[92508, 95044]]) tensor([[98717, 93956]])
300 / 585 Sun Nov 19 03:32:09 2023
错误 错误
tensor([[5437]]) tensor([[5437]])
400 / 585 Sun Nov 19 03:32:51 2023
铸铁;合金钢;热处理钢 高速钢;高强度合金工具钢;硬质合金
tensor([[94782, 93138, 92399, 24902, 93655, 92399, 92787,  2639, 93655]]) tensor([[ 3622, 93655, 92399, 50034, 24902,  5027, 93655, 92399, 93564, 92691,
         24902]])
500 / 585 Sun Nov 19 03:33:35 2023
错误 错误
tensor([[5437]]) tensor([[5437]])
{'rouge-1': 38.69851264957267, 'rouge-2': 8.006663760683757, 'rouge-l': 38.22754290598292, 'bleu-4': 25.29877145299145}
saved to  /data/yinxiaoln/save/Baichuan2/2023-11-18_11-37-32/output.json
